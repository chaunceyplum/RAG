{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-function",
    "title": "CREATE FUNCTION | Snowflake Documentation",
    "paragraphs": [
        "Creates a new UDF (user-defined function). Depending on how you configure it, the function can\nreturn either scalar results or tabular results.",
        "When you create a UDF, you specify a handler whose code is written in one of the supported languages. Depending on the handler\u2019s language,\nyou can either include the handler source code in-line with the CREATE FUNCTION statement or reference the handler\u2019s location from\nCREATE FUNCTION, where the handler is precompiled or source code on a stage.",
        "The following table lists each of the supported languages and whether its code may be kept in-line with CREATE FUNCTION or kept on a stage.\nFor more information, see Keeping handler code in-line or on a stage.",
        "Language",
        "Handler Location",
        "Java",
        "In-line or staged",
        "JavaScript",
        "In-line",
        "Python",
        "In-line or staged",
        "Scala",
        "In-line or staged",
        "SQL",
        "In-line",
        "This command supports the following variants:",
        "CREATE OR ALTER FUNCTION: Creates a function if it doesn\u2019t exist or alters an existing function.",
        "ALTER FUNCTION, DROP FUNCTION, SHOW USER FUNCTIONS , DESCRIBE FUNCTION, CREATE OR ALTER <object>",
        "The syntax for CREATE FUNCTION varies depending on which language you\u2019re using as the UDF handler.",
        "Use the syntax below if the source code is in-line:",
        "Use the following syntax if the handler code will be referenced on a stage (such as in a JAR):",
        "Preview Feature \u2014 Open",
        "Using Python to write a handler for a user-defined aggregate function (UDAF) with the AGGREGATE parameter is a preview feature that is\navailable to all accounts.",
        "Use the syntax below if the source code is in-line:",
        "Use the following syntax if the handler code will be referenced on a stage (such as in a module):",
        "Preview Feature \u2014 Open",
        "Using Scala to write a handler for a user-defined function (UDF) is a preview feature that is available to all accounts. Note that\nreturning a TABLE is not yet available for Scala UDF handlers.",
        "Use the syntax below if the source code is in-line:",
        "Use the following syntax if the handler code will be referenced on a stage (such as in a JAR):",
        "Preview Feature \u2014 Open",
        "Available to all accounts.",
        "Creates a new function if it doesn\u2019t already exist, or transforms an existing function into the function defined in the statement.\nA CREATE OR ALTER FUNCTION statement follows the syntax rules of a CREATE FUNCTION statement and has the same limitations as an\nALTER FUNCTION statement.",
        "Supported function alterations include:",
        "Change function properties and parameters. For example, SECURE, MAX_BATCH_ROWS, LOG_LEVEL, or COMMENT.",
        "For more information, see CREATE OR ALTER FUNCTION usage notes.",
        "Note",
        "The COPY GRANTS parameter is not supported with this variant syntax.",
        "Specifies the identifier (name), any input arguments, and the default values for any optional arguments for the UDF.",
        "For the identifier:",
        "The identifier does not need to be unique for the schema in which the function is created because UDFs are\nidentified and resolved by the combination of the name and argument types.",
        "The identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (for example, \u201cMy object\u201d). Identifiers enclosed in double quotes are also\ncase-sensitive. See Identifier requirements.",
        "For the input arguments:",
        "For arg_name, specify the name of the input argument.",
        "For arg_data_type, use the Snowflake data type that corresponds to the handler language that you are using.",
        "For Java handlers, see SQL-Java Data Type Mappings.",
        "For JavaScript handlers, see SQL and JavaScript data type mapping.",
        "For Python handlers, see SQL-Python Data Type Mappings.",
        "For Scala handlers, see SQL-Scala Data Type Mappings.",
        "To indicate that an argument is optional, use DEFAULT default_value to specify the default value of the argument.\nFor the default value, you can use a literal or an expression.",
        "If you specify any optional arguments, you must place these after the required arguments.",
        "If a function has optional arguments, you cannot define additional functions with the same name and different signatures.",
        "For details, see Specify optional arguments.",
        "Specifies the results returned by the UDF, which determines the UDF type:",
        "result_data_type: Creates a scalar UDF that returns a single value with the specified data type.",
        "Note",
        "For UDF handlers written in Java, Python, or Scala, the result_data_type must be in the SQL Data Type column of the\nfollowing table corresponding to the handler language:",
        "SQL-Java Type Mappings table",
        "SQL-Python Type Mappings table",
        "SQL-Scala Type Mappings table",
        "TABLE ( col_name col_data_type , ... ): Creates a table UDF that returns tabular results with the specified table column(s)\nand column type(s).",
        "Note",
        "For Scala UDFs, the TABLE return type is not supported.",
        "Defines the handler code executed when the UDF is called. The function_definition value must be source code in one of the\nlanguages supported for handlers. The code may be:",
        "Java. For more information, see Introduction to Java UDFs.",
        "JavaScript. For more information, see Introduction to JavaScript UDFs.",
        "Python. For more information, see Introduction to Python UDFs.",
        "Scala. For more information, see Introduction to Scala UDFs.",
        "A SQL expression. For more information, see Introduction to SQL UDFs.",
        "For more details, see General usage notes (in this topic).",
        "Note",
        "The AS clause is not required when the UDF handler code is referenced on a stage with the IMPORTS clause.",
        "Specifies that the code is in the Java language.",
        "Specifies the Java JDK runtime version to use. The supported versions of Java are:",
        "11.x",
        "17.x",
        "If RUNTIME_VERSION is not set, Java JDK 11 is used.",
        "The location (stage), path, and name of the file(s) to import.",
        "A file can be a JAR file or another type of file.",
        "If the file is a JAR file, it can contain one or more .class files and zero or more resource files.",
        "JNI (Java Native Interface) is not supported. Snowflake prohibits loading libraries that contain native code (as opposed to Java\nbytecode).",
        "Java UDFs can also read non-JAR files. For an example, see Reading a file specified statically in IMPORTS.",
        "If you plan to copy a file (JAR file or other file) to a stage, then Snowflake recommends using a named internal stage because the\nPUT command supports copying files to named internal stages, and the PUT command is usually the easiest way to move a JAR file\nto a stage.",
        "External stages are allowed, but are not supported by PUT.",
        "Each file in the IMPORTS clause must have a unique name, even if the files are in different subdirectories or different stages.",
        "If both the IMPORTS and TARGET_PATH clauses are present, the file name in the TARGET_PATH clause must be different\nfrom each file name in the IMPORTS clause, even if the files are in different subdirectories or different stages.",
        "Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an existing file.",
        "For a UDF whose handler is on a stage, the IMPORTS clause is required because it specifies the location of the JAR file that\ncontains the UDF.",
        "For UDF whose handler code is in-line, the IMPORTS clause is needed only if the in-line UDF needs to access other files, such as\nlibraries or text files.",
        "For Snowflake system packages, such the Snowpark package,\nyou can specify the package with the PACKAGES clause rather than specifying its JAR file with IMPORTS. When you do, the package\nJAR file need not be included in an IMPORTS value.",
        "In-line Java",
        "In-line Java UDFs require a function definition.",
        "The name of the handler method or class.",
        "If the handler is for a scalar UDF, returning a non-tabular value, the HANDLER value should be a method name, as in the following\nform: MyClass.myMethod.",
        "If the handler is for a tabular UDF, the HANDLER value should be the name of a handler class.",
        "Specifies that the code is in the JavaScript language.",
        "Specifies that the code is in the Python language.",
        "Specifies the Python version to use. The supported versions of Python are:",
        "3.9",
        "3.10",
        "3.11",
        "The location (stage), path, and name of the file(s) to import.",
        "A file can be a .py file or another type of file.",
        "Python UDFs can also read non-Python files, such as text files. For an example, see Reading a file.",
        "If you plan to copy a file to a stage, then Snowflake recommends using a named internal stage because the\nPUT command supports copying files to named internal stages, and the PUT command is usually the easiest way to move a file\nto a stage.",
        "External stages are allowed, but are not supported by PUT.",
        "Each file in the IMPORTS clause must have a unique name, even if the files are in different subdirectories or different stages.",
        "When the handler code is stored in a stage, you must use the IMPORTS clause to specify the handler code\u2019s location.",
        "For an in-line Python UDF, the IMPORTS clause is needed only if the UDF handler needs to access other files, such as\npackages or text files.",
        "For packages included on the Snowflake system, such numpy,\nyou can specify the package with the PACKAGES clause alone, omitting the package\u2019s source as an IMPORTS value.",
        "The name of the handler function or class.",
        "If the handler is for a scalar UDF, returning a non-tabular value, the HANDLER value should be a function name. If the handler code\nis in-line with the CREATE FUNCTION statement, you can use the function name alone. When the handler code is referenced at a stage, this\nvalue should be qualified with the module name, as in the following form: my_module.my_function.",
        "If the handler is for a tabular UDF, the HANDLER value should be the name of a handler class.",
        "Specifies that the code is in the Scala language.",
        "Specifies the Scala runtime version to use. The supported versions of Scala are:",
        "2.12",
        "If RUNTIME_VERSION is not set, Scala 2.12 is used.",
        "The location (stage), path, and name of the file(s) to import, such as a JAR or other kind of file.",
        "The JAR file might contain handler dependency libraries. It can contain one or more .class files and zero or more resource files.",
        "JNI (Java Native Interface) is not supported. Snowflake prohibits loading libraries that contain native code (as opposed to Java\nbytecode).",
        "A non-JAR file might a file read by handler code. For an example, see Reading a file specified statically in IMPORTS.",
        "If you plan to copy a file to a stage, then Snowflake recommends using a named internal stage because the PUT command supports\ncopying files to named internal stages, and the PUT command is usually the easiest way to move a JAR file to a stage. External\nstages are allowed, but are not supported by PUT.",
        "Each file in the IMPORTS clause must have a unique name, even if the files are in different stage subdirectories or different stages.",
        "If both the IMPORTS and TARGET_PATH clauses are present, the file name in the TARGET_PATH clause must be different\nfrom that of any file listed in the IMPORTS clause, even if the files are in different stage subdirectories or different stages.",
        "For a UDF whose handler is on a stage, the IMPORTS clause is required because it specifies the location of the JAR file that\ncontains the UDF.",
        "For UDF whose handler code is in-line, the IMPORTS clause is needed only if the in-line UDF needs to access other files, such as\nlibraries or text files.",
        "For Snowflake system packages, such the Snowpark package,\nyou can specify the package with the PACKAGES clause rather than specifying its JAR file with IMPORTS. When you do, the package\nJAR file need not be included in an IMPORTS value.",
        "In-line Scala",
        "UDFs with in-line Scala handler code require a function definition.",
        "The name of the handler method or class.",
        "If the handler is for a scalar UDF, returning a non-tabular value, the HANDLER value should be a method name, as in the following\nform: MyClass.myMethod.",
        "Specifies that the function is secure. For more information about secure functions, see Protecting Sensitive Information with Secure UDFs and Stored Procedures.",
        "Specifies that the function persists only for the duration of the session that you created it in. A\ntemporary function is dropped at the end of the session.",
        "Default: No value. If a function is not declared as TEMPORARY, the function is permanent.",
        "You cannot create temporary user-defined functions that have the same name as a function that already\nexists in the schema.",
        "Specifies whether the function can return NULL values or must return only NON-NULL values. The default is NULL (i.e. the function can\nreturn NULL).",
        "Note",
        "Currently, the NOT NULL clause is not enforced for SQL UDFs.\nSQL UDFs declared as NOT NULL can return NULL values. Snowflake recommends avoiding NOT NULL\nfor SQL UDFs unless the code in the function is written to ensure that NULL values are never returned.",
        "Specifies the behavior of the UDF when called with null inputs. In contrast to system-defined functions, which always return null when any\ninput is null, UDFs can handle null inputs, returning non-null values even when an input is null:",
        "CALLED ON NULL INPUT will always call the UDF with null inputs. It is up to the UDF to handle such values appropriately.",
        "RETURNS NULL ON NULL INPUT (or its synonym STRICT) will not call the UDF if any input is null. Instead, a null value\nwill always be returned for that row. Note that the UDF might still return null for non-null inputs.",
        "Note",
        "RETURNS NULL ON NULL INPUT (STRICT) is not supported for SQL UDFs. SQL UDFs effectively use\nCALLED ON NULL INPUT. In your SQL UDFs, you must handle null input values.",
        "Default: CALLED ON NULL INPUT",
        "Specifies the behavior of the UDF when returning results:",
        "VOLATILE: UDF might return different values for different rows, even for the same input (e.g. due to non-determinism and\nstatefullness).",
        "IMMUTABLE: UDF assumes that the function, when called with the same inputs, will always return the same result. This guarantee\nis not checked. Specifying IMMUTABLE for a UDF that returns different values for the same input will result in undefined\nbehavior.",
        "Default: VOLATILE",
        "Note",
        "IMMUTABLE is not supported on an aggregate function (when you use the AGGREGATE parameter). Therefore, all aggregate functions are\nVOLATILE by default.",
        "Specifies a comment for the UDF, which is displayed in the DESCRIPTION column in the SHOW FUNCTIONS and SHOW USER FUNCTIONS\noutput.",
        "Default: user-defined function",
        "Specifies to retain the access privileges from the original function when a new function is created using CREATE OR REPLACE FUNCTION.",
        "The parameter copies all privileges, except OWNERSHIP, from the existing function to the new function. The new function will\ninherit any future grants defined for the object type in the schema. By default, the role that executes the CREATE FUNCTION\nstatement owns the new function.",
        "Note:",
        "With data sharing, if the existing function was shared to another account, the replacement function is\nalso shared.",
        "The SHOW GRANTS output for the replacement function lists the grantee for the copied privileges as the\nrole that executed the CREATE FUNCTION statement, with the current timestamp when the statement was executed.",
        "The operation to copy grants occurs atomically in the CREATE FUNCTION command (i.e. within the same transaction).",
        "The name and version number of Snowflake system packages required as dependencies. The value should be of the form\npackage_name:version_number, where package_name is snowflake_domain:package. Note that you can\nspecify latest as the version number in order to have Snowflake use the latest version available on the system.",
        "For example:",
        "You can discover the list of supported system packages by executing the following SQL in Snowflake:",
        "For a dependency you specify with PACKAGES, you do not need to also specify its JAR file in an IMPORTS clause.",
        "In-line Java",
        "The TARGET_PATH clause specifies the location to which Snowflake should write the compiled code (JAR file) after compiling\nthe source code specified in the function_definition.",
        "If this clause is included, the user should manually remove the JAR file when it is no longer needed (typically when the\nJava UDF is dropped).",
        "If this clause is omitted, Snowflake re-compiles the source code each time the code is needed. The JAR file is not stored\npermanently, and the user does not need to clean up the JAR file.",
        "Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file.",
        "The names of external access integrations needed in order for this\nfunction\u2019s handler code to access external networks.",
        "An external access integration specifies network rules and\nsecrets that specify external locations and credentials (if any) allowed for use by handler code\nwhen making requests of an external network, such as an external REST API.",
        "Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code.",
        "Secrets you specify here must be allowed by the external access integration\nspecified as a value of this CREATE FUNCTION command\u2019s EXTERNAL_ACCESS_INTEGRATIONS parameter",
        "This parameter\u2019s value is a comma-separated list of assignment expressions with the following parts:",
        "secret_name as the name of the allowed secret.",
        "You will receive an error if you specify a SECRETS value whose secret isn\u2019t also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter.",
        "'secret_variable_name' as the variable that will be used in handler code when retrieving information from the secret.",
        "For more information, including an example, refer to Using the external access integration in a function or procedure.",
        "Specifies that the function is an aggregate function. For more information about user-defined aggregate functions, see\nPython user-defined aggregate functions.",
        "Preview Feature \u2014 Open",
        "Using Python to write a handler for a user-defined aggregate function (UDAF) is a preview feature that is available to all accounts.",
        "Note",
        "IMMUTABLE is not supported on an aggregate function (when you use the AGGREGATE parameter). Therefore, all aggregate functions are\nVOLATILE by default.",
        "The name and version number of packages required as dependencies. The value should be of the form\npackage_name==version_number. If you omit the version number, Snowflake will use the latest package available on the\nsystem.",
        "For example:",
        "You can discover the list of supported system packages by executing the following SQL in Snowflake:",
        "For more information about included packages, see Using third-party packages.",
        "Preview Feature \u2014 Open",
        "Specifying a range of Python package versions is available as a preview feature to all accounts.",
        "You can specify package versions by using these version\nspecifiers: ==, <=, >=, <,or >.",
        "For example:",
        "The names of external access integrations needed in order for this\nfunction\u2019s handler code to access external networks.",
        "An external access integration specifies network rules and\nsecrets that specify external locations and credentials (if any) allowed for use by handler code\nwhen making requests of an external network, such as an external REST API.",
        "Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code.",
        "Secrets you specify here must be allowed by the external access integration\nspecified as a value of this CREATE FUNCTION command\u2019s EXTERNAL_ACCESS_INTEGRATIONS parameter",
        "This parameter\u2019s value is a comma-separated list of assignment expressions with the following parts:",
        "secret_name as the name of the allowed secret.",
        "You will receive an error if you specify a SECRETS value whose secret isn\u2019t also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter.",
        "'secret_variable_name' as the variable that will be used in handler code when retrieving information from the secret.",
        "For more information, including an example, refer to Using the external access integration in a function or procedure.",
        "Specifies that the function is memoizable.",
        "For details, see Memoizable UDFs.",
        "The name and version number of Snowflake system packages required as dependencies. The value should be of the form\npackage_name:version_number, where package_name is snowflake_domain:package. Note that you can\nspecify latest as the version number in order to have Snowflake use the latest version available on the system.",
        "For example:",
        "You can discover the list of supported system packages by executing the following SQL in Snowflake:",
        "For a dependency you specify with PACKAGES, you do not need to also specify its JAR file in an IMPORTS clause.",
        "In-line Scala",
        "The TARGET_PATH clause specifies the location to which Snowflake should write the compiled code (JAR file) after compiling\nthe source code specified in the function_definition.",
        "If this clause is included, you should manually remove the JAR file when it is no longer needed (typically when the\nUDF is dropped).",
        "If this clause is omitted, Snowflake re-compiles the source code each time the code is needed. The JAR file is not stored\npermanently, and you do not need to clean up the JAR file.",
        "Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file.",
        "A role used to execute this SQL command must have the following\nprivileges at a minimum:",
        "Privilege",
        "Object",
        "Notes",
        "CREATE FUNCTION",
        "Schema",
        "The privilege only enables the creation of user-defined functions in the schema.",
        "If you want to enable the creation of data metric functions, the role must have the CREATE DATA METRIC FUNCTION privilege.",
        "USAGE",
        "Function",
        "Granting the USAGE privilege on the newly created function to a role allows users with that role to call the function elsewhere in\nSnowflake (e.g. masking policy owner role for External Tokenization).",
        "USAGE",
        "External access integration",
        "Required on integrations, if any, specified by the EXTERNAL_ACCESS_INTEGRATIONS parameter. For more information, see\nCREATE EXTERNAL ACCESS INTEGRATION.",
        "READ",
        "Secret",
        "Required on secrets, if any, specified by the SECRETS parameter. For more information, see\nCreating a secret to represent credentials and Using the external access integration in a function or procedure.",
        "USAGE",
        "Schema",
        "Required on schemas containing secrets, if any, specified by the SECRETS parameter. For more information,\nsee Creating a secret to represent credentials and Using the external access integration in a function or procedure.",
        "The USAGE privilege on the parent database and schema are required to perform operations on any object in a schema.",
        "For instructions on creating a custom role with a specified set of privileges, see Creating custom roles.",
        "For general information about roles and privilege grants for performing SQL actions on\nsecurable objects, see Overview of Access Control.",
        "function_definition has size restrictions. The maximum allowable size is subject to change.",
        "The delimiters around the function_definition can be either single quotes or a pair of dollar signs.",
        "Using $$ as the delimiter makes it easier to write functions that contain single quotes.",
        "If the delimiter for the body of the function is the single quote character,\nthen any single quotes within function_definition (e.g. string\nliterals) must be escaped by single quotes.",
        "If using a UDF in a masking policy, ensure the data type of the column, UDF, and masking policy match. For\nmore information, see User-defined functions in a masking policy.",
        "If you specify the CURRENT_DATABASE or CURRENT_SCHEMA function in the\nhandler code of the UDF, the function returns the database or schema that contains the UDF, not the database or schema in use for\nthe session.",
        "Regarding metadata:",
        "Attention",
        "Customers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.",
        "CREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.",
        "In Java, primitive data types don\u2019t allow NULL values, so passing a NULL for an argument of such a type results in\nan error.",
        "In the HANDLER clause, the method name is case-sensitive.",
        "In the IMPORTS and TARGET_PATH clauses:",
        "Package, class, and file name(s) are case-sensitive.",
        "Stage name(s) are case-insensitive.",
        "You can use the PACKAGES clause to specify package name and version number for Snowflake system-defined dependencies, such as those\nfrom Snowpark. For other dependencies, specify dependency JAR files with the IMPORTS clause.",
        "Snowflake validates that:",
        "The JAR file specified in the CREATE FUNCTION statement\u2019s HANDLER exists and contains the specified\nclass and method.",
        "The input and output types specified in the UDF declaration are compatible with the input and output types\nof the Java method.",
        "Validation can be done at creation time or execution time.",
        "If the user is connected to an active Snowflake warehouse at the time the CREATE FUNCTION statement is\nexecuted, then the UDF is validated at creation time.",
        "Otherwise, the UDF is created, but is not validated immediately, and Snowflake returns the following message:\nFunction <name> created successfully, but could not be validated since there is no active warehouse.",
        "Snowflake does not validate JavaScript code at UDF creation time (i.e. creation of the UDF succeeds regardless of whether the code is\nvalid). If the code is not valid, errors will be returned when the UDF is called at query time.",
        "In the HANDLER clause, the handler function name is case-sensitive.",
        "In the IMPORTS clause:",
        "File name(s) are case-sensitive.",
        "Stage name(s) are case-insensitive.",
        "You can use the PACKAGES clause to specify package name and version number for dependencies, such as those\nfrom Snowpark. For other dependencies, specify dependency files with the IMPORTS clause.",
        "Snowflake validates that:",
        "The function or class specified in the CREATE FUNCTION statement\u2019s HANDLER exists.",
        "The input and output types specified in the UDF declaration are compatible with the input and output types\nof the handler.",
        "In the HANDLER clause, the method name is case-sensitive.",
        "In the IMPORTS and TARGET_PATH clauses:",
        "Package, class, and file name(s) are case-sensitive.",
        "Stage name(s) are case-insensitive.",
        "You can use the PACKAGES clause to specify package name and version number for Snowflake system-defined dependencies, such as those\nfrom Snowpark. For other dependencies, specify dependency JAR files with the IMPORTS clause.",
        "Snowflake validates that:",
        "The JAR file specified in the CREATE FUNCTION statement\u2019s HANDLER exists and contains the specified\nclass and method.",
        "The input and output types specified in the UDF declaration are compatible with the input and output types\nof the Scala method.",
        "Validation can be done at creation time or execution time.",
        "If the user is connected to an active Snowflake warehouse at the time the CREATE FUNCTION statement is\nexecuted, then the UDF is validated at creation time.",
        "Otherwise, the UDF is created, but is not validated immediately, and Snowflake returns the following message:\nFunction <name> created successfully, but could not be validated since there is no active warehouse.",
        "Currently, the NOT NULL clause is not enforced for SQL UDFs.",
        "Preview Feature \u2014 Open",
        "Available to all accounts.",
        "All limitations of the ALTER FUNCTION command apply.",
        "You cannot replace or transform a FUNCTION with a PROCEDURE or a PROCEDURE with a FUNCTION.",
        "You cannot replace or transform a temporary FUNCTION with a non-temporary FUNCTION, or a non-temporary FUNCTION with a temporary FUNCTION.",
        "You cannot replace or transform a regular FUNCTION with an EXTERNAL FUNCTION, or an EXTERNAL FUNCTION with a regular FUNCTION.",
        "Changing the LANGUAGE, IMPORTS, RETURNS, HANDLER, RUNTIME_VERSION,  PACKAGES, VOLATILITY, NULL_HANDLING, and TARGET_PATH properties is not supported.",
        "Setting or unsetting a tag is not supported. Existing tags are not altered by a CREATE OR ALTER FUNCTION statement and remain unchanged.",
        "Here is a basic example of CREATE FUNCTION with an in-line handler:",
        "Here is a basic example of CREATE FUNCTION with a reference to a staged handler:",
        "For more examples of Java UDFs, see examples.",
        "Create a JavaScript UDF named js_factorial:",
        "Code in the following example creates a py_udf function whose handler code is in-line as udf.",
        "Code in the following example creates a dream function whose handler is in a sleepy.py file located on the\n@my_stage stage.",
        "Here is a basic example of CREATE FUNCTION with an in-line handler:",
        "Here is a basic example of CREATE FUNCTION with a reference to a staged handler:",
        "For more examples of Scala UDFs, see Scala UDF handler examples.",
        "Create a simple SQL scalar UDF that returns a hard-coded approximation of the\nmathematical constant pi:",
        "Create a simple SQL table UDF that returns hard-coded values:",
        "Output:",
        "Create a UDF that accepts multiple parameters:",
        "Create a SQL table UDF named get_countries_for_user that returns the results of a query:",
        "Create a function multiply that accepts two numbers:",
        "Alter multiply to add a comment and make the function secure:",
        "Was this page helpful?",
        "On this page"
    ]
}