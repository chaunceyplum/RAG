{
    "url": "https://docs.snowflake.com/en/developer-guide/snowpark/java/working-with-dataframes.html#label-snowpark-java-dataframe-stages-copy-into-location",
    "title": "Working with DataFrames in Snowpark Java | Snowflake Documentation",
    "paragraphs": [
        "In Snowpark, the main way in which you query and process data is through a DataFrame. This topic explains how to work with\nDataFrames.",
        "To retrieve and manipulate data, you use the DataFrame class. A DataFrame represents a relational dataset that is evaluated\nlazily: it only executes when a specific action is triggered. In a sense, a DataFrame is like a query that needs to be evaluated\nin order to retrieve data.",
        "To retrieve data into a DataFrame:",
        "Construct a DataFrame, specifying the source of the data for the dataset.",
        "For example, you can create a DataFrame to hold data from a table, an external CSV file, or the execution of a SQL statement.",
        "Specify how the dataset in the DataFrame should be transformed.",
        "For example, you can specify which columns should be selected, how the rows should be filtered, how the results should be\nsorted and grouped, etc.",
        "Execute the statement to retrieve the data into the DataFrame.",
        "In order to retrieve the data into the DataFrame, you must invoke a method that performs an action (for example, the\ncollect() method).",
        "The next sections explain these steps in more detail.",
        "Some of the examples of this section use a DataFrame to query a table named sample_product_data. If you want to run these\nexamples, you can create this table and fill the table with some data by executing the following SQL statements:",
        "To verify that the table was created, run:",
        "To construct a DataFrame, you can use methods in the Session class. Each of the following methods constructs a DataFrame\nfrom a different type of data source:",
        "To create a DataFrame from data in a table, view, or stream, call the table method:",
        "Note",
        "The table method returns an Updatable object. Updatable extends DataFrame and provides\nadditional methods for working with data in the table (e.g. methods for updating and deleting data). See\nUpdating, Deleting, and Merging Rows in a Table.",
        "To create a DataFrame from specified values:",
        "Construct an array of Row objects that contain the values.",
        "Construct a StructType object that describes the data types of those values.",
        "Call the createDataFrame method, passing in the array and StructType object.",
        "Note",
        "Words reserved by Snowflake are not valid as column names when constructing a DataFrame. For a list of reserved words, refer to\nReserved & limited keywords.",
        "To create a DataFrame containing a range of values, call the range method:",
        "To create a DataFrame for a file in a stage, call read to get a\nDataFrameReader object. In the DataFrameReader object, call the method corresponding to the format of the data\nin the file:",
        "To create a DataFrame to hold the results of a SQL query, call the sql method:",
        "Note: Although you can use this method to execute SELECT statements that retrieve data from tables and staged files, you should\nuse the table and read methods instead. Methods like table and read can provide better syntax\nhighlighting, error highlighting, and intelligent code completion in development tools.",
        "To specify which columns should be selected and how the results should be filtered, sorted, grouped, etc., call the DataFrame\nmethods that transform the dataset. To identify columns in these methods, use the Functions.col static method or an\nexpression that evaluates to a column. (See Specifying Columns and Expressions.)",
        "For example:",
        "To specify which rows should be returned, call the filter method:",
        "To specify the columns that should be selected, call the select method:",
        "Each method returns a new DataFrame object that has been transformed. (The method does not affect the original DataFrame object.)\nThis means that if you want to apply multiple transformations, you can\nchain method calls, calling each subsequent transformation method\non the new DataFrame object returned by the previous method call.",
        "Note that these transformation methods do not retrieve data from the Snowflake database. (The action methods described in\nPerforming an Action to Evaluate a DataFrame perform the data retrieval.) The transformation methods simply specify how\nthe SQL statement should be constructed.",
        "When calling these transformation methods, you might need to specify columns or expressions that use columns. For example, when\ncalling the select method, you need to specify the columns that should be selected.",
        "To refer to a column, create a Column object by calling the Functions.col static method.",
        "Note",
        "To create a Column object for a literal, see Using Literals as Column Objects.",
        "When specifying a filter, projection, join condition, etc., you can use Column objects in an expression. For example:",
        "You can use Column objects with the filter method to specify a filter condition:",
        "You can use Column objects with the select method to define an alias:",
        "You can use Column objects with the join method to define a join condition:",
        "When referring to columns in two different DataFrame objects that have the same name (for example, joining the DataFrames on that\ncolumn), you can use the col method in each DataFrame object to refer to a column in that object (for example,\ndf1.col(\"name\") and df2.col(\"name\")).",
        "The following example demonstrates how to use the col method to refer to a column in a specific DataFrame. The example\njoins two DataFrame objects that both have a column named value. The example uses the as method of the Column\nobject to change the names of the columns in the newly created DataFrame.",
        "The names of databases, schemas, tables, and stages that you specify must conform to the\nSnowflake identifier requirements. When you specify a name, Snowflake considers the\nname to be in upper case. For example, the following calls are equivalent:",
        "If the name does not conform to the identifier requirements, you must use double quotes (\") around the name. Use a backslash\n(\\) to escape the double quote character within a Scala string literal. For example, the following table name does not start\nwith a letter or an underscore, so you must use double quotes around the name:",
        "Note that when specifying the name of a column, you don\u2019t need to use double quotes around the name. The Snowpark library\nautomatically encloses the column name in double quotes for you if the name does not comply with the identifier requirements:.",
        "If you have already added double quotes around a column name, the library does not insert additional double quotes around the\nname.",
        "In some cases, the column name might contain double quote characters:",
        "As explained in Identifier requirements, for each double quote character within a double-quoted identifier, you\nmust use two double quote characters (e.g. \"name_with_\"\"air\"\"_quotes\" and \"\"\"column_name_quoted\"\"\"):",
        "Keep in mind that when an identifier is enclosed in double quotes (whether you explicitly added the quotes or the library added\nthe quotes for you), Snowflake treats the identifier as case-sensitive:",
        "To use a literal in a method that passes in a Column object, create a Column object for the literal by passing\nthe literal to the lit static method in the Functions class. For example:",
        "If the literal is a floating point or double value in Java (e.g. 0.05 is treated as a Double by default), the Snowpark library\ngenerates SQL that implicitly casts the value to the corresponding Snowpark data type (e.g. 0.05::DOUBLE). This can produce\nan approximate value that differs from the exact number specified.",
        "For example, the following code displays no matching rows, even though the filter (that matches values greater than or equal to\n0.05) should match the rows in the DataFrame:",
        "The problem is that Functions.lit(0.06) and Functions.lit(0.01) produce approximate values for 0.06 and 0.01,\nnot the exact values.",
        "To avoid this problem, cast the literal to the Snowpark type that you want to\nuse. For example, to use a NUMBER with a precision of 5 and a scale of 2:",
        "To cast a Column object to a specific type, call the cast method, and pass in a type object from the\ncom.snowflake.snowpark_java.types package. For example, to cast a literal as a NUMBER with a precision\nof 5 and a scale of 2:",
        "Because each method that transforms a DataFrame object returns a new DataFrame\nobject that has the transformation applied, you can chain method calls to\nproduce a new DataFrame that is transformed in additional ways.",
        "The following example returns a DataFrame that is configured to:",
        "Query the sample_product_data table.",
        "Return the row with id = 1.",
        "Select the name and serial_number columns.",
        "In this example:",
        "session.table(\"sample_product_data\") returns a DataFrame for the sample_product_data table.",
        "Although the DataFrame does not yet contain the data from the table, the object does contain the definitions of the columns in\nthe table.",
        "filter(Functions.col(\"id\").equal_to(Functions.lit(1))) returns a DataFrame for the sample_product_data table that is\nset up to return the row with id = 1.",
        "Note again that the DataFrame does not yet contain the matching row from the table. The matching row is not retrieved until you\ncall an action method.",
        "select(Functions.col(\"name\"), Functions.col(\"serial_number\")) returns a DataFrame that contains the name and\nserial_number columns for the row in the sample_product_data table that has id = 1.",
        "When you chain method calls, keep in mind that the order of calls is important. Each method call returns a DataFrame that has been\ntransformed. Make sure that subsequent calls work with the transformed DataFrame.",
        "For example, in the code below, the select method returns a DataFrame that just contains two columns: name and\nserial_number. The filter method call on this DataFrame fails because it uses the id column, which is not in the\ntransformed DataFrame.",
        "In contrast, the following code executes successfully because the filter() method is called on a DataFrame that contains\nall of the columns in the sample_product_data table (including the id column):",
        "Keep in mind that you might need to make the select and filter method calls in a different order than you would\nuse the equivalent keywords (SELECT and WHERE) in a SQL statement.",
        "To limit the number of rows in a DataFrame, you can use the limit transformation method.",
        "The Snowpark API also provides action methods for retrieving and printing out a limited number of rows:",
        "the first action method (to execute the query and return the first n rows)",
        "the show action method (to execute the query and print the first n rows)",
        "These methods effectively add a LIMIT clause to the SQL statement that is executed.",
        "As explained in the usage notes for LIMIT, the results are non-deterministic unless you\nspecify a sort order (ORDER BY) in conjunction with LIMIT.",
        "To keep the ORDER BY clause with the LIMIT clause (e.g. so that ORDER BY is not in a separate subquery), you must call the method\nthat limits results on the DataFrame returned by the sort method.",
        "For example, if you are chaining method calls:",
        "To retrieve the definition of the columns in the dataset for the DataFrame, call the schema method. This method returns\na StructType object that contains an Array of StructField objects. Each StructField object\ncontains the definition of a column.",
        "In the returned StructType object, the column names are always normalized. Unquoted identifiers are returned in uppercase,\nand quoted identifiers are returned in the exact case in which they were defined.",
        "The following example creates a DataFrame containing the columns named ID and 3rd. For the column name 3rd, the\nSnowpark library automatically encloses the name in double quotes (\"3rd\") because\nthe name does not comply with the requirements for an identifier.",
        "The example calls the schema method and then calls the names method on the returned StructType object to\nget an array of column names. The names are normalized in the StructType returned by the schema method.",
        "To join DataFrame objects, call the join method.",
        "The following sections explain how to use DataFrames to perform a join:",
        "Setting up the Sample Data for the Joins",
        "Specifying the Columns for the Join",
        "Performing a Natural Join",
        "Specifying the Type of Join",
        "Joining Multiple Tables",
        "Performing a Self-Join",
        "The examples in the next sections use sample data that you can set up by executing the following SQL statements:",
        "With the DataFrame.join method, you can specify the columns to use in one of the following ways:",
        "Specify a Column expression that describes the join condition.",
        "Specify one or more columns that should be used as the common columns in the join.",
        "The following example performs an inner join on the column named id_a:",
        "Note that the example uses the DataFrame.col method to specify the condition to use for the join. See\nSpecifying Columns and Expressions for more about this method.",
        "This prints the following output:",
        "In the DataFrame resulting from a join, the Snowpark library uses the column names found in the tables that were joined even when the\ncolumn names are identical across tables. When this happens, these column names are duplicated in the DataFrame resulting from the join.\nTo access a duplicated column by name, call the col method on the DataFrame representing the column\u2019s original table. (For more\ninformation about specifying columns, see Referring to Columns in Different DataFrames.)",
        "Code in the following example joins two DataFrames, then calls the select method on the joined DataFrame. It specifies the columns\nto select by calling the col method from the variable representing the respective DataFrame objects: dfRhs and\ndfLhs. It uses the as method to give the columns new names in the DataFrame that the select method creates.",
        "This prints the following output:",
        "Note that when a DataFrame resulting from a join includes duplicate column names, you must deduplicate or rename columns to remove\nduplication in the DataFrame before you save the result to a table or cache the DataFrame. For duplicate column names in a DataFrame that\nyou save to a table or cache, the Snowpark library will replace duplicate column names with aliases so that they\u2019re no longer duplicated.",
        "The following example illustrates how the output of a cached DataFrame might appear if column names ID_A and VALUE were\nduplicated in a join from two tables, then not deduplicated or renamed prior to caching the result.",
        "To perform a natural join (where DataFrames are joined on columns that have the same name),\ncall the naturalJoin method.",
        "The following example joins the DataFrames for the tables sample_a and sample_b on their common columns (the column\nid_a):",
        "This prints the following output:",
        "By default, the DataFrame.join method creates an inner join. To specify a different type of join, set the\njoinType argument to one of the following values:",
        "Type of Join",
        "joinType",
        "Inner join",
        "inner (default)",
        "Left outer join",
        "left",
        "Right outer join",
        "right",
        "Full outer join",
        "full",
        "Cross join",
        "cross",
        "For example:",
        "This prints the following output:",
        "To join multiple tables:",
        "Create a DataFrame for each table.",
        "Call the DataFrame.join method on the first DataFrame, passing in the second DataFrame.",
        "Using the DataFrame returned by the join method, call the join method, passing in the third DataFrame.",
        "You can chain the join calls as shown below:",
        "This prints the following output:",
        "If you need to join a table with itself on different columns, you cannot perform the self-join with a single DataFrame. The\nfollowing examples that use a single DataFrame to perform a self-join fail because the column expressions for \"id\" are\npresent in the left and right sides of the join:",
        "Both of these examples fail with the following exception:",
        "Instead, use the clone method to create a clone of the DataFrame object, and use the two DataFrame objects to perform the join:",
        "If you want to perform a self-join on the same column, call the join method that passes in the name of the column (or an\narray of column names) for the USING clause:",
        "As mentioned earlier, the DataFrame is lazily evaluated, which means the SQL statement isn\u2019t sent to the server for execution\nuntil you perform an action. An action causes the DataFrame to be evaluated and sends the corresponding SQL statement to the\nserver for execution.",
        "The following sections explain how to perform an action synchronously and asynchronously on a DataFrame:",
        "Performing an Action Synchronously",
        "Performing an Action Asynchronously",
        "To perform an action synchronously, call one of the following action methods:",
        "Method to Perform an Action Synchronously",
        "Description",
        "DataFrame.collect()",
        "Evaluates the DataFrame and returns the resulting dataset as an Array of Row objects. See Returning All Rows.",
        "DataFrame.toLocalIterator()",
        "Evaluates the DataFrame and returns an Iterator  of Row objects. If the result set is large, use this method to avoid loading all the results into memory at once. See Returning an Iterator for the Rows.",
        "DataFrame.count()",
        "Evaluates the DataFrame and returns the number of rows.",
        "DataFrame.show()",
        "Evaluates the DataFrame and prints the rows to the console. Note that this method limits the number of rows to 10 (by default). See Printing the Rows in a DataFrame.",
        "DataFrame.cacheResult()",
        "Executes the query, creates a temporary table, and puts the results into the table. The method returns a HasCachedResult object that you can use to access the data in this temporary table. See Caching a DataFrame.",
        "DataFrame.write().saveAsTable()",
        "Saves the data in the DataFrame to the specified table. See Saving Data to a Table.",
        "DataFrame.read().fileformat().copyInto('tableName')",
        "Copies the data in the DataFrame to the specified table. See Copying Data from Files into a Table.",
        "Session.table('tableName').delete()",
        "Deletes rows in the specified table. See Updating, Deleting, and Merging Rows in a Table.",
        "Session.table('tableName').update(), Session.table('tableName').updateColumn()",
        "Updates rows in the specified table. See Updating, Deleting, and Merging Rows in a Table.",
        "Session.table('tableName').merge().methods.collect()",
        "Merges rows into the specified table. See Updating, Deleting, and Merging Rows in a Table.",
        "For example, to execute the query and return the number of results, call the count method:",
        "You can also call action methods to:",
        "Execute a query against a table and return the results.",
        "Execute a query and print the results to the console.",
        "Note: If you are calling the schema method to get the definitions of the columns in the DataFrame, you do not need to\ncall an action method.",
        "Note",
        "This feature was introduced in Snowpark 0.11.0.",
        "To perform an action asynchronously, call the async method to return an \u201casync actor\u201d object (e.g.\nDataFrameAsyncActor), and call an asynchronous action method in that object.",
        "These action methods of an async actor object return a TypedAsyncJob object, which you can use to check\nthe status of the asynchronous action and retrieve the results of the action.",
        "The next sections explain how to perform actions asynchronously and check the results.",
        "Understanding the Basic Flow of Asynchronous Actions",
        "Specifying the Maximum Number of Seconds to Wait",
        "Accessing an Asynchronous Query by ID",
        "You can use the following methods to perform an action asynchronously:",
        "Method to Perform an Action Asynchronously",
        "Description",
        "DataFrame.async().collect()",
        "Asynchronously evaluates the DataFrame to retrieve the resulting dataset as an Array of Row objects. See Returning All Rows.",
        "DataFrame.async.toLocalIterator",
        "Asynchronously evaluates the DataFrame to retrieve an Iterator  of Row objects. If the result set is large, use this method to avoid loading all the results into memory at once. See Returning an Iterator for the Rows.",
        "DataFrame.async().count()",
        "Asynchronously evaluates the DataFrame to retrieve the number of rows.",
        "DataFrame.write().async().saveAsTable()",
        "Asynchronously saves the data in the DataFrame to the specified table. See Saving Data to a Table.",
        "DataFrame.read().fileformat().async().copyInto('tableName')",
        "Asynchronously copies the data in the DataFrame to the specified table. See Copying Data from Files into a Table.",
        "Session.table('tableName').async().delete()",
        "Asynchronously deletes rows in the specified table. See Updating, Deleting, and Merging Rows in a Table.",
        "Session.table('tableName').async().update() and Session.table('tableName').async().updateColumn()",
        "Asynchronously updates rows in the specified table. See Updating, Deleting, and Merging Rows in a Table.",
        "From the returned TypedAsyncJob object, you can do the following:",
        "To determine if the action has completed, call the isDone method.",
        "To get the query ID that corresponds to the action, call the getQueryId method.",
        "To return the results of the action (e.g. the Array of Row objects for the collect method or the count\nof rows for the count method), call the getResult method.",
        "Note that getResult is a blocking call.",
        "To cancel the action, call the cancel method.",
        "For example, to execute a query asynchronously and retrieve the results as an Array of Row objects, call\nasync().collect():",
        "To execute the query asynchronously and retrieve the number of results, call async().count():",
        "When calling the getResult method, you can use the maxWaitTimeInSeconds argument to specify the maximum number of\nseconds to wait for the query to complete before attempting to retrieve the results. For example:",
        "If you omit this argument, the method waits for the maximum number of seconds specified by the\nsnowpark_request_timeout_in_seconds configuration property. (This is a\nproperty that you can set when creating the Session object.)",
        "If you have the query ID of an asynchronous query that you submitted earlier, you can call Session.createAsyncJob method\nto create an AsyncJob object that you can use to check the status of the query, retrieve the query results, or cancel the\nquery.",
        "Note that unlike TypedAsyncJob, AsyncJob does not provide a getResult method for retrieving the results.\nIf you need to retrieve the results, call the getRows or getIterator method instead.",
        "For example:",
        "After you specify how the DataFrame should be transformed, you can\ncall an action method to execute a query and return the results. You can\nreturn all of the rows in an Array, or you can return an Iterator that allows you to iterate over the results,\nrow by row. In the latter case, if the amount of data is large, the rows are loaded into memory by chunk to avoid loading a large\namount of data into memory.",
        "Returning All Rows",
        "Returning an Iterator for the Rows",
        "Returning the First n Rows",
        "To return all rows at once, call the collect method. This method returns an Array of Row objects. To retrieve the values\nfrom the row, call the getType method (e.g. getString, getInt, etc.).",
        "For example:",
        "If you want to use an Iterator to iterate over the Row objects in the results, call toLocalIterator. If the amount\nof data in the results is large, the method loads the rows by chunk to avoid loading all rows into memory at once.",
        "For example:",
        "To return the first n rows, call the first method, passing in the number of rows to return.",
        "As explained in Limiting the Number of Rows in a DataFrame, the results are non-deterministic. If you want the results to be\ndeterministic, call this method on a sorted DataFrame (df.sort().first()).",
        "For example:",
        "To print the first 10 rows in the DataFrame to the console, call the show method. To print out a different number of rows, pass\nin the number of rows to print.",
        "As explained in Limiting the Number of Rows in a DataFrame, the results are non-deterministic. If you want the results to be\ndeterministic, call this method on a sorted DataFrame (df.sort().show()).",
        "For example:",
        "Note",
        "This feature was introduced in Snowpark 0.7.0.",
        "When you call Session.table to create a DataFrame object for a table, the method returns an Updatable\nobject, which extends DataFrame with additional methods for updating and deleting data in the table. (See Updatable.)",
        "If you need to update or delete rows in a table, you can use the following methods of the Updatable class:",
        "Call update or updateColumn to update existing rows in the table. See\nUpdating Rows in a Table.",
        "Call delete to delete rows from a table. See Deleting Rows in a Table.",
        "Call merge to insert, update, and delete rows in one table, based on data in a second table or subquery. (This is the\nequivalent of the MERGE command in SQL.) See Merging Rows into a Table.",
        "To update the rows in a table, call the update or updateColumn method, passing in a Map that associates\nthe columns to update and the corresponding values to assign to those columns:",
        "To specify the column names as strings in the Map, call updateColumn.",
        "To specify Column objects in the Map, call update.",
        "Both methods return an UpdateResult object, which contains the number of rows that were updated. (See UpdateResult.)",
        "Note",
        "Both methods are  action methods, which means that calling the method\nsends SQL statements to the server for execution.",
        "For example, to replace the values in the column named count with the value 1, and you want to use a Map that\nassociates the column name (a String) with the corresponding value, call updateColumn:",
        "If you want to use a Column object in the Map to identify the column to update, call update:",
        "If the update should be made only when a condition is met, you can specify that condition as an argument. For example, to replace\nthe values in the column named count with 2 for rows in which the category_id column has the value 20:",
        "If you need to base the condition on a join with a different DataFrame object, you can pass that DataFrame in as\nan argument and use that DataFrame in the condition. For example, to replace the values in the column named count with\n3 for rows in which the category_id column matches the category_id in the DataFrame dfParts:",
        "For the delete method, you can specify a condition that identifies the rows to delete, and you can base that condition on\na join with another DataFrame. delete returns a DeleteResult object, which contains the\nnumber of rows that were deleted. (See DeleteResult.)",
        "Note",
        "delete is an action method, which means that calling the method sends\nSQL statements to the server for execution.",
        "For example, to delete the rows that have the value 1 in the category_id column:",
        "If the condition refers to columns in a different DataFrame, pass that DataFrame in as the second argument. For example, to delete\nthe rows in which the category_id column matches the category_id in the DataFrame dfParts, pass in dfParts\nas the second argument:",
        "To insert, update, and deletes rows in one table based on values in a second table or a subquery (the equivalent of the\nMERGE command in SQL), do the following:",
        "In the Updatable object for the table where you want the data merged in, call the merge method, passing in\nthe DataFrame object for the other table and the column expression for the join condition.",
        "This returns a MergeBuilder object that you can use to specify the actions to take (e.g. insert, update, or delete) on\nthe rows that match and the rows that don\u2019t match. (See MergeBuilder.)",
        "Using the MergeBuilder object:",
        "To specify the update or deletion that should be performed on matching rows, call the whenMatched method.",
        "If you need to specify an additional condition whe rows should be updated or deleted, you can pass in a column expression for\nthat condition.",
        "This method returns a MatchedClauseBuilder object that you can use to specify the action to perform. (See\nMatchedClauseBuilder.)",
        "Call the update or delete method in the MatchedClauseBuilder object to specify the update or delete\naction that should be performed on matching rows. These methods return a MergeBuilder object that you can use to\nspecify additional clauses.",
        "To specify the insert that should be performed when rows do not match, call the whenNotMatched method.",
        "If you need to specify an additional condition when rows should be inserted, you can pass in a column expression for that\ncondition.",
        "This method returns a NotMatchedClauseBuilder object that you can use to specify the action to perform. (See\nNotMatchedClauseBuilder.)",
        "Call the insert method in the NotMatchedClauseBuilder object to specify the insert action that should be\nperformed when rows do not match. These methods return a MergeBuilder object that you can use to specify additional\nclauses.",
        "When you are done specifying the inserts, updates, and deletions that should be performed, call the collect method of\nthe MergeBuilder object to perform the specified inserts, updates, and deletions on the table.",
        "collect returns a MergeResult object, which contains the number of rows that were inserted, updated, and\ndeleted. (See MergeResult.)",
        "The following example inserts a row with the id and value columns from the source table into the target table if\nthe target table does not contain a row with a matching ID:",
        "The following example updates a row in the target table with the value of the value column from the row in the source\ntable that has the same ID:",
        "You can save the contents of a DataFrame to a new or existing table. In order to do this, you must have the following privileges:",
        "CREATE TABLE privileges on the schema, if the table does not exist.",
        "INSERT privileges on the table.",
        "To save the contents of a DataFrame to a table:",
        "Call the write method of the DataFrame to get a DataFrameWriter object.",
        "Call the mode method of the DataFrameWriter object, passing in a SaveMode object that specifies your preferences\nfor writing to the table:",
        "To insert rows, pass in SaveMode.Append.",
        "To overwrite the existing table, pass in SaveMode.Overwrite.",
        "This method returns the same DataFrameWriter object configured with the specified mode.",
        "If you are inserting rows into an existing table (SaveMode.Append) and the column names in the DataFrame match the\ncolumn names in the table, call the DataFrameWriter.option, passing in \"columnOrder\" and \"name\" as\narguments.",
        "Note",
        "This method was introduced in Snowpark 1.4.0.",
        "By default, the columnOrder option is set to \"index\", which means that the DataFrameWriter inserts the\nvalues in the order that the columns appear. For example, the DataFrameWriter inserts the value from the first column\nfrom the DataFrame in the first column in the table, the second column from the DataFrame in the second column in the table,\netc.",
        "This method returns the same DataFrameWriter object configured with the specified option.",
        "Call the saveAsTable method of the DataFrameWriter object to save the contents of the DataFrame to a specified\ntable.",
        "You do not need to call a separate method (e.g. collect) to execute the SQL statement that saves the data to the table.\nsaveAsTable is an action method that executes the SQL statement.",
        "The following example overwrites an existing table (identified by the tableName variable) with the contents of the DataFrame\ndf:",
        "The following example inserts rows from the DataFrame df into an existing table (identified by the tableName variable).\nIn this example, the table and the DataFrame both contain the columns c1 and c2.",
        "The example demonstrates the difference between setting the columnOrder option to \"name\" (which inserts values\ninto the table columns with the same names as the DataFrame columns) and using the default columnOrder option (which\ninserts values into the table columns based on the order of the columns in the DataFrame).",
        "To create a view from a DataFrame, call the createOrReplaceView method:",
        "Note that calling createOrReplaceView immediately creates the  new view. More importantly, it does not\ncause the DataFrame to be evaluated. (The DataFrame itself is not evaluated until you\nperform an action.)",
        "Views that you create by calling createOrReplaceView are persistent. If you no longer need that view, you can\ndrop the view manually.",
        "If you need to create a temporary view just for the session, call the createOrReplaceTempView method instead:",
        "In some cases, you may need to perform a complex query and keep the results for use in subsequent operations (rather than\nexecuting the same query again). In these cases, you can cache the contents of a DataFrame by calling the cacheResult method.",
        "This method:",
        "Runs the query.",
        "You do not need to call a separate action method to retrieve the results\nbefore calling cacheResult. cacheResult is an action method that executes the query.",
        "Saves the results in a temporary table",
        "Because cacheResult creates a temporary table, you must have the CREATE TABLE privilege on the schema that is in use.",
        "Returns a HasCachedResult object, which provides access to the results in the temporary table.",
        "Because HasCachedResult extends DataFrame, you can perform some of the same operations on this cached data as\nyou can perform on a DataFrame.",
        "Note",
        "Because cacheResult executes the query and saves the results to a table, the method can result in increased compute and\nstorage costs.",
        "For example:",
        "Note that the original DataFrame is not affected when you call this method. For example, suppose that dfTable is a DataFrame\nfor the table sample_product_data:",
        "After you call cacheResult, dfTable still points to the sample_product_data table, and you can continue to use\ndfTable to query and update that table.",
        "To use the cached data in the temporary table, you use dfTempTable (the HasCachedResult object returned by\ncacheResult).",
        "The Snowpark library provides classes and methods that you can use to load data into Snowflake and\nunload data from Snowflake by using files in stages.",
        "Note",
        "In order to use these classes and methods on a stage, you must have the required\nprivileges for working with the stage.",
        "The next sections explain how to use these classes and methods:",
        "Uploading and Downloading Files in a Stage",
        "Using Input Streams to Upload and Download Data in a Stage",
        "Setting Up a DataFrame for Files in a Stage",
        "Loading Data from Files into a DataFrame",
        "Copying Data from Files into a Table",
        "Saving a DataFrame to Files on a Stage",
        "To upload and download files in a stage, use the put and get methods of the FileOperation object:",
        "Uploading Files to a Stage",
        "Downloading Files from a Stage",
        "To upload files to a stage:",
        "Verify that you have the privileges to upload files to the stage.",
        "Use the file method of the Session object to access the FileOperation object for the session.",
        "Call the put method of the FileOperation object to upload the files to a stage.",
        "This method executes a SQL PUT command.",
        "To specify any optional parameters for the PUT command, create a Map of the\nparameters and values, and pass in the Map as the options argument. For example:",
        "In the localFileName argument, you can use wildcards (* and ?) to identify a set of files to upload. For\nexample:",
        "Check the Array of PutResult objects returned by the put method to determine if the files were successfully\nuploaded. For example, to print the filename and the status of the PUT operation for that file:",
        "To download files from a stage:",
        "Verify that you have the privileges to download files from the stage.",
        "Use the file method of the Session object to access the FileOperation object for the session.",
        "Call the get method of the FileOperation object to download the files from a stage.",
        "This method executes a SQL GET command.",
        "To specify any optional parameters for the GET command, create a Map of the\nparameters and values, and pass in the Map as the options argument. For example:",
        "Check the Array of GetResult objects returned by the get method to determine if the files were successfully\ndownloaded. For example, to print the filename and the status of the GET operation for that file:",
        "Note",
        "This feature was introduced in Snowpark 1.4.0.",
        "To use input streams to upload data to a file on a stage and download data from a file on a stage, use the uploadStream\nand downloadStream methods of the FileOperation object:",
        "Using an Input Stream to Upload Data to a File on a Stage",
        "Using an Input Stream to Download Data from a File on a Stage",
        "To upload the data from a java.io.InputStream object to a file on a stage:",
        "Verify that you have the privileges to upload files to the stage.",
        "Use the file method of the Session object to access the FileOperation object for the session.",
        "Call the uploadStream method of the FileOperation object.",
        "Pass in the complete path to the file on the stage where the data should be written and the InputStream object. In\naddition, use the compress argument to specify whether or not the data should be compressed before it is uploaded.",
        "For example:",
        "To download data from a file on a stage to a java.io.InputStream object:",
        "Verify that you have the privileges to download files from the stage.",
        "Use the file method of the Session object to access the FileOperation object for the session.",
        "Call the downloadStream method of the FileOperation object.",
        "Pass in the complete path to the file on the stage containing the data to download. Use the decompress argument to\nspecify whether or not the data in the file is compressed.",
        "For example:",
        "This section explains how to set up a DataFrame for files in a Snowflake stage. Once you create this DataFrame, you can use the\nDataFrame to:",
        "retrieve data from the files",
        "copy data from the files into a table",
        "To set up a DataFrame for files in a Snowflake stage, use the DataFrameReader class:",
        "Verify that you have the following privileges:",
        "Privileges to access files in the stage.",
        "One of the following:",
        "CREATE TABLE privileges on the schema, if you plan to specify\ncopy options that determine how data is copied from the staged files.",
        "CREATE FILE FORMAT privileges on the schema, otherwise.",
        "Call the read method in the Session class to access a DataFrameReader object.",
        "If the files are in CSV format, describe the fields in the file. To do this:",
        "Create a StructType object that consists of an array of StructField objects that describe the fields in the file.",
        "For each StructField object, specify the following:",
        "The name of the field.",
        "The data type of the field (specified as an object in the com.snowflake.snowpark_java.types package).",
        "Whether or not the field is nullable.",
        "For example:",
        "Call the schema method in the DataFrameReader object, passing in the StructType object.",
        "For example:",
        "The schema method returns a DataFrameReader object that is configured to read files containing the specified\nfields.",
        "Note that you do not need to do this for files in other formats (such as JSON). For those files, the\nDataFrameReader treats the data as a single field of the VARIANT type with the field name $1.",
        "If you need to specify additional information about how the data should be read (for example, that the data is compressed or\nthat a CSV file uses a semicolon instead of a comma to delimit fields), call the DataFrameReader.option method or the\nDataFrameReader.options method.",
        "Pass in the name and value of the option that you want to set. You can set the following types of options:",
        "The file format options described in the\ndocumentation on CREATE FILE FORMAT.",
        "The copy options described in the\nCOPY INTO TABLE documentation.",
        "Note that setting copy options can result in a more expensive execution strategy when you\nretrieve the data into the DataFrame.",
        "The following example sets up the DataFrameReader object to query data in a CSV file that is not compressed and that\nuses a semicolon for the field delimiter.",
        "The option method returns a DataFrameReader object that is configured with the specified options.",
        "To set multiple options, you can either\nchain calls to the option method (as shown in the example\nabove) or call the DataFrameReader.options method, passing in a Map of the names and values of the options.",
        "Call the method corresponding to the format of the files. You can call one of the following methods:",
        "DataFrameReader.avro",
        "DataFrameReader.csv",
        "DataFrameReader.json",
        "DataFrameReader.orc",
        "DataFrameReader.parquet",
        "DataFrameReader.xml",
        "When calling these methods, pass in the stage location of the files to be read. For example:",
        "To specify multiple files that start with the same prefix, specify the prefix after the stage name. For example, to load files\nthat have the prefix csv_ from the stage @mystage:",
        "The methods corresponding to the format of a file return a CopyableDataFrame object for that file. CopyableDataFrame\nextends DataFrame and provides additional methods for working the data in staged files.",
        "Call an action method to:",
        "retrieve data from the files, or",
        "copy data from the files into a table",
        "As is the case with DataFrames for tables, the data is not retrieved into the DataFrame until you call\nan action method.",
        "After you set up a DataFrame for files in a stage, you can load data from the\nfiles into the DataFrame:",
        "Use the DataFrame object methods to perform any transformations needed on the\ndataset (for example, selecting specific fields, filtering rows, etc.).",
        "For example, to extract the color element from a JSON file named data.json in the stage named mystage:",
        "As explained earlier, for files in formats other than CSV (e.g. JSON), the DataFrameReader treats the data in the file\nas a single VARIANT column with the name $1.",
        "Call the DataFrame.collect method to load the data. For example:",
        "After you set up a DataFrame for files in a stage, you can call the\ncopyInto method to copy the data into a table. This method executes the COPY INTO <table> command.",
        "Note",
        "You do not need to call the collect method before calling copyInto. The data from the files does not need to\nbe in the DataFrame before you call copyInto.",
        "For example, the following code loads data from the CSV file specified by myFileStage into the table mytable. Because the\ndata is in a CSV file, the code must also describe the fields in the file. The example does this by calling the schema method of the\nDataFrameReader object and passing in a StructType object (schemaForDataFile) containing an array of\nStructField objects that describe the fields.",
        "Preview Feature \u2014 Open",
        "Available to all accounts.",
        "Note",
        "This feature was introduced in Snowpark 1.5.0.",
        "If you need to save a DataFrame to files on a stage, you can call the DataFrameWriter method corresponding to the format of\nthe file (e.g. the csv method to write to a CSV file), passing in the stage location where the files should be saved.\nThese DataFrameWriter methods execute the COPY INTO <location> command.",
        "Note",
        "You do not need to call the collect method before calling these DataFrameWriter methods. The data from the file\ndoes not need to be in the DataFrame before you call these methods.",
        "To save the contents of a DataFrame to files on a stage:",
        "Call the write method of the DataFrame object to get a DataFrameWriter object. For example, to get the\nDataFrameWriter object for a DataFrame that represents the table named sample_product_data:",
        "If you want to overwrite the contents of the file (if the file exists), call the mode method of the DataFrameWriter\nobject, passing in SaveMode.Overwrite.",
        "Otherwise, by default, the DataFrameWriter reports an error if the specified file on the stage already exists.",
        "The mode method returns the same DataFrameWriter object configured with the specified mode.",
        "For example, to specify that the DataFrameWriter should overwrite the file on the stage:",
        "If you need to specify additional information about how the data should be saved (for example, that the data should be\ncompressed or that you want to use a semicolon to delimit fields in a CSV file), call the DataFrameWriter.option method\nor the DataFrameWriter.options method.",
        "Pass in the name and value of the option that you want to set. You can set the following types of options:",
        "The file format options described in the\ndocumentation on COPY INTO <location>.",
        "The copy options described in the\ndocumentation on COPY INTO <location>.",
        "PARTITION BY or HEADER.",
        "Note that you cannot use the option method to set the following options:",
        "The TYPE format type option.",
        "The OVERWRITE copy option. To set this option, call the mode method instead (as mentioned in the previous step).",
        "The following example sets up the DataFrameWriter object to save data to a CSV file in uncompressed form, using a\nsemicolon (rather than a comma) as the field delimiter.",
        "The option method returns a DataFrameWriter object that is configured with the specified option.",
        "To set multiple options, you can\nchain calls to the option method (as shown in the example\nabove) or call the DataFrameWriter.options method, passing in a Map of the names and values of the options.",
        "To return details about each file that was saved, set the DETAILED_OUTPUT\ncopy option to TRUE.",
        "By default, DETAILED_OUTPUT is FALSE, which means that the method returns a single row of output containing the\nfields \"rows_unloaded\", \"input_bytes\", and \"output_bytes\".",
        "When you set DETAILED_OUTPUT to TRUE, the method returns a row of output for each file saved. Each row contains\nthe fields FILE_NAME, FILE_SIZE, and ROW_COUNT.",
        "Call the method corresponding to the format of the file to save the data to the file. You can call one of the following\nmethods:",
        "DataFrameWriter.csv",
        "DataFrameWriter.json",
        "DataFrameWriter.parquet",
        "When calling these methods, pass in the stage location of the file where the data should be written (e.g. @mystage).",
        "By default, the method saves the data to filenames with the prefix data_ (e.g. @mystage/data_0_0_0.csv). If you want\nthe files to be named with a different prefix, specify the prefix after the stage name. For example:",
        "This example saves the contents of the DataFrame to files that begin with the prefix saved_data (e.g.\n@mystage/saved_data_0_0_0.csv).",
        "Check the WriteFileResult object returned for information about the amount of data written to the file.",
        "From the WriteFileResult object, you can access the output produced by the COPY INTO <location> command:",
        "To access the rows of output as an array of Row objects, call the getRows method.",
        "To determine which fields are present in the rows, call the getSchema method, which returns a StructType that\ndescribes the fields in the row.",
        "For example, to print out the names of the fields and values in the output rows:",
        "The following example uses a DataFrame to save the contents of the table named car_sales to JSON files with the prefix\nsaved_data on the stage @mystage (e.g. @mystage/saved_data_0_0_0.json). The sample code:",
        "Overwrites the file, if the file already exists on the stage.",
        "Returns detailed output about the save operation.",
        "Saves the data uncompressed.",
        "Finally, the sample code prints out each field and value in the output rows returned:",
        "Using a DataFrame, you can query and access semi-structured data (e.g JSON data). The\nnext sections explain how to work with semi-structured data in a DataFrame.",
        "Traversing Semi-Structured Data",
        "Explicitly Casting Values in Semi-Structured Data",
        "Flattening an Array of Objects into Rows",
        "Note",
        "The examples in these sections use the sample data in Sample Data Used in Examples.",
        "To refer to a specific field or element in semi-structured data, use the following methods of the Column object:",
        "Use subField(\u201c<field_name>\u201d) to return a Column object for a field in an OBJECT (or a VARIANT that contains an\nOBJECT).",
        "Use subField(<index>) to return a Column object for an element in an ARRAY (or a VARIANT that contains an ARRAY).",
        "Note",
        "If the field name or elements in the path are irregular and make it difficult to use the Column.apply methods, you can\nuse Functions.get, Functions.get_ignore_case, or Functions.get_path as an alternative.",
        "For example, the following code selects the dealership field in objects in the src column of the\nsample data:",
        "The code prints the following output:",
        "Note",
        "The values in the DataFrame are surrounded by double quotes because these values are returned as string literals. To cast these\nvalues to a specific type, see Explicitly Casting Values in Semi-Structured Data.",
        "You can also chain method calls to traverse a path to a specific\nfield or element.",
        "For example, the following code selects the name field in the salesperson object:",
        "The code prints the following output:",
        "As another example, the following code selects the first element of vehicle field, which holds an array of vehicles. The\nexample also selects the price field from the first element.",
        "The code prints the following output:",
        "As an alternative to the apply method, you can use Functions.get, Functions.get_ignore_case, or\nFunctions.get_path functions if the field name or elements in the path are irregular and make it difficult to use the\nColumn.subField methods.",
        "For example, the following lines of code both print the value of a specified field in an object:",
        "Similarly, the following lines of code both print the value of a field at a specified path in an object:",
        "By default, the values of fields and elements are returned as string literals (including the double quotes), as shown in the\nexamples above.",
        "To avoid unexpected results, call the cast method to cast the value to a specific\ntype. For example, the following code prints out the values without and with casting:",
        "The code prints the following output:",
        "If you need to \u201cflatten\u201d semi-structured data into a DataFrame (e.g. producing a row for every object in an array), call the\nflatten method. This method is equivalent to the FLATTEN SQL function. If you pass in\na path to an object or array, the method returns a DataFrame that contains a row for each field or element in the object or array.",
        "For example, in the  sample data, src:customer is an array of objects that\ncontain information about a customer. Each object contains a name and address field.",
        "If you pass this path to the flatten function:",
        "the method returns a DataFrame:",
        "From this DataFrame, you can select the name and address fields from each object in the VALUE field:",
        "The following code adds to the previous example by\ncasting the values to a specific type and changing the names of the columns:",
        "To execute a SQL statement that you specify, call the sql method in the Session class, and pass in the statement\nto be executed. The method returns a DataFrame.",
        "Note that the SQL statement won\u2019t be executed until you call an action method.",
        "If you want to call methods to transform the DataFrame (e.g. filter, select,\netc.), note that these methods work only if the underlying SQL statement is a SELECT statement. The transformation methods are not\nsupported for other kinds of SQL statements.",
        "Was this page helpful?",
        "On this page",
        "Related content"
    ]
}