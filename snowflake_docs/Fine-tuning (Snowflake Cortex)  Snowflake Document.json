{
    "url": "https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-finetuning",
    "title": "Fine-tuning (Snowflake Cortex) | Snowflake Documentation",
    "paragraphs": [
        "Preview Feature \u2014 Open",
        "Fine-tuning is available to paid accounts in the same regions as Cortex Search, namely:",
        "AWS US West 2 (Oregon)",
        "AWS US East 1 (N. Virginia)",
        "AWS Europe Central 1 (Frankfurt)",
        "Azure East US 2 (Virginia)",
        "Support for inference of fine-tuned models is available to accounts in regions that support the COMPLETE function for the base model.\nFor details, see Availability.",
        "Fine-tuning is not available to trial accounts.",
        "The Snowflake Cortex Fine-tuning function offers a way to customize large language models for your specific task.\nThis topic describes how the feature works and how to get started with creating your own fine-tuned model.",
        "Cortex Fine-tuning allows users to leverage parameter-efficient fine-tuning (PEFT) to create customized\nadaptors for use with pre-trained models on more specialized tasks. If you don\u2019t want\nthe high cost of training a large model from scratch but need better latency and results than you\u2019re\ngetting from prompt engineering or even retrieval augmented generation (RAG) methods, fine-tuning\nan existing large model is an option. Fine-tuning allows you to use examples to adjust the behavior\nof the model and improve the model\u2019s knowledge of domain-specific tasks.",
        "Cortex Fine-tuning is a fully managed service that lets you fine-tune popular LLMs using your data, all within Snowflake.",
        "Cortex Fine-tuning features are provided as a Snowflake Cortex function, FINETUNE,\nwith the following arguments:",
        "CREATE: Creates a fine-tuning job with the given training data.",
        "SHOW: Lists all the fine-tuning jobs that the current role has access to.",
        "DESCRIBE: Describes the progress and status of a particular fine-tuning job.",
        "CANCEL: Cancels a given fine-tuning job.",
        "The Snowflake Cortex Fine-tuning function incurs compute cost based on the number of tokens used in training. In addition,\nrunning the COMPLETE function on a fine-tuned model incurs compute costs based on the number of tokens\nprocessed. Refer to the Snowflake Service Consumption Table for each\ncost in credits per million tokens.",
        "A token is the smallest unit of text processed by the Snowflake Cortex Fine-tuning function, approximately equal to four\ncharacters of text. The equivalence of raw input or output text to tokens can vary by model.",
        "For the COMPLETE function, which generates new text in the response, both input and output tokens are counted.",
        "Fine-tuning trained tokens are calculated as follows:",
        "Use the FINETUNE ('DESCRIBE') (SNOWFLAKE.CORTEX) to see the number of trained tokens for your fine-tuning job.",
        "In addition to the charges incurred for tuning and inference, there are normal\nstorage and warehouse\ncosts for storing the output customized adaptors, as well as for running any SQL commands.",
        "Fine-tuning jobs are often long running and are not attached to a worksheet session.",
        "The following limits apply to training/validation dataset size for each model:",
        "Model",
        "Training data limit",
        "llama3-8b",
        "1 GB",
        "llama3-70b",
        "250 MB",
        "llama3.1-8b",
        "1 GB",
        "llama3.1-70b",
        "250 MB",
        "mistral-7b",
        "1 GB",
        "mixtral-8x7b",
        "250 MB",
        "To run a fine-tuning job, the role that creates the fine-tuning job needs the following privileges:",
        "Privilege",
        "Object",
        "Notes",
        "USAGE",
        "DATABASE",
        "The database that the training (and validation) data are queried from.",
        "(CREATE MODEL and USAGE) or OWNERSHIP",
        "SCHEMA",
        "The schema that the model is saved to.",
        "The following SQL is an example of granting the CREATE MODEL privilege to a role, my_role, on my_schema.",
        "Additionally, to use the FINETUNE function,\nthe ACCOUNTADMIN role must grant the SNOWFLAKE.CORTEX_USER database role to the user who will call the function.\nSee LLM Functions required privileges topic for details.",
        "To give other roles access to use the fine-tuned model, you must grant usage on the model. For details, see\nModel Privileges.",
        "You have the following base models that you can fine-tune. Models available for fine-tuning may be added or removed in the future:",
        "Name",
        "Description",
        "llama3-8b",
        "A large language model from Meta that is ideal for tasks that require low to moderate reasoning with better accuracy than the\nllama2-70b-chat model, like text classification, summarization, and sentiment analysis.",
        "llama3-70b",
        "An LLM from Meta that delivers state of the art performance ideal for chat applications, content creation, and enterprise applications.",
        "llama3.1-8b",
        "A large language model from Meta that is ideal for tasks that require low to moderate reasoning. It\u2019s a light-weight, ultra-fast\nmodel with a context window of 128K.",
        "llama3.1-70b",
        "An open source model that demonstrates state-of-the-art performance ideal for chat applications,\ncontent creation, and enterprise applications. It is a highly performant, cost effective model that enables diverse use\ncases with a context window of 128K. llama3-70b is still supported and has a context window of 8K.",
        "mistral-7b",
        "7 billion parameter large language model from Mistral AI that is ideal for your simplest summarization, structuration, and question\nanswering tasks that need to be done quickly. It offers low latency and high throughput processing for multiple pages of text with\nits 32K context window.",
        "mixtral-8x7b",
        "A large language model from Mistral AI that is ideal for text generation, classification, and question answering. Mistral models are\noptimized for low latency with low memory requirements, which translates into higher throughput for enterprise use cases.",
        "The overall workflow for tuning a model is as follows:",
        "Prepare the training data.",
        "Start the fine-tuning job with the required parameters.",
        "Monitor training job.",
        "Once training is complete, you can use the model name provided by Cortex Fine-tuning to run inference on your model.",
        "The training data must come from a Snowflake table or view and the query result must contain columns named prompt and completion.\nIf your table or view does not contain columns with the required names, use a column alias in your query to name them. This query is given\nas a parameter to the FINETUNE function. You will get an error if the results do not contain prompt and completion column names.",
        "Note",
        "All columns other than the prompt and completion columns will be ignored by the FINETUNE function. Snowflake recommends using a\nquery that selects only the columns you need.",
        "The following code calls the FINETUNE function and uses the SELECT ... AS syntax to set two of the columns in the query result\nto prompt and completion.",
        "A prompt is an input to the LLM and completion is the response from the LLM. Your training data should include prompt and completion pairs\nthat show how you want the model to respond to particular prompts.",
        "The following are additional recommendations and requirements regarding your training data for getting\noptimal performance from fine-tuning.",
        "Start with a few hundred examples. Starting with too many examples may increase tuning time drastically with\nminimal improvement in performance.",
        "For each example, you must use only a portion of the allotted context window for the base model you are tuning. Context window is\ndefined in terms of tokens. A token is the smallest unit of text processed by Snowflake Cortex functions, approximately equal to\nfour characters of text. Prompt and completion pairs that exceed this limit will be truncated, which may negatively impact\nthe quality of the trained model.",
        "The portion of the context window alloted for prompt and completion for each base model is defined in the following table:",
        "Model",
        "Context Window",
        "Input Context (prompt)",
        "Output Context (completion)",
        "llama3-8b",
        "8k",
        "6k",
        "2k",
        "llama3-70b",
        "8k",
        "6k",
        "2k",
        "llama3.1-8b",
        "24k",
        "20k",
        "4k",
        "llama3.1-70b",
        "8k",
        "6k",
        "2k",
        "mistral-7b",
        "32k",
        "28k",
        "4k",
        "mixtral-8x7b",
        "32k",
        "28k",
        "4k",
        "You can start a fine-tuning job by\ncalling the SNOWFLAKE.CORTEX.FINETUNE function and passing in \u2018CREATE\u2019 as the first argument\nor using Snowsight.",
        "This example uses the mistral-7b model as the base model to create a job with a model output name of my_tuned_model and training\nand validation data querying from the my_training_data and my_validation_data tables respectively.",
        "You can use absolute paths for each of the database objects such as the model or data if you want to use different database and schema for each. The following example shows creating a fine-tuning job with data from mydb2.myschema2 database and schema and saving the fine-tuned model to the mydb.myschema database and schema.",
        "The SNOWFLAKE.CORTEX.FINETUNE function with \u2018CREATE\u2019 as the first argument\nreturns a fine-tuned model ID as the output. Use this ID to get status or job progress using the\nSNOWFLAKE.CORTEX.FINETUNE function with \u2018DESCRIBE\u2019 as the first argument.",
        "Follow these steps to create a fine-tuning job in the Snowsight:",
        "Sign in to Snowsight.",
        "Choose a role that is granted the SNOWFLAKE.CORTEX_USER database role.",
        "In the navigation menu, select AI & ML \u00bb Studio.",
        "Select Fine-tune from the Create Custom LLM box.",
        "Select a base model using the drop-down menu.",
        "Select the role under which the fine-tuning job will execute and the warehouse where it will run. The role must be granted\nthe SNOWFLAKE.CORTEX_USER database role.",
        "Select a database in which to store the fine-tuned model.",
        "Enter a name for your fine-tuned model, then select Let\u2019s go.",
        "Select the table or view that contains your training data, then select Next. The training data can come from any database or\nschema that the role has access to.",
        "Select the column that contains the prompts in your training data, then select Next.",
        "Select the column that contains the completions in your training data, then select Next.",
        "If you have a validation dataset, select the table or view that contains your validation data, then select Next.\nIf you don\u2019t have separate validation data, select Skip this option.",
        "Verify your choices, then select Start training.",
        "The final step confirms that your fine-tuning job has started and displays the Job ID. Use this ID to get status or job progress\nusing the SNOWFLAKE.CORTEX.FINETUNE function with \u2018DESCRIBE\u2019 as the first argument.",
        "Fine-tuning jobs are long running, which means they are not tied to a worksheet session. You can check the status of your tuning job using the\nSNOWFLAKE.CORTEX.FINETUNE function with \u2018SHOW\u2019 or\n\u2018DESCRIBE\u2019 as the first argument.",
        "If you no longer need a fine-tuning job, you can use\nSNOWFLAKE.CORTEX.FINETUNE function with \u2018CANCEL\u2019 as the first argument and the job ID as the second\nargument to terminate it.",
        "Use the COMPLETE LLM function with the name your fine-tuned model to make inferences.",
        "This example shows a call to the COMPLETE function with\nthe name of your fine-tuned model.",
        "The following is a snippet of the output from the example call:",
        "Fine-tuning jobs are listable at the account-level only.",
        "The fine-tuning jobs returned from FINETUNE ('SHOW') (SNOWFLAKE.CORTEX) are not permanent and may be garbage\ncollected periodically.",
        "If a base model is removed from the Cortex LLM Functions, your fine-tuned model will no longer work.",
        "Preview Feature \u2014 Open",
        "Available to all accounts.",
        "Fine-tuned models can be shared to other accounts with the USAGE privilege via Data Sharing.",
        "Cross-region inference does not support fine-tuned models. Inference must take place in the same\nregion where the model object is located. You can use database replication to replicate the fine-tuned model object to a region you want\nto make inference from if it\u2019s different than the region the model was trained in.",
        "For example,\nif you create a fine-tuned model based on mistral-7b in your account in the AWS US West 2 region, you can use data sharing to share it\nwith another account in this region, or you can use database replication to replicate the model to another account in your organization\nin a different region that supports the mistral-7b model, such as AWS Europe West. For details on replicating objects, see\nReplicating databases and account objects across multiple accounts.",
        "The data classification of inputs and outputs are as set forth in the following table.",
        "Input data classification",
        "Output data classification",
        "Designation",
        "Usage Data",
        "Customer Data",
        "Preview AI Features [1]",
        "Represents the defined term used in the AI Terms and Acceptable Use Policy.",
        "For additional information, refer to Snowflake AI and ML.",
        "Was this page helpful?",
        "On this page",
        "Related content"
    ]
}