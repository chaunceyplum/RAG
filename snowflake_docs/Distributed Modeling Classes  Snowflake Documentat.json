{
    "url": "https://docs.snowflake.com/en/developer-guide/snowpark-ml/reference/latest/distributors#snowflake-ml-modeling-distributors-pytorch-pytorchdistributor",
    "title": "Distributed Modeling Classes | Snowflake Documentation",
    "paragraphs": [
        "When using Container Runtime for ML\nin a Snowflake Notebook, a set of distributed\nmodeling classes is available to train selected types of models on large datasets using the full resources of a\nSnowpark Container Services (SPCS) compute pool.",
        "The following model types are supported:",
        "XGBoost",
        "LightGBM",
        "PyTorch",
        "Xgboost Estimator that supports distributed training.",
        "Args:",
        "Number of estimators. Default is 100.",
        "The objective function used for training. \u2018reg:squarederror\u2019[Default] for regression,\n\u2018binary:logistic\u2019 for binary classification, \u2018multi:softmax\u2019 for multi-class classification.",
        "Additional parameters for the XGBoost Estimator.",
        "booster: Specify which booster to use: gbtree[Default], gblinear or dart.",
        "max_depth: Maximum depth of a tree. Default is 6.",
        "max_leaves: Maximum number of nodes to be added. Default is 0.",
        "Default is 256.",
        "eval_metric: Evaluation metrics for validation data.",
        "Full list of supported parameter can be found at https://xgboost.readthedocs.io/en/stable/parameter.html\nIf params dict contains keys \u2018n_estimators\u2019 or \u2018objective\u2019, they override the value provided\nby n_estimators and objective arguments.",
        "Scaling config for XGBoost Estimator.  Defaults to None. If None, the estimator will use all available\nresources.",
        "XGBScalingConfig(BaseScalingConfig)",
        "Scaling config for XGBoost Estimator",
        "Number of workers to use for distributed training. Default is -1, meaning the estimator will\nuse all available workers.",
        "Number of CPU cores to use per worker. Default is -1, meaning the estimator will use\nall available CPU cores.",
        "Whether to use GPU for training. If None, the estimator will choose to use GPU or not\nbased on the environment.",
        "LightGBM Estimator for distributed training and inference.",
        "Args:",
        "Number of boosting iterations. Defaults to 100.",
        "The learning task and corresponding objective. Defaults to \u201cregression\u201d.",
        "\u201cregression\u201d[Default] for regression tasks, \u201cbinary\u201d for binary classification, \u201cmulticlass\u201d for\nmulti-class classification.",
        "params (Optional[Dict[str, Any]], optional):",
        "Additional parameters for LightGBM. Defaults to None.",
        "Some key params are:",
        "boosting: The type of boosting to use. \u201cgbdt\u201d[Default] for Gradient Boosting Decision Tree, \u201cdart\u201d for\nDropouts meet Multiple Additive Regression Trees.",
        "num_leaves: The maximum number of leaves in one tree. Default is 31.",
        "max_depth: The maximum depth of the tree. Default is -1, which means no limit.",
        "early_stopping_rounds: Activates early stopping. The model will train until the validation score\nstops improving. Default is 0, meaning no early stopping.",
        "Full list of supported parameter can be found at https://lightgbm.readthedocs.io/en/latest/Parameters.html.",
        "Configuration for scaling. Defaults to None. If None, the estimator will use all available resources.",
        "LightGBMScalingConfig(BaseScalingConfig)",
        "Scaling config for LightGBM Estimator.",
        "Attributes:",
        "The number of worker processes to use. Default is -1, which utilizes all available resources.",
        "Number of CPUs allocated per worker. Default is -1, which means all available resources.",
        "Whether to use GPU for training. Default is None, allowing the estimator to choose\nautomatically based on the environment.",
        "Enables users to run distributed training with PyTorch on ContainerRuntime cluster.",
        "PyTorchDistributor is responsible for setting up the environment, scheduling the training processes,\nmanaging the communication between the processes, and collecting the results.",
        "Args:",
        "A callable object that defines the training logic to be executed.",
        "Configuration for scaling and other settings related to the training job.",
        "snowflake.ml.modeling.distributors.pytorch.PyTorchScalingConfig",
        "Scaling configuration for training PyTorch models.",
        "This class defines the scaling configuration for a PyTorch training job,\nincluding the number of nodes, the number of workers per node, and the\nresource requirements for each worker.",
        "Attributes:",
        "num_nodes (int): The number of nodes to use for training.",
        "num_workers_per_node (int): The number of workers to use per node.",
        "resource_requirements_per_worker (WorkerResourceConfig): The resource requirements\nfor each worker, such as the number of CPUs and GPUs.",
        "snowflake.ml.modeling.distributors.pytorch.WorkerResourceConfig",
        "Resources requirements per worker.",
        "This class defines the resource requirements for each worker in a distributed\ntraining job, specifying the number of CPU and GPU resources to allocate.",
        "Attributes:",
        "num_cpus (int): The number of CPU cores to reserve for each worker.",
        "num_gpus (int): The number of GPUs to reserve for each worker.\nDefault is 0, indicating no GPUs are reserved.",
        "snowflake.ml.modeling.distributors.pytorch.Context",
        "Context for setting up the PyTorch distributed environment for training scripts.",
        "Context defines the necessary methods to manage and retrieve information\nabout the distributed training environment, including worker and node ranks,\nworld size, and backend configurations.",
        "Definitions:",
        "Node: A physical instance or a container.",
        "Worker: A worker process in the context of distributed training.",
        "WorkerGroup: The set of workers that execute the same function (e.g., trainers).",
        "LocalWorkerGroup: A subset of the workers in the worker group running on the same node.",
        "RANK: The rank of the worker within a worker group.",
        "WORLD_SIZE: The total number of workers in a worker group.",
        "LOCAL_RANK: The rank of the worker within a local worker group.",
        "LOCAL_WORLD_SIZE: The size of the local worker group.\nrdzv_id: An ID that uniquely identifies the worker group for a job. This ID is used by each node to join as",
        "a member of a particular worker group.",
        "key-value store.",
        "rdzv_endpoint: The rendezvous backend endpoint; usually in the form <host>:<port>.",
        "Methods:",
        "Return the number of workers (or processes) participating in the job.",
        "For example, if training is running on 2 nodes (servers) each with 4 GPUs,\nthen the world size is 8 (2 nodes * 4 GPUs per node). Usually, each GPU corresponds\nto a training process.",
        "Return the rank of the current process across all processes.",
        "Rank is the unique ID given to a process to identify it uniquely across the world.\nIt should be a number between 0 and world_size - 1.",
        "Some frameworks also call it world_rank, to distinguish it from local_rank.\nFor example, if training is running on 2 nodes (servers) each with 4 GPUs,\nthen the ranks will be [0, 1, 2, 3, 4, 5, 6, 7], i.e., from 0 to world_size - 1.",
        "Return the local rank for the current worker.",
        "Local rank is a unique local ID for a worker (or process) running on the current node.",
        "For example, if training is running on 2 nodes (servers) each with 4 GPUs, then\nlocal rank for workers(or processes) running on node 0 will be [0, 1, 2, 3] and\nsimilarly four workers(or processes) running on node 1 will have local_rank [0, 1, 2, 3].",
        "Return the number of workers running in the current node.",
        "For example, if training is running on 2 nodes (servers) each with 4 GPUs,\nthen local_world_size will be 4 for all processes on both nodes.",
        "Return the rank of the current node across all nodes.",
        "Node rank is a unique ID given to each node to identify it uniquely across all nodes\nin the world.",
        "For example, if training is running on 2 nodes (servers) each with 4 GPUs,\nthen node ranks will be [0, 1] respectively.",
        "Return IP address of the master node.",
        "This is typically the address of the node with node_rank 0.",
        "Return port on master_addr that hosts the rendezvous server.",
        "Return default backend selected by MCE.",
        "Return list of supported backends by MCE.",
        "Return hyperparameter map provided to trainer.run(\u2026) method.",
        "Return dataset map provided to trainer.run(\u2026) method.",
        "snowflake.ml.modeling.distributors.pytorch.get_context",
        "Fetches the context object that contains the worker specific runtime information.",
        "Returns:",
        "Context: An instance of the Context interface that provides methods for\nmanaging the distributed training environment.",
        "Raises:",
        "RuntimeError: If the PyTorch context is not available.",
        "Was this page helpful?",
        "On this page"
    ]
}