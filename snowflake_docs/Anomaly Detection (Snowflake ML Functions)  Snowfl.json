{
    "url": "https://docs.snowflake.com/en/user-guide/ml-functions/anomaly-detection",
    "title": "Anomaly Detection (Snowflake ML Functions) | Snowflake Documentation",
    "paragraphs": [
        "Anomaly detection is the process of identifying outliers in data. The anomaly detection function lets you train a model\nto detect outliers in your time-series data. Outliers, which are data points that deviate from the expected range, can\nhave an outsized impact on statistics and models derived from your data. Spotting and removing outliers can therefore\nhelp improve the quality of your results.",
        "Note",
        "Anomaly Detection is part of Snowflake\u2019s suite of business analysis tools powered by machine learning.",
        "Detecting outliers can also be useful in pinpointing the origin of problems or deviations in processes when there is no\nobvious cause. For example:",
        "Determining when a problem started to occur with your logging pipeline.",
        "Identifying the days when your Snowflake compute costs are higher than expected.",
        "Anomaly detection works with either single-series or multi-series data. Multi-series data represents multiple\nindependent threads of events. For example, if you have sales data for multiple stores, each store\u2019s sales can be\nchecked separately by a single model based on the store identifier.",
        "The data must include:",
        "A timestamp column.",
        "A target column representing some quantity of interest at each timestamp.",
        "Note",
        "Ideally, the training data for an Anomaly Detection model has time steps at equally spaced intervals (for example,\ndaily). However, model training can handle real-world data that has missing, duplicate, or misaligned time steps.\nFor more information, see Dealing with real-world data in Time-Series Forecasting.",
        "To detect outliers in time-series data, use the Snowflake built-in class ANOMALY_DETECTION (SNOWFLAKE.ML),\nand follow these steps:",
        "Create an anomaly detection object,\npassing in a reference to the training data.",
        "This object fits a model to the training data that you provide. The model is a schema-level object.",
        "Using this anomaly detection model object, call the <model_name>!DETECT_ANOMALIES method to\ndetect anomalies, passing in a reference to the data to analyze.",
        "The method uses the model to identify outliers in the data.",
        "Anomaly detection is closely related to Forecasting. An anomaly detection model\nproduces a forecast for the same time period as the data you\u2019re checking for anomalies, then compares the actual data to\nthe forecast to identify outliers.",
        "Important",
        "Legal notice. This Snowflake ML function is powered by machine learning technology. Machine\nlearning technology and results provided may be inaccurate, inappropriate, or biased. Decisions based on machine\nlearning outputs, including those built into automatic pipelines, should have human oversight and review processes\nto ensure model-generated content is accurate. Snowflake Cortex ML function queries will be treated as any\nother SQL query and may be considered metadata.",
        "Metadata. When you use Snowflake Cortex ML functions, Snowflake logs generic error messages returned by an ML\nfunction. These error logs help us troubleshoot issues that arise and improve these functions to serve you better.",
        "For further information, see Snowflake AI Trust and Safety FAQ.",
        "The anomaly detection algorithm is powered by a gradient boosting machine\n(GBM). Like an ARIMA model, it uses a\ndifferencing transformation to model data with a non-stationary trend and uses auto-regressive lags of the historical\ntarget data as model variables.",
        "Additionally, the algorithm uses rolling averages of historical target data to help predict trends, and automatically\nproduces cyclic calendar variables (such as day of week and week of year) from timestamp data.",
        "You can fit models with only historical target and timestamp data, or you may include exogenous data (variables) that\nmight have influenced the target value. Exogenous variables can be numerical or categorical and may be NULL (rows\ncontaining NULLs for exogenous variables are not dropped).",
        "The algorithm does not rely on one-hot encoding when training on categorical variables, so you can use categorical data\nwith many dimensions (high cardinality).",
        "If your model incorporates exogenous variables, you must provide values for those variables at timestamps in the future\nwhen detecting anomalies. Appropriate exogenous variables could include weather data (temperature, rainfall),\ncompany-specific information (historic and planned company holidays, advertisement campaigns, event schedules), or any\nother external factors you believe may help predict your target variable.",
        "Optionally, individual historical rows can be labeled as anomalous or non-anomalous by using a separate Boolean column.",
        "A prediction interval is an estimated range of values within an upper bound and a lower bound in which a certain\npercentage of data is likely to fall. For example, a 0.99 value means that 99% of the data likely appears within the\ninterval. The anomaly detection model identifies any data that falls outside of the prediction interval as an anomaly. You can\nspecify a prediction interval or use the default, which is 0.99. You may want to set this value to be very close\nto 1.0; 0.9999 or even closer.",
        "Important",
        "From time to time, Snowflake may refine the anomaly detection algorithm. Such improvements roll out\nthrough the regular Snowflake release process. You cannot revert to a previous version of the feature, but models you\ncreated with a previous version continue to use that version for anomaly detection.",
        "You cannot choose or adjust the anomaly detection algorithm. In particular, the algorithm does not provide parameters\nto override trend, seasonality, or seasonal amplitudes; these are inferred from the data.",
        "The minimum number of rows for the main anomaly detection algorithm is 12 per time series. For time series with\nbetween 2 and 11 observations, anomaly detection produces a \u201cnaive\u201d result in which all predicted values are equal to\nthe last observed target value. For the labeled anomaly detection case, the number of observations used is the number\nof rows where the label column is false.",
        "The minimum acceptable granularity of data is one second. (Timestamps must not be less than one second apart.)",
        "The minimum granularity of seasonal components is one minute. (The function cannot detect cyclic patterns at smaller\ntime deltas.)",
        "The \u201cseason length\u201d of autoregressive features is tied to the input frequency (24 for hourly data, 7 for daily data,\nand so on).",
        "Anomaly detection models, once trained, are immutable. You cannot update existing models with new data; you must train\nan entirely new model. Models do not support versioning. Generally, you should retrain models on a regular cadence,\nsuch as once a day, once a week, or once a month, depending on how frequently you receive new data, to help the model\nkeep up with changing trends.",
        "This feature only detects anomalies in the test data; it cannot detect anomalies in the training data. Furthermore,\ntimestamps in the test data must all be greater than timestamps in the training data. Ensure that the training data\ncovers a typical period free of actual outliers, or label known outliers in a Boolean column.",
        "You cannot clone models or share models across roles or accounts. When cloning a schema or database, model objects are skipped.",
        "You cannot replicate an instance of the ANOMALY_DETECTION\nclass.",
        "Before you can use anomaly detection, you must:",
        "Select a virtual warehouse\nin which to train and run your models.",
        "Grant the privileges to create anomaly detection objects.",
        "You might also want to modify your search path to include\nSNOWFLAKE.ML.",
        "A Snowflake virtual warehouse provides the compute resources for training and using the\nmachine learning models for this feature. This section provides general guidance on selecting the best size and type of\nwarehouse for this purpose, focusing on the training step (the most time-consuming and memory-intensive part of\nthe process).",
        "For models trained on single-series data, you should choose the warehouse type based on the size of your training data.\nStandard warehouses are subject to a lower Snowpark memory limit,\nand are more appropriate for training jobs with fewer rows or exogenous features.\nIf your training data does not contain any exogenous features, you can train on a standard warehouse if the dataset has 5 million rows or less.\nIf your training data uses 5 or more exogenous features, then the maximum row count is lower.\nOtherwise, Snowflake suggests using a Snowpark-optimized warehouse for larger training jobs.",
        "In general, for single-series data, a larger warehouse size does not result in a faster training time or higher memory limits.\nAs a rough rule of thumb, training time is proportional to the number of rows in your time series. For example, on a XS\nstandard warehouse, with evaluation turned off (CONFIG_OBJECT => {'evaluate': False}), training on a\n100,000-row dataset takes about 60 seconds, while training on a 1,000,000-row dataset takes about 125 seconds. With\nevaluation turned on, training time increases roughly linearly by the number of splits used.",
        "For best performance, Snowflake recommends using a dedicated warehouse without other concurrent workloads to train your model.",
        "As with single-series data, choose the warehouse type based on the number of rows in your largest time series. If your\nlargest time series contains more than 5 million rows, the training job is likely to exceed memory limits on a standard\nwarehouse.",
        "Unlike single-series data, multi-series data trains considerably faster on larger warehouse sizes.\nThe following data points can guide you in your selection. Once again, all these times are done with evaluation turned\noff.",
        "Warehouse type and size",
        "Number of time series",
        "Number of rows per time series",
        "Training time (seconds)",
        "Standard XS",
        "1",
        "100,000",
        "60 seconds",
        "Standard XS",
        "10",
        "100,000",
        "204 seconds",
        "Standard XS",
        "100",
        "100,000",
        "720 seconds",
        "Standard XL",
        "10",
        "100,000",
        "104 seconds",
        "Standard XL",
        "100",
        "100,000",
        "211 seconds",
        "Standard XL",
        "1000",
        "100,000",
        "840 seconds",
        "Snowpark-optimized XL",
        "10",
        "100,000",
        "65 seconds",
        "Snowpark-optimized XL",
        "100",
        "100,000",
        "293 seconds",
        "Snowpark-optimized XL",
        "1000",
        "100,000",
        "831 seconds",
        "The inference step takes approximately 1 second to process 100 rows in the input dataset, regardless of warehouse size.",
        "Training an anomaly detection model results in a schema-level object. Therefore, the role you use to create models must\nhave the CREATE SNOWFLAKE.ML.ANOMALY_DETECTION privilege on the schema where the model is created, which allows the\nmodel to be stored there. This privilege is similar to other schema privileges like CREATE TABLE or CREATE VIEW.",
        "Snowflake recommends that you create a role named analyst to be used by people who need to detect anomalies.",
        "In the following example, the admin role is the owner of the schema admin_db.admin_schema. The\nanalyst role needs to create models in this schema.",
        "To use this schema, a user assumes the role analyst:",
        "If the analyst role has CREATE SCHEMA privileges in database analyst_db, the role can create a new schema\nanalyst_db.analyst_schema and create anomaly detection models in that schema:",
        "To revoke a role\u2019s model creation privilege on the schema, use REVOKE <privileges>:",
        "The examples in the following sections use a sample dataset that contains daily sales for items in different stores along with\ndaily weather data (humidity and temperature). The dataset also contains a column that indicates whether the day is a holiday.",
        "Execute the following statements to create a table named historical_sales_data that contains the training data for the model:",
        "Execute the following statements to create a table named new_sales_data that contains the data to analyze:",
        "Use CREATE SNOWFLAKE.ML.ANOMALY_DETECTION to create and train a model. The model is trained on the dataset you\nprovide.",
        "See ANOMALY_DETECTION (SNOWFLAKE.ML) for complete details about the SNOWFLAKE.ML.ANOMALY_DETECTION\nconstructor. For examples of creating a model, see Detecting Anomalies.",
        "Note",
        "SNOWFLAKE.ML.ANOMALY_DETECTION runs using limited privileges, so by default it does not have access to your data. You must\ntherefore pass tables and views as references, which pass along the\ncaller\u2019s privileges. You can also provide a query reference instead of a\nreference to a table or a view.",
        "To create this reference, you can use the TABLE keyword with the table name, view name,\nor query, or you can call the SYSTEM$REFERENCE or\nSYSTEM$QUERY_REFERENCE function.",
        "To detect anomalies, call the model\u2019s <model_name>!DETECT_ANOMALIES method:",
        "To select columns from the tabular output of the method, you can\ncall the method in the FROM clause:",
        "To view a list of your models, use the SHOW SNOWFLAKE.ML.ANOMALY_DETECTION command:",
        "To remove a model, use the DROP SNOWFLAKE.ML.ANOMALY_DETECTION command:",
        "To update a model, delete it and train a new one.  Models are immutable and cannot be updated in place.",
        "The following sections demonstrate how to use anomaly detection to detect outliers. These sections provide examples of\ndetecting anomalies for a single time series, for multiple time series, with and without exogenous variables, with a\nuser-defined prediction interval, and with a supervised (labeled) approach.",
        "Detecting Anomalies for a Single Time Series (Unsupervised)",
        "Training an Anomaly Detection Model with Labeled Data",
        "Specifying the Prediction Interval For Anomaly Detection",
        "Including Additional Columns for Analysis",
        "Detecting Anomalies in Multiple Series",
        "To detect anomalies in your data:",
        "Train an anomaly detection model using historical data.",
        "Use the trained anomaly detection model to detect anomalies in historical or projected data. The timestamps in the test data\nmust chronologically follow the timestamps in the training data. You need at least 2 data points to train a model, at least\n12 for non-naive results, and at least 60 for non-linear results.",
        "See ANOMALY_DETECTION (SNOWFLAKE.ML) for information on the parameters used in creating and using a model.",
        "To create an anomaly detection model object, execute the CREATE SNOWFLAKE.ML.ANOMALY_DETECTION command.",
        "For example, suppose that you want to analyze the sales for jackets in the store with the store_id of 1:",
        "Create a view or design a query that returns the data for training the model for anomaly detection.",
        "For this example, execute the CREATE VIEW command to create a view named view_with_training_data\nthat contains the date and sales information:",
        "Create an anomaly detection object, and train its model on the data in that view.",
        "For this example, execute the CREATE SNOWFLAKE.ML.ANOMALY_DETECTION command to create an anomaly detection object named\nbasic_model. Pass in the following arguments:",
        "This example passes in a reference to a view as the INPUT_DATA argument. The example\nuses the TABLE keyword to create the reference. As an alternative, you can call\nSYSTEM$REFERENCE to create the reference.",
        "The purpose of the label column is to tell the model which rows are known anomalies. Because this example uses\nunsupervised training, you do not need to use the label column. Pass an empty string as the name of the label column.",
        "Tip",
        "If you don\u2019t want to create a view for the INPUT_DATA argument, you can pass in a\nreference to a query that uses a SELECT statement that serves as an inline\nview.",
        "You can use the TABLE keyword to create this query reference. For example:",
        "Escape any single quotes and other special characters with a backslash.",
        "As an alternative to using the TABLE keyword, you can call SYSTEM$QUERY_REFERENCE to create\nthe query reference.",
        "If the command is executed successfully, a message indicates that your anomaly detection instance was created\nsuccessfully:",
        "Creating the anomaly detection object trains the model and stores it in the schema. To use the anomaly detection object\nto detect anomalies, call the <model_name>!DETECT_ANOMALIES method of the object. For example:",
        "Create a view or design a query that returns the data for analysis.",
        "For this example, execute the CREATE VIEW command to create a view named\nview_with_data_to_analyze that contains the date and sales information:",
        "Using the object for the anomaly detection model (in this example, basic_model, which\nyou created earlier),\ncall the <model_name>!DETECT_ANOMALIES method:",
        "The method returns a table that includes rows for the data currently in the view view_with_data_to_analyze along with the\nprediction of the detector. For a description of the columns in this table, see Returns.",
        "Output",
        "The results have been rounded for readability.",
        "To save your results directly to a table, use CREATE TABLE \u2026 AS SELECT \u2026 and\ncall the DETECT_ANOMALIES method in the FROM clause:",
        "As shown in the example above, when calling the method, omit the CALL command. Instead, put the call\nin parentheses, preceded by the TABLE keyword.",
        "In the previous example, the result of the model appears to be inaccurate. This is probably because:",
        "The anomaly detection model was trained on very little input data.",
        "A larger number of jackets (30) were sold on 2020-01-03. This skewed the predictions upward and increased the size of\nthe prediction interval.",
        "To improve the accuracy of the anomaly detection model, you can either include more training data or label the training data\n(supervised training). Labeled training data has an additional Boolean column that indicates whether each row is a known\nanomaly. Labeling can help the anomaly detection model to avoid overfitting to known anomalies in the training data.",
        "To include labeled data in the training data, specify the column containing the label in the LABEL_COLNAME constructor argument\nof the CREATE SNOWFLAKE.ML.ANOMALY_DETECTION command. For example:",
        "Create a view or design a query that returns the labels with the training data.",
        "For this example, execute the CREATE VIEW command to create a view named\nview_with_labeled_data that contains the labels in a column named label:",
        "Create an object for the anomaly detection model, and train the model on the data in that view.",
        "For this example, execute the CREATE SNOWFLAKE.ML.ANOMALY_DETECTION command to create an anomaly detection object named\nmodel_trained_with_labeled_data. The following statement creates the anomaly detection object:",
        "Using this new anomaly detection model, call the <model_name>!DETECT_ANOMALIES method,\npassing in the same arguments that you used in Detecting Anomalies for a Single Time Series (Unsupervised):",
        "The method returns a table that includes rows for the data currently in the view view_with_data_to_analyze along with the\nprediction of the detector. For a description of the columns in this table, see Returns.",
        "Output",
        "The results have been rounded for readability.",
        "You can detect anomalies with varying levels of sensitivity. To specify the percentage of observations to classify as\nanomalies, create an OBJECT that contains configuration settings for <model_name>!DETECT_ANOMALIES, and set the\nprediction_interval key to the percentage of the observations that should be marked as anomalies.",
        "To construct this object, you can use either an object constant or the\nOBJECT_CONSTRUCT function.",
        "Then, when calling the <model_name>!DETECT_ANOMALIES method, pass in this object as the CONFIG_OBJECT argument.",
        "By default, the value associated with the prediction_interval key is set to 0.99, which means that roughly 1% of the data is\nmarked as anomalies. You can specify a value between 0 and 1:",
        "To mark fewer observations as anomalies, specify a higher value for prediction_interval.",
        "To mark more observations as anomalies, reduce the prediction_interval value.",
        "The following example configures anomaly detection to be more strict by setting the prediction_interval to 0.995. The example also\nuses the model trained on labeled data (that you set up in Training an Anomaly Detection Model with Labeled Data) with the view\nthat contains the data to analyze (that you set up in Detecting Anomalies for a Single Time Series (Unsupervised)).",
        "This statement produces a table that includes rows for the data currently in the view view_with_data_to_analyze. Each row\nincludes a column with the prediction of the detector. You can see that the result\nof this model is more accurate than the unlabeled example.",
        "Output",
        "The results have been rounded for readability.",
        "You can include additional columns in the data (for example, temperature, weather, is_black_friday) in the data for training\nand analysis, if these columns can help you improve the identification of true anomalies.",
        "To include new columns for analysis:",
        "For the training data, create a view or design a query that includes the new columns, and create a new anomaly detection object,\npassing in a reference to that view or query.",
        "For the data to analyze, create a view or design a query that includes the new columns, and pass a reference to that\nview or query to the <model_name>!DETECT_ANOMALIES method.",
        "The anomaly detection model detects and uses the additional columns automatically.",
        "Note",
        "You must provide a view or query with the same set of additional columns when executing the CREATE SNOWFLAKE.ML.ANOMALY_DETECTION\ncommand and when calling the <model_name>!DETECT_ANOMALIES method. If there is a mismatch between the columns in the training data\npassed to the command and the columns in the data for analysis passed to the function, an error occurs.",
        "For example, suppose that you want to add the columns temperature, humidity, and holiday:",
        "Create a view or design a query that returns the training data with these additional columns.",
        "For this example, execute the CREATE VIEW command to create a view named\nview_with_training_data_extra_columns:",
        "Create an object for the anomaly detection model, and train the model on the data in that view.",
        "For this example, execute the CREATE SNOWFLAKE.ML.ANOMALY_DETECTION command to create an anomaly detection object named\nmodel_with_additional_columns, passing in a reference to the new view:",
        "Create a view or design a query that returns the data to analyze with these additional columns.",
        "For this example, execute the CREATE VIEW command to create a view named\nview_with_data_for_analysis_extra_columns:",
        "Using this new anomaly detection object, call the <model_name>!DETECT_ANOMALIES method, passing in the new view:",
        "This statement produces a table that includes rows for the data currently in the view\nview_with_data_for_analysis_extra_columns along with the prediction of the detector. The format of the output\nis the same as the format of the output shown for the commands that you ran earlier.",
        "Output",
        "The results have been rounded for readability.",
        "The previous sections provided examples of detecting anomalies for a single series. These examples flagged anomalies for\nthe sale of one type of item (jackets) in one store (store ID 1). To detect anomalies for multiple time series at the\nsame time (for example, for multiple combinations of items and stores):",
        "For the training data, create a view or design a query that includes a column that identifies the series, and create\na new anomaly detection object, passing in a reference to that view or query and specifying the name of the series\ncolumn for the SERIES_COLNAME argument.",
        "For the data to analyze, create a view or design a query that includes the column that identifies the series. Call\nthe <model_name>!DETECT_ANOMALIES method, passing in a reference to that view or query and\nspecifying the name of the series column for the SERIES_COLNAME argument.",
        "For example, suppose that you want to use the combination of the store_id and item columns to identify the series:",
        "Create a view or design a query that returns the training data with the column for the series.",
        "For this example, execute the CREATE VIEW command to create a view named\nview_with_training_data_multiple_series that contains a column named store_item that identifies the series as\na combination of store ID and item:",
        "Create an object for the anomaly detection, and train the model on the data in that view.",
        "For this example, execute the CREATE SNOWFLAKE.ML.ANOMALY_DETECTION command to create an anomaly detection object named\nmodel_for_multiple_series, passing in a reference to the new view and specifying store_item for the SERIES_COLNAME\nargument:",
        "Create a view or design a query that returns the data to analyze with the series column.",
        "For this example, execute the CREATE VIEW command to create a view named\nview_with_data_for_analysis_multiple_series that contains a column named store_item for the series:",
        "Using this new anomaly detection object, call the <model_name>!DETECT_ANOMALIES method, passing in the new view and specifying\nstore_item for the SERIES_COLNAME argument:",
        "This statement produces a table that includes rows for the data currently in the view\nview_with_data_for_analysis_multiple_series along with the prediction of the detector. The output includes the column that\nidentifies the series.",
        "Output",
        "The results have been rounded for readability.",
        "Use Snowsight to review and visualize the results of anomaly detection. In Snowsight, when\nyou call the <model_name>!DETECT_ANOMALIES method, the results are displayed in a table under the worksheet.",
        "To visualize the results, you can use the chart feature in Snowsight.",
        "After calling the <model_name>!DETECT_ANOMALIES method, select Charts above the results table.",
        "In the Data section on the right side of the chart:",
        "Select the Y column, and under Aggregation, select None.",
        "Select the TS column, and under Bucketing, select None.",
        "Add the LOWER_BOUND and UPPER_BOUND columns, and under Aggregation, select None.",
        "To display the initial visualization, select Chart.",
        "Select Add Column on the right side of the page, and select the columns you want to visualize:",
        "LOWER_BOUND",
        "UPPER_BOUND",
        "IS_ANOMALY",
        "Results:",
        "Hover over the high spike to see that Y lies outside of the upper bound and is tagged with a 1 in the IS_ANOMALY field.",
        "Tip",
        "To better understand your results, try Top Insights.",
        "You can create an automated anomaly detection pipeline, both for retraining the model and for monitoring your data for anomalies, by using Anomaly Detection functions within Snowflake Tasks or Alerts.",
        "Recurring Training with a Snowflake Task",
        "Monitoring with a Snowflake Task",
        "Monitoring with a Snowflake Alert",
        "You can update your model to reflect the most up-to-date data using Snowflake Tasks.",
        "To create a task that refreshes the anomaly detection object every hour, run following statement, replacing your_warehouse_name with your warehouse name:",
        "By default, newly created tasks are suspended.",
        "To resume the task, execute the ALTER TASK \u2026 RESUME command:",
        "To pause the task, execute the ALTER TASK \u2026 SUSPEND command:",
        "You can also use Snowflake Tasks to monitor your data at a given frequency.",
        "First, create a table to hold the results\nof anomaly detection:",
        "Create a task to store the results of a recurring anomaly detection operation in the table.\nThis example sets the WAREHOUSE parameter to snowhouse. You can replace that with\nyour own warehouse:",
        "To resume the task, execute the ALTER TASK \u2026 RESUME command:",
        "anomaly_res_table then contains all the results for each task run.",
        "To pause the task, execute the ALTER TASK \u2026 SUSPEND command:",
        "You can also use Snowflake Alerts to monitor your data at a given frequency and send you\nemail with detected anomalies. The following statements create an alert that detects anomalies every minute. First you\ndefine a stored procedure to detect anomalies, then create an alert\nthat uses that stored procedure.",
        "Note",
        "You must set up email integration to send mail from a stored procedure; see Notifications in Snowflake.",
        "To start or resume the alert, execute the ALTER ALERT \u2026 RESUME command:",
        "To pause the alert, execute the ALTER ALERT \u2026 SUSPEND command:",
        "An anomaly detection model can explain the relative importance of all features used in your model, including any exogenous\nvariables that you choose, automatically generated time features (such as day of week or week of year), and\ntransformations of your target variable (such as rolling averages and auto-regressive lags). This information is useful\nin understanding what factors are really influencing your data.",
        "The <model_name>!EXPLAIN_FEATURE_IMPORTANCE method counts the number of times the\nmodel\u2019s trees used each feature to make a decision. These feature importance scores are then normalized to values\nbetween 0 and 1 so that their sum is 1. The resulting scores represent an approximate ranking of the features in your\ntrained model.",
        "Features that are close in score have similar importance. For extremely simple series (for example, when the target\ncolumn has a constant value), all feature importance scores may be zero.",
        "Using multiple features that are very similar to each other may result in reduced importance scores for those features.\nFor example, if one feature is quantity of items sold and another is quantity of items in inventory, the values may be\ncorrelated because you can\u2019t sell more than you have and because you try to manage inventory so you won\u2019t have more in\nstock than you will sell. If two features are identical, the model may treat them as interchangeable when making\ndecisions, resulting in feature importance scores that are half of what those scores would be if only one of the\nfeatures were included.",
        "Feature importance also reports lag features. During training, the model infers the frequency (hourly, daily, or weekly)\nof your training data. The feature lagx (e.g. lag24) is the value of the target variable x time units ago.\nFor example, if your data is inferred to be hourly, lag24 represents your target variable 24 hours ago.",
        "All other transformations of your target variable (rolling averages, etc.) are summarized as\naggregated_endogenous_features in the results table.",
        "You cannot choose the technique used to calculate feature importance.",
        "Feature importance scores can be helpful for gaining intuition about which features are important to your model\u2019s\naccuracy, but the actual values should be considered estimates.",
        "To understand the relative importance of your features to your model, train a model, and then call\n<model_name>!EXPLAIN_FEATURE_IMPORTANCE. In this example, you first create random data with\ntwo exogenous variables: one that is random and therefore unlikely to be very important to your model, and one that\nis a copy of your target and therefore likely to be more important to your model.",
        "Execute the following statements to generate the data, train a model on it, and get the importance of the features:",
        "Output",
        "Because this example uses random data, do not expect your output to match this exactly.",
        "When you train multiple series with CONFIG_OBJECT => 'ON_ERROR': 'SKIP', individual time series models can\nfail to train without the overall training process failing. To understand which time series failed and why, call\n<model_instance>!SHOW_TRAINING_LOGS.",
        "Output",
        "For details on costs for using ML functions, see Cost Considerations in the ML functions overview.",
        "Was this page helpful?",
        "On this page",
        "Related content"
    ]
}