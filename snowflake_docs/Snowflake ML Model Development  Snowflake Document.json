{
    "url": "https://docs.snowflake.com/en/developer-guide/snowflake-ml/modeling",
    "title": "Snowflake ML Model Development | Snowflake Documentation",
    "paragraphs": [
        "Note",
        "The Snowflake ML Modeling API is Generally Available as of snowflake-ml-python package version 1.1.1.",
        "The Snowflake ML Modeling API uses familiar Python frameworks such as scikit-learn, LightGBM, and XGBoost for preprocessing data,\nfeature engineering, and training models inside Snowflake.",
        "Benefits of developing models with Snowflake ML Modeling include:",
        "Feature engineering and preprocessing: Improve performance and scalability with distributed execution for\nfrequently-used scikit-learn preprocessing functions.",
        "Model training: Accelerate training for scikit-learn, XGBoost and LightGBM models without the need to manually\ncreate stored procedures or user-defined functions (UDFs), leveraging distributed hyperparameter optimization.",
        "Tip",
        "See Introduction to Machine Learning\nfor an example of an end-to-end ML workflow, including the modeling API.",
        "Note",
        "This topic assumes that snowflake-ml-python and its modeling dependencies are already installed. See\nUsing Snowflake ML Locally.",
        "With Container Runtime for ML, available in\nNotebooks on Container Runtime, you can use popular open-source ML packages\nvith your Snowflake data, leveraging one or more GPU nodes, within the Snowflake cloud, ensuring security and governance\nfor the entire ML workflow. The included data loading and training APIs are automatically distributed across all available\nCPUs or GPUs on a node, acelerating model training with large datasets.",
        "For more information, see Getting Started with Snowflake Notebook Container Runtime,\nwhich presents a simple ML workflow leveraging the capabilities of the Container Runtime for ML.",
        "Along with the flexibility and power of the Container Runtime for ML, the Snowflake ML Modeling API provides estimators and\ntransformers that have APIs similar to those in the scikit-learn, xgboost, and lightgbm libraries. You can use these\nAPIs to build and train machine learning models that can be used with Snowflake ML Operations such as the Snowpark Model\nRegistry.",
        "Review the following examples to get a sense of the similarities of the Snowflake Modeling API to the machine learning\nlibraries you might be familiar with.",
        "This example illustrates the using Snowflake Modeling data preprocessing and transformation functions. The two\npreprocessing functions used in the example (MixMaxScaler and OrdinalEncoder) use Snowflake\u2019s distributed\nprocessing engine to provide significant performance improvements over client-side or stored procedure\nimplementations. For details, see Distributed Preprocessing.",
        "Preview Feature \u2014 Open",
        "Available to all accounts.",
        "This example shows how to load data from a Snowflake table to a pandas DataFrame or a pytorch Dataset using the\nDataConnector API, which disributes data ingestion over multiple cores or GPUs to speed up loading.",
        "Note",
        "The DataConnector API is available in the  Container Runtime for ML and can be\nused from Snowsight notebooks running on Snowpark Container Services (SPCS).",
        "This example shows how to train a simple xgboost classifier model using Snowflake ML Modeling, then run predictions. The\nAPI is similar to xgboost here, with only a few differences in how the columns are specified. For details on these\ndifferences, see General API Differences.",
        "This example uses the high-energy gamma particle data from a ground-based atmospheric Cherenkov telescope. The telescope\nobserves high energy gamma particles, taking advantage of the radiation emitted by charged particles produced in the\nelectromagnetic showers initiated by the gamma rays. The detector records the Cherenkov radiation (of visible to\nultraviolet wavelengths) that leaks through the atmosphere, allowing reconstruction of the gamma shower parameters. The\ntelescope also detects hadron rays that are abundant in cosmic showers and produce signals that mimic gamma rays.",
        "The goal is to develop a classification model for distinguishing between gamma rays and hadron rays. The model enables\nscientists to filter out background noise and focus on the genuine gamma-ray signals. Gamma rays allow scientists to\nobserve cosmic events like the birth and death of stars, cosmic explosions, and the behavior of matter in extreme\nconditions.",
        "The particle data is available for download from MAGIC Gamma Telescope.\nDownload and unzip the data, set the DATA_FILE_PATH variable to point to the data file, and run the code below to load it to Snowflake.",
        "Once you have loaded the data, use the following code to train and predict, using the following steps.",
        "Preprocess the data:",
        "Replace missing values with the mean.",
        "Center the data using a standard scaler.",
        "Train an xgboost classifier to determine the type of events.",
        "Test the accuracy of the model on both training and test datasets.",
        "This example shows how to run distributed hyperparameter optimization using Snowflake\u2019s implementation of\nscikit-learn\u2019s GridSearchCV. The individual runs are executed in parallel using distributed warehouse compute\nresources. For details on distributed hyperparameter optimization, see Distributed Hyperparameter Optimization.",
        "To really see the power of distributed optimization, train on a million rows of data.",
        "All Snowflake modeling and preprocessing classes are in the snowflake.ml.modeling namespace. The\nsnowflake-ml-python modules have the same name as the corresponding modules from the sklearn namespace. For\nexample, the module corresponding to sklearn.calibration is\nsnowflake.ml.modeling.calibration.  The xgboost and lightgbm modules correspond to\nsnowflake.ml.modeling.xgboost and snowflake.ml.modeling.lightgbm, respectively.",
        "The modeling API provides wrappers for underlying scikit-learn, xgboost, and lightgbm classes, the majority\nof which are executed as stored procedures (running on a single warehouse node) in the virtual warehouse. Not all of the\nclasses from scikit-learn are supported. See the Python API Reference\nfor a list of the classes currently available.",
        "Some classes (including preprocessing and metrics classes) support distributed execution and may provide significant\nperformance benefits compared to running the same operations locally.  For more information, see\nDistributed Preprocessing and\nDistributed Hyperparameter Optimization. The table below lists the specific classes that support\ndistributed execution.",
        "snowflake-ml-python module name",
        "Distributed classes",
        "snowflake.ml.modeling.impute",
        "SimpleImputer",
        "snowflake.ml.modeling.metrics",
        "correlation:",
        "correlation",
        "covariance:",
        "covariance",
        "classification:",
        "accuracy_score",
        "confusion_matrix",
        "f1_score",
        "fbeta_score",
        "log_loss",
        "precision_recall_fscore_support",
        "precision_score",
        "recall_score",
        "regression:",
        "mean_absolute_error",
        "mean_absolute_percentage_error",
        "mean_squared_error",
        "snowflake.ml.modeling.model_selection",
        "GridSearchCV",
        "RandomizedSearchCV",
        "snowflake.ml.modeling.preprocessing",
        "Binarizer",
        "KBinsDiscretizer",
        "LabelEncoder",
        "MaxAbsScaler",
        "MinMaxScaler",
        "Normalizer",
        "OneHotEncoder",
        "OrdinalEncoder",
        "RobustScaler",
        "StandardScaler",
        "Tip",
        "See the API Reference\nfor complete details of the modeling API.",
        "Snowflake modeling classes includes data preprocessing, transformation, and prediction algorithms based on scikit-learn,\nxgboost, and lightgbm. The Snowpark Python classes are replacements for the corresponding classes from the original\npackages, with similar signatures. However, these APIs are designed to work with Snowpark DataFrames instead of NumPy\narrays.",
        "Although the API is similar to scikit-learn, there are some key differences. This section explains how to call the\n__init__ (constructor), fit, and predict methods for the Snowflake estimator and transformer classes.",
        "The constructor of all Snowflake model classes accepts five\nadditional parameters (input_cols, output_cols, sample_weight_col, label_cols, and\ndrop_input_cols) in addition to the parameters accepted by the equivalent classes in scikit-learn, xgboost, or\nlightgbm. These are strings or sequences of strings that specify the names of the input columns, output columns,\nsample weight column, and label columns in a Snowpark or Pandas DataFrame. If some of the datasets you use have\ndifferent names, you can change these names after instantiation using one of the provided setter methods, such as\nset_input_cols.",
        "Because you specify column names when instantiating the class (or afterward, using setter methods) the fit and\npredict methods accept a single DataFrame instead of separate arrays for inputs,\nweights, and labels. The provided column names are used to access the appropriate column from the DataFrame in\nfit or predict. See fit and\npredict.",
        "By default, the transform and predict methods return a DataFrame containing all of the\ncolumns from the DataFrame passed to the method, with the output from the prediction stored in additional columns. You\ncan transform in place by specifying output column names that match the input column names, or drop the input columns\nby passing drop_input_cols = True.) The scikit-learn, xgboost, and lightgbm equivalents return arrays\ncontaining only the results.",
        "Snowpark Python transformers do not have a fit_transform method. However, as with scikit-learn, parameter\nvalidation is only performed in the fit method, so you should call fit at some point before\ntransform, even when the transformer does not do any fitting. fit returns the transformer, so the method\ncalls may be chained; for example, Binarizer(threshold=0.5).fit(df).transform(df).",
        "Snowflake transformers do not currently have an inverse_transform method. In many use cases, this method is\nunnecessary because the input columns are retained in the output dataframe by default.",
        "You can convert any Snowfalke modeling object to the corresponding scikit-learn, xgboost, or lightgbm object, allowing\nyou to use all the methods and attributes of the underlying type. See Retrieving the Underlying Model.",
        "In addition to the parameters accepted by individual scikit-learn model classes, all modeling classes accept\nthe following additional parameters at instantiation.",
        "These parameters are all technically optional, but you will often want to specify input_cols,\noutput_cols, or both. label_cols and sample_weight_col are required in specific situations noted\nin the table, but can be omitted in other cases.",
        "Tip",
        "All column names must follow the Snowflake identifier requirements. To\npreserve case or use special characters (besides dollar sign and underscore) when creating a table, column names must\nbe wrapped in double quotes. Use all-caps column names whenever possible to maintain compatibility with case-sensitive\nPandas DataFrames.",
        "Parameter",
        "Description",
        "input_cols",
        "A string or list of strings representing column names that contain features.",
        "If you omit this parameter, all columns in the input DataFrame, except the columns specified by label_cols,\nsample_weight_col, and passthrough_cols parameters, are considered input columns.",
        "label_cols",
        "A string or list of strings representing the names of columns that contain labels.",
        "You must specify label columns for supervised estimators because inferring these columns is not possible. These label\ncolumns are used as targets for model predictions and should be clearly distinguished from input_cols.",
        "output_cols",
        "A string or list of strings representing the names of columns that will store the output of predict and\ntransform operations. The length of output_cols must match the expected number of output columns from the\nspecific predictor or transformer class used.",
        "If you omit this parameter, output column names are derived by adding an OUTPUT_ prefix to the label\ncolumn names for supervised estimators, or OUTPUT_IDX for unsupervised estimators. These inferred output\ncolumn names work for predictors, but output_cols must be set explicitly for transformers. In general,\nexplicitly specifying output column names is clearer, especially if you don\u2019t specify the input column names.",
        "To transform in place, pass the same names for input_cols and output_cols.",
        "passthrough_cols",
        "A string or a list of strings indicating names of columns to exclude from training, transformation, and inference.\nPassthrough columns remain untouched between the input and output DataFrames.",
        "This option is helpful where you want to avoid using specific columns, such as index columns, during training or\ninference, but do not pass input_cols. When you do not pass input_cols, those columns would\nordinarily be considered inputs.",
        "sample_weight_col",
        "A string representing the column name containing the examples\u2019 weights.",
        "This argument is required for weighted datasets.",
        "drop_input_cols",
        "A Boolean value indicating whether the input columns are removed from the result DataFrame. The default is\nFalse.",
        "The DecisionTreeClassifier constructor does not have any required arguments in scikit-learn; all arguments have default\nvalues. So in scikit-learn, you might write:",
        "In Snowflake\u2019s version of this class, you must specify the column names (or accept the defaults by not specifying them). In this example, they\nare explicitly specified.",
        "You can initialize a DecisionTreeClassifier by passing the arguments directly to the constructor or\nby setting them as attributes of the model after instantiation. (The attributes may be changed at any time.)",
        "As constructor arguments:",
        "By setting model attributes:",
        "The fit method of a Snowflake classifier takes a single Snowpark or Pandas DataFrame containing all columns,\nincluding features, labels, and weights. This is different from scikit-learn\u2019s fit method, which takes separate\ninputs for features, labels, and weights.",
        "In scikit-learn, the DecisionTreeClassifier.fit method call looks like this:",
        "In Snowflake\u2019s fit, you only need to pass the DataFrame. You have already set the input, label, and weight column names at\ninitialization or by using setter methods, as shown in Constructing a Model.",
        "The predict method also takes a single Snowpark or Pandas DataFrame containing all\nfeature columns. The result is a DataFrame that contains all the columns in the input DataFrame unchanged and the output\ncolumns appended. You must extract the output columns from this DataFrame. This is different from the predict\nmethod in scikit-learn, which returns only the results.",
        "In scikit-learn, predict returns only the prediction results:",
        "To get only the prediction results in Snowflake\u2019s predict, extract the output columns from the returned DataFrame. Here,\noutput_column_names is a list containing the names of the output columns:",
        "Preview Feature \u2014 Open",
        "Available to all accounts.",
        "When running in a Snowflake Notebook on Snowpark Container Services (SPCS), model training and inference for these\nmodeling classes are executed on the underlying compute cluster, not in a warehouse, and are transparently distributed\nacross all nodes in the cluster to employ all available compute capability.",
        "Preprocessing and metrics operations are pushed down to the warehouse. Many preprocessing classes support distributed\nexecution when run in the warehouse; see Distributed Preprocessing.",
        "Many Snowflake data preprocessing and transformation functions are implemented using Snowflake\u2019s distributed\nexecution engine, which yields significant performance benefit compared to single-node execution (that is, stored\nprocedures). To find out which functions support distributed execution, see Snowflake Modeling Classes.",
        "The chart below shows illustrative performance numbers on large public datasets, running in a medium Snowpark-optimized\nwarehouse, comparing scikit-learn running in stored procedures to Snowflake\u2019s distributed implementations. In mary\nscenarios, your code can run 25 to 50 times faster when using Snowflake modeling classes.",
        "The fit method of a Snowflake preprocessing transformer accepts a Snowpark or pandas DataFrame, fits the dataset, and returns the fitted transformer.",
        "For Snowpark DataFrames, distributed fitting uses the SQL engine. The transformer generates SQL queries to\ncompute the necessary states (such as mean, maximum, or count). These queries are then executed by Snowflake, and the\nresults are materialized locally. For complex states that cannot be computed in SQL, the transformer fetches intermediate results from Snowflake and\nperforms local computations over metadata.",
        "For complex transformers that require temporary state tables during transformation (for example, OneHotEncoder or\nOrdinalEncoder), these tables are represented locally using pandas DataFrames.",
        "pandas DataFrames are fitted locally, similar to fitting with scikit-learn. The transformer creates a corresponding\nscikit-learn transformer with the provided parameters. Then the scikit-learn transformer is fitted, and the Snowflake\ntransformer derives necessary states from the scikit-learn object.",
        "The transform method of a preprocessing transformer accepts a Snowpark or Pandas DataFrame, transforms the\ndataset, and returns a transformed dataset.",
        "For Snowpark DataFrames, distributed transformation is performed using the SQL engine. The fitted transformer generates\na Snowpark DataFrame with underlying SQL queries representing the transformed dataset. The transform method performs lazy\nevaluation for simple transforms (for example, StandardScaler or MinMaxScaler), so that no transform is actually\nperformed during the transform method.",
        "However, certain complex transforms involve execution. This includes transformers that require temporary state tables\n(such as OneHotEncoder and OrdinalEncoder) during transformation. For such a transformer, the transformer\ncreates a temporary table from the Pandas DataFrame (which stores the state of the object) for joins and other\noperations.",
        "Furthermore, when certain parameters are set, for example when the transformer is set to handle unknown values found\nduring transformation by raising errors, the transformer materializes the data, including columns, unknown values, and\nso forth.",
        "Pandas DataFrames are transformed locally, similar to transformation with scikit-learn. The transformer creates a\ncorresponding scikit-learn transformer using the to_sklearn API and performs the transform in memory.",
        "Hyperparameter tuning is an integral part of the data science workflow. The Snowflake API provides distributed\nimplementations of the scikit-learn GridSearchCV and RandomizedSearchCV APIs to enable efficient hyperparameter\ntuning on both single-node and multiple-node warehouses.",
        "Tip",
        "Snowflake enables distributed hyperparameter optimization by default. To disable it, use the following Python import.",
        "The smallest Snowflake virtual warehouse (XS) or Snowpark-optimized warehouse (M) has one node. Each successively larger\nsize doubles the number of nodes.",
        "For single-node (XS) warehouses, the full capacity of the node is utilized by default using scikit-learn\u2019s joblib\nmultiprocessing framework.",
        "Tip",
        "Each fit operation requires its own copy of that training dataset loaded into RAM. To process extremely large datasets, disable distributed hyperparameter\noptimization (with import snowflake.ml.modeling.parameters.disable_distributed_hpo) and set the n_jobs\nparameter to 1 to minimize concurrency.",
        "For multiple-node warehouses, the fit operations within your cross-validation tuning job are distributed across the\nnodes. No code changes are required to scale up. Estimator fits are executed in parallel across all available cores on\nall nodes in the warehouse.",
        "As an illustration, consider the California housing dataset\nprovided with the scikit-learn library. The data includes 20,640 rows of data with the following information:",
        "MedInc: Median income in the block group",
        "HouseAge: Median house age in the block group",
        "AveRooms: Ave number of rooms per household",
        "AveBedrms: Average number of bedrooms per household",
        "Population: The block group population",
        "AveOccup: Average number of household members",
        "Latitude and Longitude",
        "The target of the dataset is the median income, expressed in hundreds of thousands of dollars.",
        "In this example, we do grid search cross-validation on a random forest regressor to find the best hyperparameter\ncombination to predict the median income.",
        "This example runs in just over 7 minutes on a Medium (single node) Snowpark-optimized warehouse, and takes just 3\nminutes to run on an X-Large warehouse.",
        "The result of training a model is a Python model object. You can use the trained model to make predictions by\ncalling the model\u2019s predict method. This creates a temporary user-defined function to run the model in your Snowflake\nvirtual warehouse. This function is automatically deleted at the end of your Snowflake session (for example, when your\nscript ends or when you close your notebook).",
        "To keep the user-defined function after your session ends, you can create it manually.  See the\nQuickstart\non the topic for further information.",
        "The Snowflake model registry also supports persistent models and makes finding and deploying them easier. See Snowflake Model Registry.",
        "The model registry also supports a special type of custom model where fit and inference are executed in parallel for a set of partitions. This can be a performant way to create many models at once from one dataset and execute inference immediately. Please see Snowflake Model Registry: Partitioned Models for more details.",
        "With scikit-learn, it is common to run a series of transformations using a pipeline. scikit-learn pipelines do not work\nwith Snowflake classes, so a Snowflake version of sklearn.pipeline.Pipeline is provided for\nrunning a series of transformations. This class is in the snowflake.ml.modeling.pipeline package, and it works\nthe same as the scikit-learn version.",
        "Snowflake ML models can be \u201cunwrapped,\u201d that is, converted to the underlying third-party model types, with the following\nmethods (depending on the library):",
        "to_sklearn",
        "to_xgboost",
        "to_lightgbm",
        "All attributes and methods of the underlying model can then be accessed and run locally against the estimator. For example, in the\nGridSearchCV example, we convert the grid\nsearch estimator to a scikit-learn object in order to retrieve the best score.",
        "Snowflake estimators and transformers do not currently support sparse inputs or sparse responses. If you have\nsparse data, convert it to a dense format before passing it to Snowflake\u2019s estimators or transformers.",
        "The snowflake-ml-python package does not currently support matrix data types. Any operation on estimators and transformers that\nwould produce a matrix as a result fails.",
        "The order of rows in result data is not guaranteed to match the order of rows in input data.",
        "Snowflake ML does not yet support pandas on Snowflake\nDataFrames. Convert the Pandas on Snowflake dataframe to a Snowpark dataframe to use it with the Snowflake modeling\nclasses. The following example converts a DataFrame we have read from a Snowflake table:",
        "The resulting Snowpark DataFrame is as follows:",
        "The DataFrame can then be used to train the an XGBoost classifier as follows:",
        "The Snowflake modeling library uses Snowpark Python\u2019s logging. By default, snowflake-ml-python logs INFO level messages to standard output. To get\nmore detailed logs, you can change the level to one of the\nsupported levels.",
        "DEBUG produces logs with the most details. To set the logging level to DEBUG:",
        "The following table provides some suggestions for solving possible problems with Snowflake ML Modeling.",
        "Problem or error message",
        "Possible cause",
        "Resolution",
        "NameError, such as \u201cname x is not defined,\u201d ImportError, or ModuleNotFoundError",
        "Typographical error in module or class name, or snowflake-ml-python is not installed.",
        "Refer to the modeling classes table for the correct module and class name. Ensure that snowflake-ml-python is installed\n(see Using Snowflake ML Locally).",
        "KeyError (\u201cnot in index\u201d or \u201cnone of [Index[..]] are in the [columns]\u201d)",
        "Incorrect column name.",
        "Check and correct the column name.",
        "SnowparkSQLException, \u201cdoes not exist or not authorize\u201d",
        "Table does not exist, or you do not have sufficient privileges on the table.",
        "Ensure that the table exists and that the user\u2019s role has the privileges.",
        "SnowparkSQLException, \u201cinvalid identifier PETALLENGTH\u201d",
        "Incorrect number of columns (usually a missing column).",
        "Check the number of columns specified when you created the model class, and ensure that you are passing the right number.",
        "InvalidParameterError",
        "An inappropriate type or value has been passed as a parameter.",
        "Check the class\u2019s or method\u2019s help using the help function in an interactive Python session, and correct the values.",
        "TypeError, \u201cunexpected keyword argument\u201d",
        "Typographical error in named argument.",
        "Check the class\u2019s or method\u2019s help using the help function in an interactive Python session, and correct\nthe argument name.",
        "ValueError, \u201carray with 0 sample(s)\u201d",
        "The dataset that was passed in is empty.",
        "Ensure that the dataset is not empty.",
        "SnowparkSQLException, \u201cauthentication token has expired\u201d",
        "The session has expired.",
        "If you\u2019re using a Jupyter notebook, restart the kernel to create a new session.",
        "ValueError, such as \u201ccannot convert string to float\u201d",
        "Data type mismatch.",
        "Check the class\u2019s or method\u2019s help using the help function in an interactive Python session, and correct the values.",
        "SnowparkSQLException, \u201ccannot create temporary table\u201d",
        "A model class is being used inside a stored procedure that doesn\u2019t run with the caller\u2019s rights.",
        "Create the stored procedure with the caller\u2019s rights instead of with the owner\u2019s rights.",
        "SnowparkSQLException, \u201cfunction available memory exceeded\u201d",
        "Your data set is larger than 5 GB in a standard warehouse.",
        "Switch to a Snowpark-optimized warehouse.",
        "OSError, \u201cno space left on device\u201d",
        "Your model is larger than about 500 MB in a standard warehouse.",
        "Switch to a Snowpark-optimized warehouse.",
        "Incompatible xgboost version or error when importing xgboost",
        "You installed using pip, which does not handle dependencies well.",
        "Upgrade or downgrade the package as requested by the error message.",
        "AttributeError involving to_sklearn, to_xgboost, or to_lightgbm",
        "An attempt to use one of these methods on a model of a different type.",
        "Use to_sklearn with scikit-learn-based models, etc.",
        "Jupyter notebook kernel crashes on an arm-based Mac (M1 or M2 chip): \u201cThe Kernel crashed while executing code in the current cell or a previous cell.\u201d",
        "XGBoost or another library is installed with the incorrect architecture.",
        "Recreate new conda environment with CONDA_SUBDIR=osx-arm64 conda create --name snowpark-ml\nand reinstall the Snowflake ML package.",
        "\u201clightgbm.basic.LightGBMError: (0000) Do not support special JSON characters in feature name.\u201d",
        "LightGBM doesn\u2019t support double quoted column names in input_cols, label_cols, or output_cols.",
        "Rename the columns in your Snowpark DataFrames. Replacing non-alphanumeric characters with underscores is\nsufficient in most cases. The Python helper function below may be useful.",
        "See the documentation of the original libraries for complete information on their functionality.",
        "Scikit-Learn",
        "XGBoost",
        "LightGBM",
        "Some parts of this document are derived from the Scikit-learn documentation, which is licensed under the\nBSD-3 \u201cNew\u201d or \u201cRevised\u201d license and Copyright \u00a9 2007-2023 The scikit-learn developers. All rights reserved.",
        "Some parts of this document are derived from the XGboost documentation, which is covered by Apache License 2.0, January\n2004 and Copyright \u00a9 2019. All rights reserved.",
        "Some parts of this document are derived from the LightGBM documentation, which is MIT-licensed and Copyright \u00a9\nMicrosoft Corp. All rights reserved.",
        "Was this page helpful?",
        "On this page"
    ]
}