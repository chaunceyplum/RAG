{
    "url": "https://docs.snowflake.com/en/user-guide/kafka-connector-iceberg",
    "title": "Using the Snowflake Connector for Kafka with Apache Iceberg\u2122 tables | Snowflake Documentation",
    "paragraphs": [
        "Beginning with version 3.0.0, the Snowflake Connector for Kafka can ingest data into a\nSnowflake-managed Apache Iceberg\u2122 table.",
        "Before you configure the Kafka connector for Iceberg table ingestion, note the following requirements and limitations:",
        "Iceberg table ingestion requires version 3.0.0 or later of the Kafka connector.",
        "Iceberg table ingestion is supported by the Kafka connector with Snowpipe Streaming. It\u2019s not supported by the Kafka connector with Snowpipe.",
        "Iceberg table ingestion is not supported when snowflake.streaming.enable.single.buffer is set to false.",
        "You must create an Iceberg table before running the connector. For more information, see Configuration and setup in this topic.",
        "Schema evolution for Iceberg is fully supported for schematized data formats like AVRO or Protobuf.",
        "For plain JSON without a schema, the connector considers the following message types invalid and sends them to\ndead-letter queues (DLQ):",
        "Messages with a new column if the corresponding value is null or []",
        "Messages with a new field in a structured object if the corresponding value is null or []",
        "To manually change the table schema so that the connector can ingest these message types, use an ALTER TABLE statement.",
        "To configure the Kafka connector for Iceberg table ingestion, you follow the\nregular setup steps for a Snowpipe Streaming-based connector\nwith a few differences noted in the following sections.",
        "You must grant the USAGE privilege on the external volume associated with your Iceberg table to your role for the Kafka connector.",
        "For example, if your Iceberg table uses the kafka_external_volume external volume\nand the connector uses the role kafka_connector_role, run the following statement:",
        "Before you run the connector, you must create an Iceberg table.\nThe initial table schema depends on your connector snowflake.enable.schematization settings.",
        "If you enable schematization, you can create a table with a column named record_metadata:",
        "The connector automatically creates the record_content column and alters the record_metadata column schema.",
        "If you don\u2019t enable schematization, you can create a table with a column named record_content of a type that matches the actual Kafka message content.\nThe connector automatically creates the record_metadata column.",
        "When you create an Iceberg table, you can use Iceberg data types or compatible Snowflake types.\nThe semi-structured VARIANT type isn\u2019t supported. Instead, use a  structured OBJECT or MAP.",
        "For example, consider the following message:",
        "To create an Iceberg table for the example message, use the following statement:",
        "Note",
        "Field names inside nested structures such as dogs or cats are case sensitive.",
        "Specifies whether the connector ingests data into an Iceberg table. The connector fails if this property doesn\u2019t match the actual table type.",
        "",
        "true",
        "false",
        "false",
        "Was this page helpful?",
        "On this page",
        "Related content"
    ]
}