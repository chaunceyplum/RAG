{
    "url": "https://docs.snowflake.com/en/user-guide/tables-iceberg-conversion",
    "title": "Convert an Apache Iceberg\u2122 table to use Snowflake as the catalog | Snowflake Documentation",
    "paragraphs": [
        "Convert an Apache Iceberg\u2122 table that Snowflake doesn\u2019t manage into a\ntable that uses Snowflake as the Iceberg catalog.",
        "You might choose to convert a table when you want full Snowflake platform support,\nincluding support for the Snowflake Catalog SDK.",
        "To learn about the differences between Iceberg table types, see Catalog options.",
        "When you convert an Iceberg table to use Snowflake as the catalog, the table becomes writable and Snowflake assumes life-cycle management\nfor it.",
        "The following table compares Iceberg tables before and after conversion:",
        "Before conversion",
        "After conversion",
        "Iceberg catalog",
        "An external catalog (such as AWS Glue), or no catalog at all. Requires a catalog integration.",
        "Snowflake. Snowflake registers changes to the source data and registers the changes in the Snowflake catalog.\nSnowflake then updates the table metadata on your external volume.",
        "Does not require a catalog integration.",
        "Snowflake read operations",
        "\u2714",
        "\u2714",
        "Snowflake write operations",
        "\u274c",
        "\u2714",
        "Storage location for table data and metadata",
        "External volume (external cloud storage).",
        "External volume (external cloud storage) under a base location that you specify.",
        "Data and metadata cleanup",
        "Managed by you or your external catalog.",
        "Managed by Snowflake. Snowflake never deletes any metadata, manifest lists, or manifests created before conversion from your external storage.\nSnowflake doesn\u2019t rewrite any Parquet data files during conversion. After you convert a table,\nSnowflake might rewrite some of the data files as part of regular table maintenance.",
        "Accessible from the Snowflake Catalog SDK",
        "\u274c",
        "\u2714",
        "Important",
        "When you convert an Iceberg table, Snowflake doesn\u2019t lock down or assume sole access to your external storage.\nTo prevent table corruption, ensure that you monitor or stop any non-Snowflake writes\n(such as automated maintenance jobs) to your external storage location.",
        "Before you convert an Iceberg table, ensure that Snowflake can write to your external volume.",
        "For Snowflake to write to your external volume, the following conditions must be met:",
        "Use the ALTER ICEBERG TABLE \u2026 REFRESH command to manually refresh the table before you convert it.",
        "The ALLOW_WRITES property for your external volume is set to TRUE. To update the value of this property for an existing\nexternal volume, use the ALTER EXTERNAL VOLUME command.\nFor example: ALTER EXTERNAL VOLUME my_ext_vol SET ALLOW_WRITES=TRUE.",
        "The access control permissions that you set on the cloud storage account must allow write access. For example,\nif you use an external volume configured for Amazon S3, your IAM role must have the s3:PutObject permission for your S3 location.",
        "Note",
        "Converting a table that has an un-materialized identity partition column isn\u2019t supported.\nAn un-materialized identity partition column is created when a table defines an identity transform\nusing a source column that doesn\u2019t exist in a Parquet file.",
        "Important",
        "When you convert an Iceberg table, Snowflake doesn\u2019t lock down or assume sole access to your external storage.\nTo prevent table corruption, ensure that you monitor or stop any non-Snowflake writes\n(such as automated maintenance jobs) to your external storage location.",
        "This example starts by creating an Iceberg table from Iceberg files in object storage.\nSnowflake uses the METADATA_FILE_PATH value to look for the table metadata in the following location for column definitions:\n<ext-vol-storage-base-url>/path/to/metadata/v1.metadata.json.",
        "Next, use the ALTER ICEBERG TABLE \u2026 REFRESH command to synchronize the table metadata with the latest metadata file.\nThe following example command refreshes the table by specifying a metadata file path.",
        "Finally, convert the table to use Snowflake as the Iceberg catalog by using the\nALTER ICEBERG TABLE \u2026 CONVERT TO MANAGED command.",
        "Note",
        "In this example, the ALTER statement must specify a BASE_LOCATION because the table was created\nfrom Iceberg files in object storage and BASE_LOCATION was not part of the original CREATE ICEBERG TABLE statement.\nThe BASE_LOCATION defines the relative path from your external\nvolume to a directory where Snowflake writes table data and metadata for the converted table.",
        "Otherwise, if BASE_LOCATION was specified in the original CREATE ICEBERG TABLE statement, you don\u2019t need to include it in your\nALTER ICEBERG TABLE \u2026 CONVERT TO MANAGED command.",
        "For example, Snowflake writes table data to\n<ext-vol-storage-base-url>/myBaseLocation/data/.",
        "Snowflake writes metadata for the converted table to <ext-vol-storage-base-url>/myBaseLocation/metadata/.",
        "Note",
        "You can\u2019t convert a table the uses the following Iceberg data types:",
        "uuid",
        "fixed(L)",
        "Snowflake uses Snowflake data types to process and return values,\nbut writes the original Iceberg types to table data files.",
        "For data types such as int and long, the Snowflake data type supports a larger range of values than the Iceberg data type.\nTo stay consistent with the source data type, Snowflake does not allow inserting values outside the range that the\nsource data type supports. For more information, see Approximate types.",
        "Was this page helpful?",
        "On this page",
        "Related content"
    ]
}