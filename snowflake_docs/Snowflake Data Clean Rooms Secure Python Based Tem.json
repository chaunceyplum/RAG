{
    "url": "https://docs.snowflake.com/en/user-guide/cleanrooms/demo-flows/python-based-templates",
    "title": "Snowflake Data Clean Rooms: Secure Python Based Templates | Snowflake Documentation",
    "paragraphs": [
        "Feature \u2014 Generally Available",
        "Not available in government regions.",
        "This topic describes the provider and consumer flows needed to programmatically set up a clean room, share it with a consumer, and run analyses on it in a way that uses secure Python UDFs loaded into the clean room. In this flow, a provider loads secure Python code into the clean room using an API that keeps the underlying Python code completely confidential from the consumer.",
        "This flow loads two Python functions into the clean room to do some custom data processing and aggregation. These Python UDFs are then called inside a custom SQL Jinja template that acts as glue for the data flow. The template itself calculates an aggregation along a custom grouping created by the Python UDFs.",
        "The key aspects of this flow other than the ones mentioned above are:",
        "Provider:",
        "a. Securely load 2 confidential Python UDFs into a new clean room.",
        "b. Create a custom SQL Jinja analysis template using the Python UDFs.",
        "c. Share it with a consumer.",
        "Consumer:",
        "a. Examine the template provided within the clean room.",
        "b. Run an analysis within the clean room using the template.",
        "You need two separate Snowflake accounts to complete this flow. Use the first account to execute the provider\u2019s commands, then switch to the second account to execute the consumer\u2019s commands.",
        "Note",
        "The following commands should be run in a Snowflake worksheet in the provider account.",
        "Execute the following commands to set up the Snowflake environment before using developer APIs to work with a Snowflake Data Clean Room. If you don\u2019t have the SAMOOHA_APP_ROLE role, contact your account administrator.",
        "Create a name for the clean room. Enter a new clean room name to avoid colliding with existing clean room names. Note that clean room names can only be alphanumeric. clean room names cannot contain special characters other than spaces and underscores.",
        "You can create a new clean room with the clean room name set above. If the clean room name set above already exists as an existing clean room, this process fails.",
        "This procedure takes approximately 45 seconds to run.",
        "The second argument to provider.cleanroom_init is the distribution of the clean room. This can either be INTERNAL or EXTERNAL. For testing purposes, if you are sharing the clean room to an account in the same organization, you can use INTERNAL to bypass the automated security scan which must take place before an application package is released to collaborators. However, if you are sharing this clean room to an account in a different organization, you must use an EXTERNAL clean room distribution.",
        "In order to view the status of the security scan, use:",
        "Once you have created your clean room, you must set its release directive before it can be shared with any collaborator. However, if your distribution was set to EXTERNAL, you must first wait for the security scan to complete before setting the release directive. You can continue running the remainder of the steps while the scan runs and return here before the provider.create_or_update_cleanroom_listing step.",
        "In order to set the release directive, call:",
        "In order to share a clean room with a Snowflake customer whose account is in a different region than your account, you must enable Cross-Cloud Auto-Fulfillment. For information about the additional costs associated with collaborating with consumers in other regions, see Cross-Cloud Auto-Fulfillment costs.",
        "When using developer APIs, enabling cross-region sharing is a two-step process:",
        "A Snowflake administrator with the ACCOUNTADMIN role enables Cross-Cloud Auto-Fulfillment for your Snowflake account. For instructions, see Collaborate with accounts in different regions.",
        "You execute the provider.enable_laf_for_cleanroom command to enable Cross-Cloud Auto-Fulfillment for the clean room. For example:",
        "After you have enabled Cross-Cloud Auto-Fulfillment for the clean room, you can add consumers to your listing as usual using the provider.create_or_update_cleanroom_listing command. The listing is automatically replicated to remote clouds and regions as needed.",
        "Link Snowflake tables into the clean room. Browse through the list of tables in your Snowflake account and enter the fully qualified table names (Database.Schema.Table) as an array. The procedure automatically makes the table accessible to the clean room by creating a secure view of the table from within the clean room, thereby avoiding any need to make a copy of your table.",
        "Note",
        "If this step doesn\u2019t work even though your table exists, it is likely the SAMOOHA_APP_ROLE role has not yet been given access to it. If so, switch to the ACCOUNTADMIN role, call the below procedure on the database, and then switch back for the rest of the flow:",
        "You can see the datasets linked to the clean room using the following procedure:",
        "In order to figure out which columns to use as the join policy, you can look at your dataset to determine the PII columns. To see the top 10 rows, use this query:",
        "Specify which columns the consumer is allowed to join on when running templates within the clean room. This procedure should be called on identity columns like email. The join policy is \u201creplace only\u201d, so if the function is called again, then the previously set join policy is completely replaced by the new one.",
        "If you want to view the join policy that has been added to the clean room, call the following procedure.",
        "This section shows you how to load the following Python functions into the clean room.",
        "assign_group -> a UDF that goes row by row and assigns a group ID.",
        "group_agg -> a UDF that groups by the ID and aggregates an aspect of the data.",
        "The following API allows you to define your Python functions directly as inline functions into the clean room. Alternatively you can load Python from staged files you\u2019ve uploaded into the clean room stage. See the API reference guide for an example.",
        "The following code defines and loads the assign_group UDF that goes row by row and assigns a group ID:",
        "The following code defines and loads the group_agg UDF that groups by the ID and aggregates an aspect of the data:",
        "Note",
        "Loading Python into the clean room creates a new patch for the clean room. If your clean room distribution is set to EXTERNAL, you need to wait for the security scan to complete, then update the default release directive using:",
        "Note",
        "This section is an optional alternative to the load_python_into_cleanroom commands above which define Python inline. These load Python from .py files instead, loaded into the clean room stage.",
        "As an alternative to defining, you can load Python from .py files in a stage. In order to do this, you must upload your code into the clean room code stage. Importantly, only the files in the clean room code stage are available to the clean room for use, so your files cannot be located elsewhere. The files must be in the following stage:",
        "In order to define the assign_group and group_agg UDFs in this way, you can upload the following scripts into the clean room stage:",
        "Create a file in your home directory called ~/assign_group.py and paste in the following code:",
        "Now, you need to upload the code to the clean room stage. Do this by adding it to the folder containing the version of the clean room application files that is currently published. In order to get the necessary folder, you can use the following procedure:",
        "This gives you the stage to upload this file to. You can upload this file to the stage using the following command from snowsql:",
        "Finally, you can load Python into the clean room using the following command:",
        "In a similar manner, you can create a file called ~/group_agg.py with the following code:",
        "The folder to which this needs to be uploaded will now have changed since the last call to load_python_into_cleanroom has added a patch to the cleanroom. The new folder can be obtained by rerunning the following command:",
        "The file can then be uploaded to the appropriate folder:",
        "Once uploaded, the Python UDF can be created from this file using the following command:",
        "To add a custom analysis template to the clean room you need a placeholder for table names on both the provider and consumer sides, along with join columns from the provider side. In SQL Jinja templates, these placeholders must always be:",
        "source_table: an array of table names from the provider",
        "my_table: an array of table names from the consumer",
        "Table names can be made dynamic through using these variables, but they can also be hardcoded into the template if desired using the name of the view linked to the cleanroom. Column names can either be hardcoded into the template, if desired, or set dynamically through parameters. If they are set through parameters, remember that you need to call the parameters dimensions or measure_column, which need to be arrays, in order for them to be checked against the column policy. You add these as SQL Jinja parameters in the template that will be passed in later by the consumer when querying. The join policies ensure that the consumer cannot join on columns other than the authorized ones.",
        "Alternatively, any argument in a custom SQL Jinja template can be checked for compliance with the join and column policies using the following filters:",
        "join_policy: checks if a string value or filter clause is compliant with the join policy",
        "column_policy: checks if a string value or filter clause is compliant with the column policy",
        "join_and_column_policy: checks if columns used for a join in a filter clause are compliant with the join policy, and that columns used as a filter are compliant with the column policy",
        "For example, in the clause {{ provider_id | sqlsafe | join_policy }}, an input of p.HEM will be parsed to check if p.HEM is in the join policy. Note: Only use the sqlsafe filter with caution as it allows collaborators to put pure SQL into the template.",
        "Note",
        "All provider/consumer tables must be referenced using these arguments since the name of the secure view actually linked to the cleanroom will be different to the table name. Critically, provider table aliases MUST be p (or p1), p2, p3, p4, etc. and consumer table aliases must be c (or c1), c2, c3, etc. This is required in order to enforce security policies in the cleanroom.",
        "Note that this function overrides any existing template with the same name. If you want to update any existing template, you can simply call this function again with the updated template.",
        "This template first enriches the provider data with a hash of a series of columns from the provider\u2019s table. This enriched data is then inner-joined onto the consumer\u2019s dataset on email, with an optional filter clause passed in. Finally, the custom Python group_agg UDF is used to calculate some aggregation as a function of the hashed columns from the first UDF.",
        "Note",
        "You can add Differential Privacy sensitivity to samooha_by_snowflake_local_db.provider.add_custom_sql_template procedure call above as the last parameter (if you do not add it, it will default to 1)",
        "If you want to view the templates that are currently active in the clean room, call the following procedure.",
        "Display the data linked to see the columns present inside the table. To view the top 10 rows, call the following procedure.",
        "Set the columns the consumer can group, aggregate (e.g. SUM/AVG) and generally use in an analysis for every table and template combination. This gives flexibility so the same table can allow different column selections depending on the underlying template. This should only be called after adding the template.",
        "Note that the column policy is replace only, so if the function is called again, then the previously set column policy is completely replaced by the new one.",
        "Column policy should not be used on identity columns like email, HEM, RampID, etc. since you don\u2019t want the consumer to be able to group by these columns. In the production environment, the system will intelligently infer PII columns and block this operation, but this feature is not available in the sandbox environment. It should only be used on columns that you want the consumer to be able to aggregate and group by, like Status, Age Band, Region Code, Days Active, etc.",
        "Note that for the \u201ccolumn_policy\u201d and \u201cjoin_policy\u201d to carry out checks on the consumer analysis requests, all column names MUST be referred to as  dimensions or measure_columns in the SQL Jinja template. Make sure you use these tags to refer to columns you want to be checked in custom SQL Jinja templates.",
        "If you want to view the column policy that has been added to the clean room, call the following procedure.",
        "Finally, add a data consumer to the clean room by adding their Snowflake account locator and account names as shown below. The Snowflake account name must be of the form <ORGANIZATION>.<ACCOUNT_NAME>.",
        "Note",
        "In order to call the following procedures, make sure you have first set the release directive using provider.set_default_release_directive. You can see the latest available version and patches using:",
        "Note",
        "Note this call takes about 60 seconds to complete, since it sets up a number of tasks for listening and logging requests from the consumer.",
        "Multiple consumer account locators can be passed into the provider.add_consumers function as a comma separated string, or as separate calls to provider.add_consumers.",
        "If you want to view the consumers who have been added to this clean room, call the following procedure.",
        "View the clean rooms that have been recently created via the following procedure:",
        "View more insights on the clean room recently created via the following procedure.",
        "Any clean room created can also be deleted. The following command drops the clean room entirely, so any consumers who previously had access to the clean room will no longer be able to use it. If a clean room with the same name is desired in the future, it must be re-initialized using the above flow.",
        "Note",
        "The provider flow is now finished. Switch to the consumer account to continue with consumer flow.",
        "Note",
        "The following commands should be run in a Snowflake worksheet in the consumer account",
        "Execute the following commands to set up the Snowflake environment before using developer APIs to work with a Snowflake Data Clean Room. If you don\u2019t have the SAMOOHA_APP_ROLE role, contact your account administrator.",
        "Once a clean room share has been installed, the list of clean rooms available can be viewed using the below command.",
        "Assign a name for the clean room that the provider has shared with you.",
        "The following command installs the clean room on the consumer account with the associated provider and selected clean room.",
        "This procedure takes approximately 45 seconds to run.",
        "Once the clean room has been installed, the provider has to finish setting up the clean room on their side before it is enabled for use. The below function allows you to check the status of the clean room. Once it has been enabled, you should be able to run the Run Analysis command below. It typically takes about 1 minute for the clean room to be enabled.",
        "Link datasets into the clean room to carry out secure computation with the provider\u2019s data.",
        "Note",
        "If this step doesn\u2019t work even though your table exists, it is likely the SAMOOHA_APP_ROLE role has not yet been given access to it. If so, switch to the ACCOUNTADMIN role, call the below procedure on the database, and then switch back for the rest of the flow:",
        "To run the analysis, you need to pass in the consumer table. If you want to view the datasets that you have added to the clean room, call the following procedure.",
        "Now that the clean room is installed, you can run the analysis template given to the clean room by the provider using a \u201crun_analysis\u201d command. You can see how each field is determined in the sections below.",
        "The number of datasets passable is constrained by the template the provider has implemented. Some templates require a specific number of tables. The template creator can implement the requirements they wish to support.",
        "Note",
        "Before running the analysis, you can alter the warehouse size, or use a new, bigger, warehouse size if your tables are large.",
        "For each of the columns referred to in either the dataset filtering \u201cwhere_clause\u201d, or the dimensions or measure_columns, you can use p. to refer to fields in provider tables, and c. to refer to fields in consumer tables. Use p2, p3, etc. for more than one provider table and c2, c3, etc. for more than one consumer table.",
        "To run the analysis, you need to pass in some parameters to the run_analysis function. This section will show you how to determine what parameters to pass in.",
        "Template names",
        "First, you can see the supported analysis templates by calling the following procedure.",
        "Before running an analysis with a template, you need to know what arguments to specify and what types are expected. For custom templates, you can execute the following.",
        "This can often also contain a large number of different SQL Jinja parameters. The following functionality parses the SQL Jinja template and extracts the arguments that need to be specified in run_analysis into a convenient list.",
        "Dataset names",
        "If you want to view the dataset names that have been added to the clean room by the provider, call the following procedure. Note that you cannot view the data present in the datasets that have been added to the clean room by the provider due to the security properties of the clean room.",
        "You can also see the tables you\u2019ve linked to the clean room by using the following call:",
        "Dimension and measure columns",
        "While running the analysis, you might want to filter, group by and aggregate on certain columns. If you want to view the column policy that has been added to the clean room by the provider, call the following procedure.",
        "Common errors",
        "If you are getting Not approved: unauthorized columns used error as a result of run analysis, you may want to view the join policy and column policy set by the provider again.",
        "It is also possible that you have exhausted your privacy budget, which prevents you from executing more queries. Your remaining privacy budget can be viewed using the below command. It resets daily, or the clean room provider can reset it if they wish.",
        "You can check if Differential Privacy has been enabled for your clean room using the following API:",
        "Was this page helpful?",
        "On this page",
        "Related content"
    ]
}