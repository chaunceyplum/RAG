{
    "url": "https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/container",
    "title": "Model Serving in Snowpark Container Services | Snowflake Documentation",
    "paragraphs": [
        "Preview Feature \u2014 Open",
        "In preview in Amazon Web Services (AWS) commercial regions.",
        "Note",
        "The ability to run models in Snowpark Container Services (SPCS) described in this topic is available in\nsnowflake-ml-python version 1.6.4 and later.",
        "The Snowflake Model Registry allows you to run models\neither in a warehouse (the default), or in a Snowpark Container Services (SPCS) compute pool through Model Serving.\nRunning models in a warehouse imposes a few limitations on the size and kinds of models you can use (specifically,\nsmall-to-medium size CPU-only models whose dependencies can be satisfied by packages available in the Snowflake conda\nchannel).",
        "Running models on Snowpark Container Services (SPCS) eases these restrictions, or eliminates them entirely. You can use\nany packages you want, including those from the Python Package Index (PyPI) or other sources. Large models can be run on\ndistributed clusters of GPUs. And you don\u2019t need to know anything about container technologies, such as Docker or\nKubernetes. Snowflake Model Serving takes care of all the details.",
        "A simplified high-level overview of the Snowflake Model Serving inference architecture is shown below.",
        "The main components of the architecture are:",
        "Inference server: The server that runs the model and serves predictions. The inference server can use multiple\ninference processes to fully utilize the node\u2019s capabilities. Requests to the model are dispatched by admission\ncontrol, which manages the incoming request queue to avoid out-of-memory conditions, rejecting clients when the server\nis overloaded. Today, Snowflake provides a simple and flexible Python-based inference server that can run inference\nfor all types of models. Over time, Snowflake plans to offer inference servers optimized for specific model types.",
        "Model-specific Python environment: To reduce the latency of starting a model, which includes the time required to\ndownload dependencies and load the model, Snowflake builds a container that encapsulates the dependencies of the\nspecific model.",
        "Service functions: To talk to the inference server from code running in a warehouse, Snowflake Model Serving\nbuilds functions that have the same signature as the model, but which instead call the inference server via the\nexternal function protocol.",
        "Ingress endpoint: To allow applications outside Snowflake to call the model, Snowflake Model Serving\ncan provision an optional HTTP endpoint, accessible to the public\nInternet.",
        "The following diagram shows how the Snowflake Model Serving deploys and serves models in either a warehouse or on SPCS.",
        "As you can see, the path to SPCS deployment is more complex than the path to warehouse deployment, but Snowflake\nModel Serving does all the work for you, including building the container image that holds the model and its\ndependencies, and creating the inference server that runs the model.",
        "Before you begin, make sure you have the following:",
        "A Snowflake account in any commercial AWS region. Gov regions are not supported. Contact your account representative if your account is in Azure.",
        "Version 1.6.4 or later of the snowflake-ml-python Python package.",
        "A model you want to run on Snowpark Container Services.",
        "Familiarity with the Snowflake Model Registry.",
        "Familiarity with the Snowpark Container Services.\nIn particular, you should understand compute pools,\nimage repositories, and related privileges.",
        "Snowpark Container Services (SPCS) runs container images in compute pools. If you don\u2019t already have a suitable compute pool,\ncreate one as follows:",
        "See the family names table for a list of valid instance families.",
        "Make sure the role that will run the model is the owner of the compute pool or else has the USAGE or OPERATE privilege on the pool.",
        "Snowflake Model Serving builds a container image that holds the model and its dependencies. To store this image,\nyou need an image repository. If you don\u2019t already have one, create one as follows:",
        "If you will be using an image repository that you do not own, make sure the role that will build the container image has the READ, WRITE, SERVICE READ,\nand SERVICE WRITE privileges on the repository. Grant these privileges as follows:",
        "While this capability is in preview, the following limitations apply. Snowflake intends to address these limitations before general availability.",
        "Only the owner of a model can deploy it to Snowpark Container Services.",
        "Size of compute cluster does not auto-scale. You can manually alter the number of instances at runtime using ALTER SERVICE myservice MIN_INSTANCES = n.\nIn some cases this causes existing nodes to fail.",
        "Scaling up services and compute pools is slower than expected. This should be improved before general availability.",
        "Auto-suspend of a container service is not supported. If you expect sporadic usage, you might want to manually suspend the service after each use.",
        "Image building fails if it takes more than an hour.",
        "Table functions are not supported. Models with no regular function cannot currently be deployed to Snowpark Container Services.",
        "Either log a new model version (using reg.log_model)\nor obtain a reference to an existing model version (reg.get_model(...).version(...)).\nIn either situation, you end up with a reference to a ModelVersion object.",
        "A model\u2019s dependencies determine if it can run in a warehouse, in a SPCS service, or both. You can, if necessary,\nintentionally specify dependencies to make a model ineligible to run in one of these environments.",
        "The Snowflake conda channel is available only in warehouses and is the only source for warehouse dependencies. By\ndefault, conda dependencies for SPCS models obtain their dependencies from conda-forge.",
        "When you log a model version, the model\u2019s conda dependencies are validated against the Snowflake conda channel. If all\nthe model\u2019s conda dependencies are available there, the model is deemed eligible to run in a warehouse. It may also be\neligible to run in a SPCS service if all of its dependencies are available from conda-forge, although this is not\nchecked until you create a service.",
        "Models logged with PyPI dependencies must be run on SPCS. Specifying at least one PyPI dependency is one way to make\na model ineligible to run in a warehouse. If your model has only conda dependencies, specify at least one with an\nexplicit channel (even conda-forge), as shown in the following example.",
        "For SPCS-deployed models, conda dependencies, if any, are installed first, then any PyPI dependencies are installed in\nthe conda environment using pip.",
        "To create a SPCS service and deploy the model to it, call the model version\u2019s create_service method, as shown in\nthe following example.",
        "The following the required arguments to create_service:",
        "service_name: The name of the service to create. This name must be unique within the account.",
        "service_compute_pool: The name of the compute pool to use to run the model. The compute pool must already exist.",
        "image_repo: The name of the image repository to use to store the container image. The repo must already exist and\nthe user must have the SERVICE WRITE privilege on it (or OWNERSHIP).",
        "ingress_enabled: If True, the service is made accessible via an HTTP endpoint. To create the endpoint, the user must\nhave the BIND SERVICE ENDPOINT privilege.",
        "gpu_requests: A string specifying the number of GPUs. For a model that can be run on either CPU or GPU, this argument\ndetermines whether the model will be run on the CPU or on the GPUs. If the model is of a known type that can only be\nrun on CPU (for example, scikit-learn models), the image build fails if GPUs are requested.",
        "This example shows only the required and most commonly used arguments. See the\nModelVersion API reference\nfor a complete list of arguments.",
        "By default, a CPU-powered model uses twice the number of CPUs, plus one, worker processes. GPU-powered models use one\nworker process. You can override this using the num_workers argument.",
        "Some models are not thread-safe. Therefore, the service loads a separate copy of the model for each worker process. This\ncan result in resource depletion for large models.",
        "By default, the inference server optimizes for running a single inference at a time, aiming to make full use of all CPU\nand memory of each node.",
        "By default, Snowflake Model Serving builds the container image using the same compute pool that will be used to run\nthe model. This inference compute pool is likely overpowered for this task (for example, GPUs are not used in building\ncontainer images). In most cases, this won\u2019t have a significant impact on compute costs, but if it is a concern, you\ncan choose a less powerful compute pool for building images by specifying the image_build_compute_pool argument.",
        "create_service is an idempotent function. Calling it multiple times does not trigger image building every time.\nHowever, container images may be rebuilt based on updates in the inference service, including fixes for\nvulnerabilities in dependent packages. When this happens, create_service automatically triggers a rebuild\nof the image.",
        "You can call a model\u2019s methods using SQL, Python, or an HTTP endpoint.",
        "Snowflake Model Serving creates service functions when deploying a model to SPCS. These functions serve as a bridge\nfrom SQL to the model running in the SPCS compute pool. One service function is created for each method of the model,\nand they are named like model_name_method_name. For example, if the model has two methods named PREDICT\nand EXPLAIN and is being deployed to a service named MY_SERVICE, the resulting service functions are\nMY_SERVICE_PREDICT and MY_SERVICE_EXPLAIN.",
        "Note",
        "Service functions are contained within the service. For this reason, they have only a single point of access control, the\nservice. You can\u2019t have different access control privileges for different functions in a single service.",
        "Calling a model\u2019s service functions in SQL is done using code like the following:",
        "Call a service\u2019s methods using the run method of a model version object, including the service_name argument\nto specify the service where the method will run. For example:",
        "If you do not include the service_name argument, the model runs in a warehouse.",
        "After deploying a service with ingress enabled, a new HTTP endpoint is available for the service. You can find the endpoint\nusing the ShOW ENDPOINTS IN SERVICE command.",
        "Take note of the ingress_url column, which should look like random_str-account-id.snowflakecomputing.app.",
        "To learn more about using this endpoint, see the SPCS tutorial\nCreate a Snowpark Container Services service\nand the topic Using a service from outside Snowflake in the Developer Guide.\nFor more information about the required data format, see Remote service input and output data formats.",
        "See Deploying a Hugging Face sentence transformer for GPU-powered inference for an example of using a model service HTTP endpoint.",
        "Snowpark Container Services offers a SQL interface for managing services. You can use the\nDESCRIBE SERVICE and ALTER SERVICE commands with SPCS services created by\nSnowflake Model Serving just as you would for managing any other SPCS service. For example, you can:",
        "Change MIN_INSTANCES and other properties of a service",
        "Drop (delete) a service",
        "Share a service to another account",
        "Change ownership of a service (the new owner must have access to the model)",
        "Note",
        "If the owner of a service loses access to the underlying model for any reason, the service stops working after\na restart. It will continue running until it is restarted.",
        "To ensure reproducibility and debugability, you may not change the specification of an existing inference service. You\ncan, however, copy the specification, customize it, and use the customized specification to create your own service to\nhost the model. However, this method does not protect the underlying model from being deleted. It is generally best to\nallow Snowflake Model Serving to create services.",
        "Once you are done using a model, or if there is no usage, it is a good practice to suspend the service to save costs.\nYou can do this using the ALTER SERVICE command.",
        "The service restarts automatically when it receives a request, subject to scheduling and startup delays. Scheduling delay\ndepends on the availability of the compute pool, and startup delay depends on the size of the model.",
        "You can manage models and model versions as usual with either the SQL interface or the Python API, with the restriction\nthat a model or model version that is being used by a service (whether running or suspended) cannot be dropped\n(deleted). To drop a model or model version, drop the service first.",
        "These examples assume you have already created a compute pool, an image repository, and have granted privileges as needed.\nSee Prerequisites for details.",
        "THe following code illustrates the key steps in deploying an XGBoost model for inference in SPCS, then using the deployed model for inference.\nA notebook for this example is available.",
        "This code trains and deploys a Hugging Face sentence transformer, including an HTTP endpoint.",
        "Since this model has ingress enabled, you can call its HTTP endpoint as follows.",
        "See this quickstart\nfor an example of training and deploying a PyTorch deep learning recommendation model (DLRM) to SPCS for GPU inference.",
        "It is common for multiple users or roles to use the same model. Using a single image repository allows the image to be built once and reused by\nall users, saving time and expense. All roles that will use the repo need the SERVICE READ, SERVICE WRITE, READ, and WRITE privileges on the repo.\nSince the image might need to be rebuilt to update dependencies, you should keep the write privieges; don\u2019t revoke them after the image is initially built.",
        "Snowpark Container Services autoscaling is very conservative and does not scale up or down fast enough for most ML workloads. For this reason, Snowflake\nrecommends that you set both MIN_INSTANCES and MAX_INSTANCES to the same value, choosing these values to get the performance you need for your typical\nworkloads. Use SQL like the following:",
        "For the same reason, when initially creating the service using the Python API, the create_service method accepts only max_instances and uses that\nsame value for min_instances.",
        "Use the smallest GPU node where the model fits into memory. Scale by increasing the number of instances, as opposed to increasing num_workers in a larger GPU node.\nFor example, if the model fits in the GPU_NV_S instance type, use gpu_requests=1 and scale up by increasing max_instances rather than using a combination of\ngpu_requests and num_workers on a larger GPU instance.",
        "The larger the warehouse is, the more parallel requests are sent to inference servers. Inference is an expensive operation, so use a smaller warehouse where possible.\nUsing warehouse size larger than medium does not accelerate query performance and incurs additional cost.",
        "Creating a service creates multiple schema-level objects (the service itself and one service function per model function). To avoid clutter,\nuse separate schemas for storing models (Snowflake Model Registry) and deploying them (Snowflake Model Serving).",
        "You can monitor deployment by inspecting the services being launched using the following SQL query.",
        "Two jobs are launched:",
        "MODEL_BUILD_xxxxx: The final characters of the name are randomized to avoid name conflicts.\nThis job builds the image and ends after the image has been built. If an image already exists,\nthe job is skipped.",
        "The logs are useful for debugging issues such as conflicts in package dependencies.\nTo see the logs from this job, run the SQL below, being sure to use the same final characters:",
        "MYSERVICE: The name of the service as specified in the call to create_service. This job is started if the\nMODEL_BUILD job is successful or skipped. To see the logs from this job, run the SQL below:",
        "Two systems dictate the packages installed in the service container: the model itself and the inference server. To minmize conflicts with your\nmodel\u2019s dependencies, the inference server requires only the following packages:",
        "gunicorn<24.0.0",
        "starlette<1.0.0",
        "uvicorn-standard<1.0.0",
        "Make sure your model dependencies, along with the above, are resolvable by pip or conda, whichever you use.",
        "If a model has both conda_dependencies and pip_requirements set, these will be installed as follows via conda:",
        "Channels:",
        "conda-forge",
        "nodefaults",
        "Dependencies:",
        "all_conda_packages",
        "all_pip_packages",
        "By default, Snowflake uses conda-forge for Anaconda packages, since the Snowflake conda channel is available in\nwarehouses, and the defaults channel requires users to accept Anaconda terms of use. To specify packages from the defaults channel, include the package name: defaults::pkg_name.",
        "Some models are not thread-safe, so Snowflake loads a copy of the model in memory for each worker process. This can\ncause out-of-memory conditions for large models with a higher number of workers. Try reducing num_workers.",
        "Usually, inference is bottlenecked by the number of instances in the inference service. Try passing a higher value for max_instances while deploying the model.",
        "The specifications of the model build and inference services cannot be changed using ALTER SERVICE. You can only change attributes such as TAG, MIN_INSTANCES, and so forth.",
        "Since the image is published in the image repo, however, you can copy the spec, modify it, and create a new service from it, which you can start manually.",
        "Was this page helpful?",
        "On this page",
        "Related content"
    ]
}