{
    "url": "https://docs.snowflake.com/en/user-guide/dynamic-tables-best-practices",
    "title": "Best practices for dynamic tables | Snowflake Documentation",
    "paragraphs": [
        "This topic provides best practices and important considerations when creating and\nmanaging dynamic tables.",
        "General best practices:",
        "Use the MONITOR privilege for viewing metadata",
        "Use dynamic tables to implement slowly changing dimensions",
        "Simplify compound expressions in grouping keys",
        "Best practices for creating dynamic tables:",
        "Chain together pipelines of dynamic tables",
        "Use a \u201ccontroller\u201d dynamic table for complex task graphs",
        "About cloning pipelines of dynamic tables",
        "Use transient dynamic tables to reduce storage cost",
        "Best practices for dynamic table refresh:",
        "Use dedicated warehouses for refreshes",
        "Use downstream lag",
        "Set the refresh mode for all production dynamic tables",
        "Best practices for optimizing performance:",
        "Choosing between refresh modes",
        "Full refresh performance",
        "Incremental refresh performance",
        "For scenarios where the user only needs to view the metadata and Information Schema of a dynamic table\n(for example, roles held by data scientists), use a role that has the MONITOR privilege on that dynamic\ntable. While the OPERATE privilege grants this access, it also includes the capability to alter dynamic\ntables, making MONITOR the more suitable option for scenarios where a user does not need to alter a\ndynamic table.",
        "For more information, see Dynamic table access control.",
        "If a grouping key contains a compound expression rather than a base column, materialize the expression\nin one dynamic table and then apply the grouping operation on the materialized column in another\ndynamic table. For more information, see How operators incrementally refresh.",
        "Dynamic tables can be used to implement Type 1 and 2 slowly changing dimensions (SCDs). When reading from\na change stream, use window functions over per-record keys ordered by a change timestamp. Using this\nmethod, dynamic tables seamlessly handle insertions, deletions, and updates that occur out of order to\nsimplify SCD creation. For more information, see\nSlowly Changing Dimensions with Dynamic Tables.",
        "When defining a new dynamic table, rather than defining a large dynamic table with many nested statements,\nuse small dynamic tables with pipelines instead.",
        "You can set up a dynamic table to query other dynamic tables. For instance, imagine a scenario where\nyour data pipeline extracts data from a staging table to update various dimension tables (e.g., customer,\nproduct, date and time). Additionally, your pipeline updates an aggregate sales table based on the\ninformation from these dimension tables. By configuring the dimension tables to query the staging table\nand the aggregate sales table to query the dimension tables, you create a cascade effect similar to a\ntask graph.",
        "In this setup, the refresh for the aggregate sales table executes only after the refreshes for the\ndimension tables have successfully completed. This ensures data consistency and meets lag targets. Through\nan automated refresh process, any changes in the source tables trigger refreshes in all dependent tables\nat the appropriate times.",
        "When you have a complex graph of dynamic tables with many roots and leaves and you want to perform\noperations (e.g. changing lag, manual refresh, suspension) on the full task graph with a single command,\ndo the following:",
        "Set the value for the TARGET_LAG of all of your dynamic tables to DOWNSTREAM.",
        "Create a \u201ccontroller\u201d dynamic table that reads from all of the leaves in your task graph. To ensure this controller doesn\u2019t consume resources, do the following:",
        "Use the controller to control the whole graph. For example:",
        "Set a new target lag for the task graph.",
        "Manually refresh the task graph.",
        "Clone all elements of the dynamic table pipeline in the same clone command to avoid reinitializations of\nyour pipeline. You can do this by consolidating all elements of the pipeline (e.g. base tables, view, and\ndynamic tables) in the same schema or database. For more information, see Known limitations for dynamic tables.",
        "Transient dynamic tables maintain data reliably over time and\nsupport Time Travel within the data retention period, but don\u2019t retain data beyond the fail-safe period. By\ndefault, dynamic table data is retained for 7 days in fail-safe storage.\nFor dynamic tables with high refresh throughput, this can significantly increase storage consumption.\nTherefore, you should make a dynamic table transient only if its data doesn\u2019t need the same level of data\nprotection and recovery provided by permanent tables.",
        "You can create a transient dynamic table or clone existing dynamic tables to transient dynamic tables using\nthe CREATE DYNAMIC TABLE statement.",
        "Dynamic tables require a virtual warehouse to perform refreshes. To get a clear understanding of costs\nrelated to your dynamic table pipelines, you should test your dynamic tables using dedicated warehouses\nsuch that the virtual warehouse consumption attributed to dynamic tables can be isolated.\nFor more information, see Understanding cost for dynamic tables.",
        "Downstream lag indicates that the dynamic table should refresh when other dependent dynamic tables require\nrefreshing.  You should use downstream lag as a best practice because of its ease of use and cost\neffectiveness. Without downstream lag, managing a chain of complex dynamic tables would require\nindividually assigning each table its own target lag and managing the associated constraints, instead of\nonly monitoring the data freshness of the final table. For more information, see\nUnderstanding target lag.",
        "A dynamic table\u2019s actual refresh mode is determined at\ncreation time and is immutable afterward. If not specified explicitly, the refresh mode defaults to AUTO,\nwhich selects a refresh mode based on various factors such as query complexity, or unsupported constructs,\noperators, or functions.",
        "To determine the best mode for your use case, experiment with refresh modes and automatic recommendations.\nFor consistent behavior across Snowflake releases, explicitly set the refresh mode on all production\ntables. The behavior of AUTO might change between Snowflake releases, which can cause unexpected\nchanges in performance if used in production pipelines.",
        "To verify the refresh mode for your dynamic tables, see View dynamic table refresh mode.",
        "To optimize your dynamic tables\u2019 performance, you should understand the system, experiment with ideas, and iterate\nbased on results. For example:",
        "Develop ways to improve your data pipeline based on your cost, data lag, and response time needs.",
        "Implement the following actions:",
        "Start with a small, fixed dataset to quickly develop queries.",
        "Test performance with data in motion.",
        "Scale the dataset to verify that it meets your needs.",
        "Adjust your workload based on findings.",
        "Repeat as needed, prioritizing tasks with the greatest performance impact.",
        "Additionally, use downstream lag to manage refresh dependencies between tables efficiently, ensuring that\nrefreshes happen only when necessary. For more information, see the\nperformance documentation.",
        "To determine the best mode for your use case, experiment with automatic recommendations and the concrete\nrefresh modes (full and incremental). The best mode for your dynamic tables\u2019 performance depends on data\nchange volume and query complexity. Additionally, testing different refresh modes with a dedicated\nwarehouse helps isolate costs and improve performance tuning based on actual workloads.",
        "To verify the refresh mode for your dynamic tables, see View dynamic table refresh mode.",
        "AUTO refresh mode: The system attempts to apply incremental refresh by default. When\nincremental refresh isn\u2019t supported\nor might not perform well, the dynamic table automatically selects full refresh instead.",
        "For consistent behavior, explicitly set the refresh mode on all production tables. The behavior of\nAUTO might change between Snowflake releases, which can cause unexpected changes in performance\nif used in production pipelines.",
        "Incremental refresh: Updates the dynamic table with only the changes since the last refresh, making\nit ideal for large datasets with frequent small updates.",
        "Best for queries compatible with incremental refresh (for example, deterministic functions, simple\njoins, and basic expressions in SELECT, WHERE, and GROUP BY). If unsupported features\nare present, and the refresh mode is set to incremental, Snowflake will fail to create the dynamic\ntable.",
        "A key practice for optimizing performance with incremental refresh is to limit change volume to\nabout 5% of the source data and to cluster your data by the grouping keys to reduce processing\noverhead.",
        "Consider that certain combinations of operations, like aggregations atop many joins, might not\nrun efficiently.",
        "Full refresh: Reprocesses the entire dataset and updates the dynamic table with the complete query\nresult.  Use for complex queries or when significant data changes require a complete update.",
        "Useful when incremental refresh isn\u2019t supported due to complex queries, non-deterministic functions,\nor major changes in the data.",
        "For more information, see How refresh mode affects dynamic table performance.",
        "Full refresh dynamic tables perform similarly to CREATE TABLE \u2026 AS SELECT (also referred to as CTAS). They can be optimized like any\nother Snowflake query.",
        "To help achieve optimal incremental refresh performance for your dynamic tables:",
        "Keep changes between refreshes minimal, ideally less than 5% of the total dataset, for both sources\nand the dynamic table.",
        "Consider the number of micro-partitions modified, not just the row count. The amount of work an\nincremental refresh must do is proportional to the size of these micro-partitions, not only the rows\nthat changed.",
        "Minimize grouping operations like joins, GROUP BYs, and PARTITION BYs in your query. Break down large\nCommon Table Expressions (CTEs) into smaller parts and create a dynamic table for each. Avoid\noverwhelming a single dynamic table with excessive aggregations or joins.",
        "Ensure data locality by aligning table changes with query keys (e.g., for joins, GROUP BYs, PARTITION\nBYs). If your tables aren\u2019t naturally clustered by these keys, consider enabling\nautomatic clustering.",
        "Was this page helpful?",
        "On this page"
    ]
}