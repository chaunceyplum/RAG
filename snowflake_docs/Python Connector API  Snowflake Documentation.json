{
    "url": "https://docs.snowflake.com/en/user-guide/python-connector-api.html#connect",
    "title": "Python Connector API | Snowflake Documentation",
    "paragraphs": [
        "The Snowflake Connector for Python implements the Python Database API v2.0 specification\n(PEP-249). This topic covers the standard\nAPI and the Snowflake-specific extensions.",
        "For more information, see the PEP-249 documentation.",
        "The main module is snowflake.connector, which creates a Connection object and provides\nError classes.",
        "String constant stating the supported API level. The connector supports API\n\"2.0\".",
        "Integer constant stating the level of thread safety the interface supports. The\nSnowflake Connector for Python supports level 2, which states that threads can share\nthe module and connections.",
        "String constant stating the type of parameter marker formatting expected\nby the interface. The connector supports the \"pyformat\" type by default, which applies to\nPython extended format codes (e.g. ...WHERE name=%s or ...WHERE name=%(name)s).\nConnection.connect can override paramstyle to change the bind variable formats to\n\"qmark\" or \"numeric\", where the variables are ? or :N, respectively.",
        "For example:",
        "Note",
        "The binding variable occurs on the client side if paramstyle is \"pyformat\" or\n\"format\", and on the server side if \"qmark\" or \"numeric\". Currently,\nthere is no significant difference between those options in terms of performance or features\nbecause the connector doesn\u2019t support compiling SQL text followed by\nmultiple executions. Instead, the \"qmark\" and \"numeric\" options align with the query text\ncompatibility of other drivers (i.e. JDBC, ODBC, Go Snowflake Driver), which support server\nside bindings with the variable format ? or :N.",
        "Constructor for creating a connection to the database. Returns a Connection object.",
        "By default, autocommit mode is enabled (i.e. if the connection is closed, all changes are committed). If you need a\ntransaction, use the BEGIN command to start the transaction, and COMMIT\nor ROLLBACK to commit or roll back any changes.",
        "The valid input parameters are:",
        "Parameter",
        "Required",
        "Description",
        "account",
        "Yes",
        "Your account identifier. The account identifier does not include the snowflakecomputing.com suffix. . . For details and examples, see Usage Notes (in this topic).",
        "user",
        "Yes",
        "Login name for the user.",
        "password",
        "Yes",
        "Password for the user.",
        "application",
        "Name that identifies the application making the connection.",
        "region",
        "Deprecated This description of the parameter is for backwards compatibility only.",
        "host",
        "No longer used Host name. Used internally only (i.e. does not need to be set).",
        "port",
        "No longer used Port number (443 by default). Used internally only (i.e. does not need to be set).",
        "database",
        "Name of the default database to use. After login, you can use USE DATABASE to change the database.",
        "schema",
        "Name of the default schema to use for the database. After login, you can use USE SCHEMA to change the schema.",
        "role",
        "Name of the default role to use. After login, you can use USE ROLE to change the role.",
        "warehouse",
        "Name of the default warehouse to use. After login, you can use USE WAREHOUSE to change the warehouse.",
        "passcode_in_password",
        "False by default. Set this to True if the MFA (Multi-Factor Authentication) passcode is embedded in the login password.",
        "passcode",
        "The passcode provided by Duo when using MFA (Multi-Factor Authentication) for login.",
        "private_key",
        "The private key used for authentication. For more information, see Using key-pair authentication and key-pair rotation.",
        "private_key_file",
        "Specifies the path to the private key file for the specified user. See Using key-pair authentication and key-pair rotation.",
        "private_key_file_pwd",
        "Specifies the passphrase to decrypt the private key file for the specified user. See Using key-pair authentication and key-pair rotation.",
        "autocommit",
        "None by default, which honors the Snowflake parameter AUTOCOMMIT. Set to True or False to enable or disable autocommit mode in the session, respectively.",
        "client_prefetch_threads",
        "Number of threads used to download the results sets (4 by default). Increasing the value improves fetch performance but requires more memory.",
        "client_session_keep_alive",
        "To keep the session active indefinitely (even if there is no activity from the user), set this to True. When setting this to True, call the close method to terminate the thread properly; otherwise, the process might hang.",
        "The default value depends on the version of the connector that you are using:",
        "Version 2.4.6 and later: None by default. . When the value is None, the CLIENT_SESSION_KEEP_ALIVE session parameter takes precedence. . . To override the session parameter, pass in True or False for this argument.",
        "Version 2.4.5 and earlier: False by default. . When the value is False (either by specifying the value explicitly or by omitting the argument), the CLIENT_SESSION_KEEP_ALIVE session parameter takes precedence. . . Passing client_session_keep_alive=False to the connect method does not override the value TRUE in the CLIENT_SESSION_KEEP_ALIVE session parameter.",
        "login_timeout",
        "Timeout in seconds for login. By default, 60 seconds. The login request gives up after the timeout length if the HTTP response is \u201csuccess\u201d.",
        "network_timeout",
        "Timeout in seconds for all other operations. By default, none/infinite. A general request gives up after the timeout length if the HTTP response is not \u201csuccess\u201d.",
        "ocsp_response_cache_filename",
        "URI for the OCSP response cache file. By default, the OCSP response cache file is created in the cache directory:",
        "Linux: ~/.cache/snowflake/ocsp_response_cache",
        "macOS: ~/Library/Caches/Snowflake/ocsp_response_cache",
        "Windows: %USERPROFILE%AppDataLocalSnowflakeCachesocsp_response_cache",
        "To locate the file in a different directory, specify the path and file name in the URI (e.g. file:///tmp/my_ocsp_response_cache.txt).",
        "authenticator",
        "Authenticator for Snowflake:",
        "snowflake (default) to use the internal Snowflake authenticator.",
        "externalbrowser to authenticate using your web browser and Okta, AD FS, or any other SAML 2.0-compliant identity provider (IdP) that has been defined for your account.",
        "https://<okta_account_name>.okta.com (i.e. the URL endpoint for your Okta account) to authenticate through native Okta.",
        "oauth to authenticate using OAuth. You must also specify the token parameter and set its value to the OAuth access token.",
        "username_password_mfa to authenticate with MFA token caching. For more details, see Using MFA token caching to minimize the number of prompts during authentication \u2014 optional.",
        "If the value is not snowflake, the user and password parameters must be your login credentials for the IdP.",
        "validate_default_parameters",
        "False by default. If True, then:",
        "Raise an exception if the specified database, schema, or warehouse doesn\u2019t exist.",
        "Print a warning to stderr if an invalid argument name or an argument value of the wrong data type is passed.",
        "paramstyle",
        "pyformat by default for client side binding. Specify qmark or numeric to change bind variable formats for server side binding.",
        "timezone",
        "None by default, which honors the Snowflake parameter TIMEZONE. Set to a valid time zone (e.g. America/Los_Angeles) to set the session time zone.",
        "arrow_number_to_decimal",
        "False by default, which means that NUMBER column values are returned as double-precision floating point numbers (float64). . . Set this to True to return DECIMAL column values as decimal numbers (decimal.Decimal) when calling the fetch_pandas_all() and fetch_pandas_batches() methods. . . This parameter was introduced in version 2.4.3 of the Snowflake Connector for Python.",
        "socket_timeout",
        "Timeout in seconds for socket-level read and connect requests. For more information, see Managing connection timeouts.",
        "backoff_policy",
        "Name of the generator function that defines how long to wait between retries. For more information, see Managing connection backoff policies for retries.",
        "enable_connection_diag",
        "Whether to generate a connectivity diagnostic report. Default is False.",
        "connection_diag_log_path",
        "Absolute path for the location of the diagnostic report. Used only if enable_connection_diag is True. Default is the default temp directory for your operating system, such as /tmp for Linux or Mac.",
        "connection_diag_allowlist_path",
        "Absolute path to a JSON file containing the output of SYSTEM$ALLOWLIST() or SYSTEM$ALLOWLIST_PRIVATELINK(). Required only if the user defined in the connection does not have permission to run the system allowlist functions or if connecting to the account URL fails.",
        "All exception classes defined by the Python database API standard. The Snowflake\nConnector for Python provides the attributes msg, errno, sqlstate,\nsfqid and raw_msg.",
        "For the required account parameter, specify your account identifier.",
        "Note that the account identifier does not include the snowflakecomputing.com domain name. Snowflake automatically\nappends this when creating the connection.",
        "The following example uses the account name as an identifier for the account myaccount in\nthe organization myorganization.",
        "The following example uses the account locator xy12345 as the account identifier:",
        "Note that this example uses an account in the AWS US West (Oregon) region. If the account is in a different region or if the\naccount uses a different cloud provider, you need to\nspecify additional segments after the account locator.",
        "A Connection object holds the connection and session information to keep the database connection active. If it is closed or the session expires, any subsequent operations will fail.",
        "Enables or disables autocommit mode. By default, autocommit is enabled (True).",
        "Closes the connection. If a transaction is still open when the connection is closed, the\nchanges are rolled back.",
        "Closing the connection explicitly removes the active session from the server; otherwise, the active session continues until it is eventually purged from the server, limiting the number of concurrent queries.",
        "For example:",
        "If autocommit is disabled, commits the current transaction. If autocommit is enabled, this\nmethod is ignored.",
        "If autocommit is disabled, rolls back the current transaction. If autocommit is enabled,\nthis method is ignored.",
        "Constructor for creating a Cursor object. The return values from\nfetch*() calls will be a single sequence or list of sequences.",
        "Constructor for creating a DictCursor object. The return values from\nfetch*() calls will be a single dict or list of dict objects. This\nis useful for fetching values by column name from the results.",
        "Execute one or more SQL statements passed as strings. If remove_comments is set to True,\ncomments are removed from the query. If return_cursors is set to True, this\nmethod returns a sequence of Cursor objects in the order of execution.",
        "This example shows executing multiple commands in a single string and then using the sequence of\ncursors that is returned:",
        "Note",
        "Methods such as execute_string() that allow multiple SQL statements in a single\nstring are vulnerable to SQL injection attacks.  Avoid using string concatenation,\nor functions such as Python\u2019s format() function, to dynamically compose a SQL statement\nby combining SQL with data from users unless you have validated the user data. The example\nbelow demonstrates the problem:",
        "The dynamically-composed statement looks like the following (newlines have\nbeen added for readability):",
        "If you are combining SQL statements with strings entered by untrusted users,\nthen it is safer to bind data to a statement than to compose a string.\nThe execute_string() method doesn\u2019t take binding parameters, so to bind parameters\nuse Cursor.execute() or Cursor.executemany().",
        "Execute one or more SQL statements passed as a stream object. If remove_comments is set to True,\ncomments are removed from the query. This generator yields each Cursor object as SQL statements run.",
        "Returns the status of a query.",
        "query_id",
        "The ID of the query. See Retrieving the Snowflake query ID.",
        "Returns the QueryStatus object that represents the status of the query.",
        "See Checking the status of a query.",
        "Returns the status of a query. If the query results in an error, this method raises a ProgrammingError (as the\nexecute() method would).",
        "query_id",
        "The ID of the query. See Retrieving the Snowflake query ID.",
        "Returns the QueryStatus object that represents the status of the query.",
        "See Checking the status of a query.",
        "Returns True if the query status indicates that the query has not yet completed or is still in process.",
        "query_status",
        "The QueryStatus object that represents the status of the query. To get this object for a query, see\nChecking the status of a query.",
        "See Checking the status of a query.",
        "Returns True if the query status indicates that the query resulted in an error.",
        "query_status",
        "The QueryStatus object that represents the status of the query. To get this object for a query, see\nChecking the status of a query.",
        "See Checking the status of a query.",
        "Tracks whether the connection\u2019s master token has expired.",
        "The list object including sequences (exception class, exception value) for all\nmessages received from the underlying database for this connection.",
        "The list is cleared automatically by any method call.",
        "Read/Write attribute that references an error handler to call in case an\nerror condition is met.",
        "The handler must be a Python callable that accepts the following arguments:",
        "errorhandler(connection, cursor, errorclass, errorvalue)",
        "All exception classes defined by the Python database API standard.",
        "A Cursor object represents a database cursor for execute and fetch operations.\nEach cursor has its own attributes, description and rowcount, such that\ncursors are isolated.",
        "Closes the cursor object.",
        "Returns metadata about the result set without executing a database command. This returns the same metadata that is\navailable in the description attribute after executing a query.",
        "This method was introduced in version 2.4.6 of the Snowflake Connector for Python.",
        "See the parameters for the execute() method.",
        "Returns a list of ResultMetadata objects that describe the columns\nin the result set.",
        "See Retrieving column metadata.",
        "Prepares and executes a database command.",
        "command",
        "A string containing the SQL statement to execute.",
        "parameters",
        "(Optional) If you used parameters for binding data in the SQL\nstatement, set this to the list or dictionary of variables that should be bound to those parameters.",
        "For more information about mapping the Python data types for the variables to the SQL data types of the corresponding\ncolumns, see Data type mappings for qmark and numeric bindings.",
        "timeout",
        "(Optional) Number of seconds to wait for the query to complete. If the query has not completed after this time has\npassed, the query should be aborted.",
        "file_stream",
        "(Optional) When executing a PUT command, you can use this parameter to upload an in-memory file-like object (e.g. the\nI/O object returned from the Python open() function), rather than a file on the filesystem. Set this\nparameter to that I/O object.",
        "When specifying the URI for the data file in the PUT command:",
        "You can use any directory path. The directory path that you specify in the URI is ignored.",
        "For the filename, specify the name of the file that should be created on the stage.",
        "For example, to upload a file from a file stream to a file named:",
        "use the following call:",
        "Returns the reference of a Cursor object.",
        "Prepares a database command and executes it against all parameter sequences\nfound in seq_of_parameters. You can use this method to\nperform a batch insert operation.",
        "command",
        "The command is a string containing the code to execute.\nThe string should contain one or more placeholders (such as\nquestion marks) for Binding data.",
        "For example:",
        "seq_of_parameters",
        "This should be a sequence (list or tuple) of lists or tuples. See the example code below for example\nsequences.",
        "Returns the reference of a Cursor object.",
        "Internally, multiple execute methods are called and the result set from the\nlast execute call will remain.",
        "Note",
        "The executemany method can only be used to execute a single parameterized SQL statement\nand pass multiple bind values to it.",
        "Executing multiple SQL statements separated by a semicolon in one execute call is not supported.\nInstead, issue a separate execute call for each statement.",
        "Prepares and submits a database command for asynchronous execution.\nSee Performing an asynchronous query.",
        "This method uses the same parameters as the execute() method.",
        "Returns the reference of a Cursor object.",
        "See Examples of asynchronous queries.",
        "This method fetches all the rows in a cursor and loads them into a PyArrow table.",
        "None.",
        "Returns a PyArrow table containing all the rows from the result set.",
        "If there are no rows, this returns None.",
        "See Distributing workloads that fetch results with the Snowflake Connector for Python.",
        "This method fetches a subset of the rows in a cursor and delivers them to a PyArrow table.",
        "None.",
        "Returns a PyArrow table containing a subset of the rows from the result set.",
        "Returns None if there are no more rows to fetch.",
        "See Distributing workloads that fetch results with the Snowflake Connector for Python.",
        "Returns a list of ResultBatch objects that you can use to fetch a\nsubset of rows from the result set.",
        "None.",
        "Returns a list of ResultBatch objects or None if the query has\nnot finished executing.",
        "See Distributing workloads that fetch results with the Snowflake Connector for Python.",
        "Retrieves the results of an asynchronous query or a previously submitted synchronous query.",
        "query_id",
        "The ID of the query. See Retrieving the Snowflake query ID.",
        "See Using the query ID to retrieve the results of a query.",
        "Fetches the next row of a query result set and returns a single sequence/dict or\nNone when no more data is available.",
        "Fetches the next rows of a query result set and returns a list of\nsequences/dict. An empty sequence is returned when no more rows are available.",
        "Fetches all or remaining rows of a query result set and returns a list of\nsequences/dict.",
        "This method fetches all the rows in a cursor and loads them into a pandas DataFrame.",
        "None.",
        "Returns a DataFrame containing all the rows from the result set.",
        "For more information about pandas data frames, see the pandas DataFrame documentation .",
        "If there are no rows, this returns None.",
        "This method is not a complete replacement for the read_sql() method of pandas; this method is to provide\na fast way to retrieve data from a SELECT query and store the data in a pandas DataFrame.",
        "Currently, this method works only for SELECT statements.",
        "This method fetches a subset of the rows in a cursor and delivers them to a pandas DataFrame.",
        "None.",
        "Returns a DataFrame containing a subset of the rows from the result set.",
        "For more information about pandas data frames, see the pandas DataFrame documentation.",
        "Returns None if there are no more rows to fetch.",
        "Depending upon the number of rows in the result set, as well as the number of rows specified in the method\ncall, the method might need to be called more than once, or it might return all rows in a single batch if\nthey all fit.",
        "This method is not a complete replacement for the read_sql() method of pandas; this method is to provide\na fast way to retrieve data from a SELECT query and store the data in a pandas DataFrame.",
        "Currently, this method works only for SELECT statements.",
        "Returns self to make cursors compatible with the iteration protocol.",
        "Read-only attribute that returns metadata about the columns in the result set.",
        "This attribute is set after you call the execute() method to execute the query. (In version 2.4.6 or later, you can\nretrieve this metadata without executing the query by calling the describe() method.)",
        "This attribute is set to one of the following:",
        "Versions 2.4.5 and earlier: This attribute is set to a list of tuples.",
        "Versions 2.4.6 and later: This attribute is set to a list of\nResultMetadata objects.",
        "Each tuple or ResultMetadata object contains the metadata that describes a column in the result set. You can access\nthe metadata by index or, in versions 2.4.6 and later, by ResultMetadata object attribute:",
        "Index of Value",
        "ResultMetadata Attribute",
        "Description",
        "0",
        "name",
        "Column name.",
        "1",
        "type_code",
        "Internal type code.",
        "2",
        "display_size",
        "(Not used. Same as internal_size.)",
        "3",
        "internal_size",
        "Internal data size.",
        "4",
        "precision",
        "Precision of numeric data.",
        "5",
        "scale",
        "Scale for numeric data.",
        "6",
        "is_nullable",
        "True if NULL values allowed for the column or False.",
        "For examples of getting this attribute, see Retrieving column metadata.",
        "Read-only attribute that returns the number of rows in the last execute produced.\nThe value is -1 or None if no execute is executed.",
        "Read-only attribute that returns the Snowflake query ID in the last execute or execute_async executed.",
        "Read/write attribute that specifies the number of rows to fetch at a time with fetchmany().\nIt defaults to 1 meaning to fetch a single row at a time.",
        "Read-only attribute that returns a reference to the Connection object on which the cursor\nwas created.",
        "List object that includes the sequences (exception class, exception value) for all messages\nwhich it received from the underlying database for the cursor.",
        "The list is cleared automatically by any method call except for fetch*() calls.",
        "Read/write attribute that references an error handler to call in case an error condition is\nmet.",
        "The handler must be a Python callable that accepts the following arguments:",
        "errorhandler(connection, cursor, errorclass, errorvalue)",
        "In the Cursor object, the description attribute and the describe() method provide a list of tuples\n(or, in versions 2.4.6 and later, ResultMetadata objects) that describe the\ncolumns in the result set.",
        "In a tuple, the value at the index 1 (the type_code attribute In the ResultMetadata object) represents the\ncolumn data type. The Snowflake Connector for Python uses the following map to get the string representation, based on the type\ncode:",
        "type_code",
        "String Representation",
        "Data Type",
        "0",
        "FIXED",
        "NUMBER/INT",
        "1",
        "REAL",
        "REAL",
        "2",
        "TEXT",
        "VARCHAR/STRING",
        "3",
        "DATE",
        "DATE",
        "4",
        "TIMESTAMP",
        "TIMESTAMP",
        "5",
        "VARIANT",
        "VARIANT",
        "6",
        "TIMESTAMP_LTZ",
        "TIMESTAMP_LTZ",
        "7",
        "TIMESTAMP_TZ",
        "TIMESTAMP_TZ",
        "8",
        "TIMESTAMP_NTZ",
        "TIMESTAMP_TZ",
        "9",
        "OBJECT",
        "OBJECT",
        "10",
        "ARRAY",
        "ARRAY",
        "11",
        "BINARY",
        "BINARY",
        "12",
        "TIME",
        "TIME",
        "13",
        "BOOLEAN",
        "BOOLEAN",
        "14",
        "GEOGRAPHY",
        "GEOGRAPHY",
        "15",
        "GEOMETRY",
        "GEOMETRY",
        "16",
        "VECTOR",
        "VECTOR",
        "If paramstyle is either \"qmark\" or \"numeric\", the following default mappings from\nPython to Snowflake data type are used:",
        "Python Data Type",
        "Data Type in Snowflake",
        "int",
        "NUMBER(38, 0)",
        "long",
        "NUMBER(38, 0)",
        "decimal",
        "NUMBER(38, <scale>)",
        "float",
        "REAL",
        "str",
        "TEXT",
        "unicode",
        "TEXT",
        "bytes",
        "BINARY",
        "bytearray",
        "BINARY",
        "bool",
        "BOOLEAN",
        "date",
        "DATE",
        "time",
        "TIME",
        "timedelta",
        "TIME",
        "datetime",
        "TIMESTAMP_NTZ",
        "struct_time",
        "TIMESTAMP_NTZ",
        "If you need to map to another Snowflake type (e.g. datetime to TIMESTAMP_LTZ), specify the\nSnowflake data type in a tuple consisting of the Snowflake data type followed by the value. See\nBinding datetime with TIMESTAMP for examples.",
        "PEP-249 defines the exceptions that the\nSnowflake Connector for Python can raise in case of errors or warnings. The application must\nhandle them properly and decide to continue or stop running the code.",
        "For more information, see the PEP-249 documentation.",
        "No methods are available for Exception objects.",
        "Snowflake DB error code.",
        "Error message including error code, SQL State code and query ID.",
        "Error message. No error code, SQL State code or query ID is included.",
        "ANSI-compliant SQL State code",
        "Snowflake query ID.",
        "A ResultBatch object encapsulates a function that retrieves a subset of rows in a result set. To\ndistribute the work of fetching results across multiple workers or nodes, you can call\nget_result_batches() method in the Cursor object to retrieve a list of\nResultBatch objects and distribute these objects to different workers or nodes for processing.",
        "Read-only attribute that returns the number of rows in the result batch.",
        "Read-only attribute that returns the size of the data (when compressed) in the result batch.",
        "Read-only attribute that returns the size of the data (uncompressed) in the result batch.",
        "This method returns a PyArrow table containing the rows in the ResultBatch object.",
        "None.",
        "Returns a PyArrow table containing the rows from the ResultBatch object.",
        "If there are no rows, this returns None.",
        "This method returns a pandas DataFrame containing the rows in the ResultBatch object.",
        "None.",
        "Returns a pandas DataFrame containing the rows from the ResultBatch object.",
        "If there are no rows, this returns an empty pandas DataFrame.",
        "A ResultMetadata object represents metadata about a column in the result set.\nA list of these objects is returned by the description attribute and describe method of the Cursor\nobject.",
        "This object was introduced in version 2.4.6 of the Snowflake Connector for Python.",
        "None.",
        "Name of the column",
        "Internal type code.",
        "Not used. Same as internal_size.",
        "Internal data size.",
        "Precision of numeric data.",
        "Scale for numeric data.",
        "True if NULL values allowed for the column or False.",
        "The snowflake.connector.constants module defines constants used in the API.",
        "Represents the status of an asynchronous query. This enum has the following constants:",
        "Enum Constant",
        "Description",
        "RUNNING",
        "The query is still running.",
        "ABORTING",
        "The query is in the process of being aborted on the server side.",
        "SUCCESS",
        "The query finished successfully.",
        "FAILED_WITH_ERROR",
        "The query finished unsuccessfully.",
        "QUEUED",
        "The query is queued for execution (i.e. has not yet started running), typically because it is waiting for resources.",
        "DISCONNECTED",
        "The session\u2019s connection is broken. The query\u2019s state will change to \u201cFAILED_WITH_ERROR\u201d soon.",
        "RESUMING_WAREHOUSE",
        "The warehouse is starting up and the query is not yet running.",
        "BLOCKED",
        "The statement is waiting on a lock held by another statement.",
        "NO_DATA",
        "Data about the statement is not yet available, typically because the statement has not yet started executing.",
        "The snowflake.connector.pandas_tools module provides functions for\nworking with the pandas data analysis library.",
        "For more information, see the pandas data analysis library documentation.",
        "Writes a pandas DataFrame to a table in a Snowflake database.",
        "To write the data to the table, the function saves the data to Parquet files, uses the PUT command to upload these files to a temporary stage, and uses the COPY INTO <table> command to copy the data from the files to the table. You can use some of the function parameters to control how the PUT and COPY INTO <table> statements are executed.",
        "The valid input parameters are:",
        "Parameter",
        "Required",
        "Description",
        "conn",
        "Yes",
        "Connection object that holds the connection to the Snowflake database.",
        "df",
        "Yes",
        "pandas.DataFrame object containing the data to be copied into the table.",
        "table_name",
        "Yes",
        "Name of the table where the data should be copied.",
        "database",
        "Name of the database containing the table. By default, the function writes to the database that is currently in use in the session. Note: If you specify this parameter, you must also specify the schema parameter.",
        "schema",
        "Name of the schema containing the table. By default, the function writes to the table in the schema that is currently in use in the session.",
        "chunk_size",
        "Number of elements to insert at a time. By default, the function inserts all elements at once in one chunk.",
        "compression",
        "The compression algorithm to use for the Parquet files. You can specify either \"gzip\" for better compression or \"snappy\" for faster compression. By default, the function uses \"gzip\".",
        "on_error",
        "Specifies how errors should be handled. Set this to one of the string values documented in the ON_ERROR copy option. By default, the function uses \"ABORT_STATEMENT\".",
        "parallel",
        "Number of threads to use when uploading the Parquet files to the temporary stage. For the default number of threads used and guidelines on choosing the number of threads, see the parallel parameter of the PUT command.",
        "quote_identifiers",
        "If False, prevents the connector from putting double quotes around identifiers before sending the identifiers to the server. By default, the connector puts double quotes around identifiers.",
        "Returns a tuple of (success, num_chunks, num_rows, output) where:",
        "success is True if the function successfully wrote the data to the table.",
        "num_chunks is the number of chunks of data that the function copied.",
        "num_rows is the number of rows that the function inserted.",
        "output is the output of the COPY INTO <table> command.",
        "The following example writes the data from a pandas DataFrame to the table named \u2018customers\u2019.",
        "pd_writer is an\ninsertion method for inserting data into\na Snowflake database.",
        "When calling pandas.DataFrame.to_sql,\npass in method=pd_writer to specify that you want to use pd_writer as the method for inserting data.\n(You do not need to call pd_writer from your own code. The to_sql method calls pd_writer and\nsupplies the input parameters needed.)",
        "For more information see:",
        "insertion method documentation.",
        "pandas documentation.",
        "Note",
        "Please note that when column names in the pandas DataFrame contain only lowercase letters, you must enclose\nthe column names in double quotes; otherwise the connector raises a ProgrammingError.",
        "The snowflake-sqlalchemy library does not quote lowercase column names when creating a table,\nwhile pd_writer quotes column names by default. The issue arises because the COPY INTO\ncommand expects column names to be quoted.",
        "Future improvements will be made in the snowflake-sqlalchemy library.",
        "For example:",
        "The pd_writer function uses the write_pandas() function to write the data in the DataFrame to the\nSnowflake database.",
        "The valid input parameters are:",
        "Parameter",
        "Required",
        "Description",
        "table",
        "Yes",
        "pandas.io.sql.SQLTable object for the table.",
        "conn",
        "Yes",
        "sqlalchemy.engine.Engine or sqlalchemy.engine.Connection object used to connect to the Snowflake database.",
        "keys",
        "Yes",
        "Names of the table columns for the data to be inserted.",
        "data_iter",
        "Yes",
        "Iterator for the rows containing the data to be inserted.",
        "The following example passes method=pd_writer to the pandas.DataFrame.to_sql method, which in turn calls\nthe pd_writer function to write the data in the pandas DataFrame to a Snowflake database.",
        "Snowflake supports multiple DATE and TIMESTAMP data types, and the Snowflake Connector\nallows binding native datetime and date objects for update and fetch operations.",
        "When fetching date and time data, the Snowflake data types are converted into Python data types:",
        "Snowflake Data Types",
        "Python Data Type",
        "Behavior",
        "TIMESTAMP_TZ",
        "datetime with tzinfo",
        "Fetches data, including the time zone offset, and translates it into a datetime with tzinfo object.",
        "TIMESTAMP_LTZ, TIMESTAMP",
        "datetime with tzinfo",
        "Fetches data, translates it into a datetime object, and attaches tzinfo based on the TIMESTAMP_TYPE_MAPPING session parameter.",
        "TIMESTAMP_NTZ",
        "datetime",
        "Fetches data and translates it into a datetime object. No time zone information is attached to the object.",
        "DATE",
        "date",
        "Fetches data and translates it into a date object. No time zone information is attached to the object.",
        "Note",
        "tzinfo is a UTC offset-based time zone object and not IANA time zone\nnames. The time zone names might not match, but equivalent offset-based\ntime zone objects are considered identical.",
        "When updating date and time data, the Python data types are converted to Snowflake data types:",
        "Python Data Type",
        "Snowflake Data Types",
        "Behavior",
        "datetime",
        "TIMESTAMP_TZ, TIMESTAMP_LTZ, TIMESTAMP_NTZ, DATE",
        "Converts a datetime object into a string in the format of YYYY-MM-DD HH24:MI:SS.FF TZH:TZM and updates it. If no time zone offset is provided, the string will be in the format of YYYY-MM-DD HH24:MI:SS.FF. The user is responsible for setting the tzinfo for the datetime object.",
        "struct_time",
        "TIMESTAMP_TZ, TIMESTAMP_LTZ, TIMESTAMP_NTZ, DATE",
        "Converts a struct_time object into a string in the format of YYYY-MM-DD HH24:MI:SS.FF TZH:TZM and updates it. The time zone information is retrieved from time.timezone, which includes the time zone offset from UTC. The user is responsible for setting the TZ environment variable for time.timezone.",
        "date",
        "TIMESTAMP_TZ, TIMESTAMP_LTZ, TIMESTAMP_NTZ, DATE",
        "Converts a date object into a string in the format of YYYY-MM-DD. No time zone is considered.",
        "time",
        "TIMESTAMP_TZ, TIMESTAMP_LTZ, TIMESTAMP_NTZ, DATE",
        "Converts a time object into a string in the format of HH24:MI:SS.FF. No time zone is considered.",
        "timedelta",
        "TIMESTAMP_TZ, TIMESTAMP_LTZ, TIMESTAMP_NTZ, DATE",
        "Converts a timedelta object into a string in the format of HH24:MI:SS.FF. No time zone is considered.",
        "Was this page helpful?",
        "On this page"
    ]
}