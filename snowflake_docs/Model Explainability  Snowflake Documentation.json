{
    "url": "https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/model-explainability",
    "title": "Model Explainability | Snowflake Documentation",
    "paragraphs": [
        "Preview Feature \u2014 Open",
        "Available to all accounts.",
        "During the training process, machine learning models infer relationships between inputs and outputs, rather than\nrequiring that these relationships be stated explicitly up front. This allows ML techniques to tackle complicated\nscenarios involving many variables without extensive setup, particularly where the causal factors of a particular\noutcome are complex or unclear, but the resulting model can be something of a black box. If a model underperforms, it\ncan be difficult to understand why, and furthermore how to improve its performance. The black box model can also conceal\nimplicit biases and fail to establish clear reasons for decisions. Industries that have regulations around trustworthy\nsystems, like finance and healthcare, might require stronger evidence that the model is producing the correct results\nfor the right reasons.",
        "To help address such concerns, the Snowflake Model Registry includes an explainability function based on\nShapley values. Shapley values are a\nway to attribute the output of a machine learning model to its input features. By considering all possible combinations of\nfeatures, Shapley values measure the average marginal contribution of each feature to the model\u2019s prediction. This\napproach ensures fairness in attributing importance and provides a solid foundation for understanding complex models.\nWhile computationally intensive, the insights gained from Shapley values are invaluable for model interpretability and\ndebugging.",
        "For example, assume we have a model for predicting the price of a house, which was trained on the homes\u2019 size, location,\nnumber of bedrooms, and whether pets are allowed. In this example, the average price of houses is $100,000, and the final\nprediction of the model was $250,000 for a house that is 2000 square feet, beachside, three bedrooms, and doesn\u2019t allow\npets. Each of these feature values might contribute to the final model prediction as shown in the following table.",
        "Feature",
        "Value",
        "Contribution vs. an average house",
        "Size",
        "2000",
        "+$50,000",
        "Location",
        "Beachside",
        "+$75,000",
        "Bedrooms",
        "3",
        "+$50,000",
        "Pets",
        "No",
        "-$25,000",
        "Together, these contributions explain why this particular house is priced $150,000 higher than an average home. Shapley\nvalues can affect the final outcome positively or negatively, adding up to a difference of outcomes compared to an\naverage. In this example, it is less desirable to live in a house where pets are not allowed, so that feature value\u2019s\ncontribution is -$25,000.",
        "The average value is calculated using background data, a representative sample of the entire dataset. For more information,\nsee Logging models with background data.",
        "This preview release supports the following Python-native model packages.",
        "XGBoost",
        "CatBoost",
        "LightGBM",
        "Scikit-learn",
        "The following Snowpark ML modeling classes from snowflake.ml.modeling are supported.",
        "XGBoost",
        "LightGBM",
        "Scikit-learn (except pipeline models)",
        "Explainability is available by default for the above models logged using Snowpark ML 1.6.2 and later. The\nimplementation uses the SHAP library.",
        "Background data, typically a sample of representative data, is an important ingredient of Shapley value-based\nexplanations. Background data gives the Shapley algorithm an idea of what \u201caverage\u201d inputs look like to which it can\ncompare individual explanations.",
        "The Shapley value is computed by systematically perturbing input features and replacing them with the background data.\nBecause it reports deviation from background data, it is important to use consistent background data when comparing\nShapley values from multiple data sets.",
        "Some tree-based models implicitly encode background data within their structure during training, and may not require\nexplicit background data. Most models, however, require background data to be provided separately for useful\nexplanations, and all models (including tree-based models) can be explained more accurately if you provide background\ndata.",
        "You can provide up to 1,000 rows of background data when logging a model by passing it in the sample_input_data\nparameter, as shown below.",
        "Note",
        "If the model is a type that requires explicit background data to calculate Shapley values, explainability cannot be\nenabled without this data.",
        "You can also provide background data while logging the model with a signature, as shown below.",
        "Models with explainability have a method named explain that returns the Shapley values for the model\u2019s features.",
        "Because Shapley values are explanations of predictions made from specific inputs, you must pass input data to explain to\ngenerate the predictions to be explained.",
        "The Snowflake model version object will have a method called explain, and you call it using ModelVersion.run in Python.",
        "The following is an example of retrieving the explanation in SQL.",
        "Important",
        "If you are using snowflake-ml-python prior to version 1.7.0, you may receive the error UnicodeDecodeError: 'utf-8' codec can't decode byte with XGBoost models.\nThis is due to an incompatibility between version 0.42.1 of the SHAP library and the latest XGBoost version (2.1.1) supported by Snowflake.\nIf you cannot upgrade snowflake-ml-python to version 1.7.0 or later, downgrade the XGBoost version to 2.0.3 and log the model with the relax_version option set to False,\nas shown in the following example.",
        "Models that were logged in the registry using a version of Snowpark ML older than 1.6.2 do not have the explainability\nfeature. Since model versions are immutable, you must create a new model version to add explainability to an existing\nmodel. You can use ModelVersion.load to retrieve the Python object represeting the model\u2019s implementation, then log\nthat to the registry as a new model version. Be sure to pass your background data as sample_input_data. This\napproach is shown below.",
        "Important",
        "The Python environment into which you load the model must be exactly the same (that is, the same version of Python\nand of all libraries) as the environment where the model is deployed. For details, see\nLoading a model version.",
        "Explainability is enabled by default if the model supports it. To log a model version in the registry without\nexplainability, pass False for the enable_explainability option when logging the model, as shown here.",
        "Was this page helpful?",
        "On this page"
    ]
}