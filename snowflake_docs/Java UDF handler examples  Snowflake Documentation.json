{
    "url": "https://docs.snowflake.com/en/developer-guide/udf/java/udf-java-cookbook",
    "title": "Java UDF handler examples | Snowflake Documentation",
    "paragraphs": [
        "This topic includes simple examples of UDF handler code written in Java.",
        "For more on using Java to create a UDF handler, see Creating a Java UDF handler.",
        "The following statements create and call an in-line Java UDF. This code returns the VARCHAR passed to it.",
        "This function is declared with the optional CALLED ON NULL INPUT clause to indicate that the function is\ncalled even if the value of the input is NULL. (This function would return NULL with or without this clause, but\nyou could modify the code to handle NULL another way, for example, to return an empty string.)",
        "Create the UDF:",
        "Call the UDF:",
        "This uses the echo_varchar() UDF defined above. The SQL NULL value is implicitly converted to\nJava null, and that Java null is returned and implicitly converted back to SQL NULL:",
        "Call the UDF:",
        "Java methods can receive SQL arrays in either of two ways:",
        "Using Java\u2019s array feature.",
        "Using Java\u2019s varargs (variable number of arguments) feature.",
        "In both cases, your SQL code must pass an ARRAY.",
        "Note",
        "Be sure to use Java types with valid mappings to SQL types. For more information, refer to SQL-Java Data Type Mappings.",
        "Declare the Java parameter as an array. For example, the third parameter in the following method is a String array:",
        "Below is a complete example:",
        "Create and load the table:",
        "Create the UDF:",
        "Call the UDF:",
        "Using varargs is very similar to using an array.",
        "In your Java code, use Java\u2019s varargs declaration style:",
        "Below is a complete example. The only significant difference between this example and the preceding example (for arrays) is the\ndeclaration of the parameters to the method.",
        "Create and load the table:",
        "Create the UDF:",
        "Call the UDF:",
        "The following code shows how to return a NULL value explicitly. The Java value null is converted to\nSQL NULL.",
        "Create the UDF:",
        "Call the UDF:",
        "The following example uses the SQL OBJECT data type and the corresponding Java\ndata type (Map<String, String>), and extracts a value from the OBJECT. This example also shows that you\ncan pass multiple parameters to a Java UDF.",
        "Create and load a table that contains a column of type OBJECT:",
        "Create the UDF:",
        "Call the UDF:",
        "The following example uses the SQL GEOGRAPHY data type.",
        "Create the UDF:",
        "You can use the PACKAGES clause to specify a Snowflake system package such as\nthe Snowpark package.\nWhen you do, you don\u2019t need to also include the Snowpark JAR file as a value of an IMPORTS clause. For more on PACKAGES, see\nCREATE FUNCTION optional parameters.",
        "Create data and call the UDF with that data:",
        "The output looks similar to:",
        "When you pass a value of the SQL VARIANT type to a Java UDF, Snowflake can convert the value to the\nVariant type\nprovided with the Snowpark package. Note that\nVariant is supported from the Snowpark package version 1.4.0 and later.",
        "The Snowpark Variant type provides methods for converting values between Variant and other types.",
        "To use the Snowpark Variant type, use the PACKAGES clause to specify the Snowpark package when creating the UDF. When you do, you\ndon\u2019t need to also include the Snowpark JAR file as a value of an IMPORTS clause. For more information on PACKAGES, see\nCREATE FUNCTION optional parameters.",
        "Code in the following example receives JSON data stored as the VARIANT type, then uses the Variant type in the Snowpark library\nto retrieve the price value from the JSON. The received JSON has a structure similar to the JSON displayed in\nSample Data Used in Examples.",
        "You can read the contents of a file with handler code. For example, you might want to read a file to process unstructured data with the\nhandler. For more information on processing unstructured data, along with example code, refer to Processing unstructured data with UDF and procedure handlers.",
        "The file must be on a Snowflake stage that\u2019s available to your handler.",
        "To read the contents of staged files, your handler can:",
        "Read a file whose file path is statically-specified in the IMPORTS clause. At run time,\nyour code reads the file from the UDF\u2019s home directory.",
        "This can be useful when you want to access the file during initialization.",
        "Read a dynamically-specified file by calling methods of either the SnowflakeFile class or the InputStream class.",
        "You might do this if you need to access a file specified by the caller. For more information, see the following in this topic:",
        "Reading a dynamically-specified file with SnowflakeFile",
        "Reading a dynamically-specified file with InputStream",
        "SnowflakeFile provides features not available with InputStream, as described in the following table.",
        "Class",
        "Input",
        "Notes",
        "SnowflakeFile",
        "URL formats:",
        "Scoped URL to reduce the risk of file injection attacks when the function\u2019s caller is not also its owner.",
        "File URL or string path for files that the UDF owner has access to.",
        "The file must be located in a named internal stage or an external stage.",
        "Easily access additional file attributes, such as file size.",
        "InputStream",
        "URL formats:",
        "Scoped URL to reduce the risk of file injection attacks when the function\u2019s caller is not also its owner.",
        "The file must be located in an internal or external stage.",
        "Before your Java handler code can read a file on a stage, you must do the following to make the file available to the code:",
        "Create a stage that\u2019s available to your handler.",
        "You can use an external stage or internal stage. If you use an internal stage, it must be a user or named stage;\nSnowflake does not currently support using a table stage for UDF dependencies. For more on creating a stage, see\nCREATE STAGE. For more on choosing an internal stage type, see\nChoosing an internal stage for local files.",
        "Keep in mind that adequate privileges on the stage must be assigned to roles performing SQL actions that read from the stage. For more\ninformation, see Granting privileges for user-defined functions.",
        "To the stage, copy the file that will be read by code.",
        "You can copy the file from a local drive to a stage by using the PUT command. For command reference, see PUT.\nFor information on staging files with PUT, see Staging data files from a local file system.",
        "Your handler can read a file whose stage path has been specified in the IMPORTS clause of the\nCREATE FUNCTION command.",
        "When you specify a file in the IMPORTS clause, Snowflake copies that file from the stage to the UDF\u2019s\nhome directory (also called the import directory), which is the directory from which the UDF actually reads the file.",
        "Because imported files are copied to a single directory and must have unique names within that directory, each file in the\nIMPORTS clause must have a distinct name, even if the files start out in different stages or different subdirectories within a\nstage.",
        "The following example creates and calls a Java UDF that reads a file.",
        "The Java source code below creates a Java method named readFile. This UDF uses this method.",
        "The following SQL code creates the UDF. This code assumes that the Java source code has been compiled and put into a JAR\nfile named TestReadRelativeFile.jar, which the UDF imports. The second and third imported files,\nmy_config_file_1.txt and my_config_file_2.txt, are configuration files that the UDF can read.",
        "This code calls the UDF:",
        "Files in a stage can be stored in compressed or uncompressed format. Users can compress the file before copying it\nto the stage, or can tell the PUT command to compress the file.",
        "When Snowflake copies a file compressed in GZIP format from a stage to the UDF home directory, Snowflake can write the copy as-is, or\nSnowflake can decompress the content before writing the file.",
        "If the file in the stage is compressed, and if you would like the copy in the UDF home directory to also be compressed, then when\nyou specify the file name in the IMPORTS clause, simply use the original file name (e.g. \u201cMyData.txt.gz\u201d) in the IMPORTS\nclause. For example:",
        "If the file in the stage is GZIP-compressed, but you would like the copy in the UDF home directory to be uncompressed, then when you\nspecify the file name in the IMPORTS clause, omit the \u201c.gz\u201d extension. For example, if your stage contains \u201cMyData.txt.gz\u201d, but you\nwant your UDF to read the file in uncompressed format, then specify \u201cMyData.txt\u201d in the IMPORTS clause. If there is not already an\nuncompressed file named \u201cMyData.txt\u201d, then Snowflake searches for \u201cMyData.txt.gz\u201d and automatically writes a decompressed copy to\n\u201cMyData.txt\u201d in the UDF home directory. Your UDF can then open and read the uncompressed file \u201cMyData.txt\u201d.",
        "Note that smart decompression applies only to the copy in the UDF home directory; the original file in the stage is not changed.",
        "Follow these best practices for handling compressed files:",
        "Follow proper file naming conventions. If a file is in GZIP-compressed format, then include the extension \u201c.gz\u201d at the end of the file\nname. If a file is not in GZIP-compressed format, then do not end the file name with the \u201c.gz\u201d extension.",
        "Avoid creating files whose names differ only by the extension \u201c.gz\u201d. For example, do not create both \u201cMyData.txt\u201d and \u201cMyData.txt.gz\u201d\nin the same stage and directory, and do not try to import both \u201cMyData.txt\u201d and \u201cMyData.txt.gz\u201d in the same CREATE FUNCTION command.",
        "Do not compress files twice. For example, if you compress a file manually, and then you PUT that file without using\nAUTO_COMPRESS=FALSE, the file will be compressed a second time. Smart decompression will decompress it only once, so the data\n(or JAR) file will still be compressed when it is stored in the UDF home directory.",
        "In the future, Snowflake might extend smart decompression to compression algorithms other than GZIP. To prevent compatibility issues\nin the future, apply these best practices to files that use any type of compression.",
        "Note",
        "JAR files can also be stored in compressed or uncompressed format in a stage. Snowflake automatically decompresses all compressed\nJAR files before making them available to the Java UDF.",
        "Using methods of the SnowflakeFile class, you can read files from a stage with your Java handler code. The SnowflakeFile\nclass is included on the classpath available to Java UDF handlers on Snowflake.",
        "Note",
        "To make your code resilient to file injection attacks, always use a scoped URL when passing a file\u2019s location to a UDF, particularly\nwhen the function\u2019s caller is not also its owner. You can create a scoped URL in SQL using the built-in function\nBUILD_SCOPED_FILE_URL. For more information about what the BUILD_SCOPED_FILE_URL does, see\nIntroduction to unstructured data.",
        "To develop your UDF code locally, add the Snowpark JAR containing SnowflakeFile to your code\u2019s class path. For information about\nsnowpark.jar, see  Setting Up Your Development Environment for Snowpark Java. Note that Snowpark client applications cannot use this class.",
        "When you use SnowflakeFile, it isn\u2019t necessary to also specify either the staged file or the JAR containing\nSnowflakeFile with an IMPORTS clause when you create the UDF, as in SQL with a CREATE FUNCTION statement.",
        "Code in the following example uses SnowflakeFile to read a file from a specified stage location. Using an\nInputStream from the getInputStream method, it reads the file\u2019s contents into a String variable.",
        "Call the UDF, passing the location of the file in a scoped URL to reduce the likelihood of file injection attacks.",
        "Note",
        "The UDF owner must have access to any files whose locations are not scoped URLs. You can read these staged files by having the handler\ncode call the SnowflakeFile.newInstance method with a boolean value for a new requireScopedUrl parameter.",
        "The following example uses SnowflakeFile.newInstance while specifying that a scoped URL is not required.",
        "You can read file contents directly into a java.io.InputStream by making your handler function\u2019s argument an InputStream\nvariable. This can be useful when the function\u2019s caller will want to pass a file path as an argument.",
        "Note",
        "To make your code resilient to file injection attacks, always use a scoped URL when passing a file\u2019s location to a UDF, particularly\nwhen the function\u2019s caller is not also its owner. You can create a scoped URL in SQL using the built-in function\nBUILD_SCOPED_FILE_URL. For more information about what the BUILD_SCOPED_FILE_URL does, see\nIntroduction to unstructured data.",
        "Code in the following example has a handler function sumTotalSales that takes an InputStream and returns an int.\nAt run time, Snowflake automatically assigns the contents of the file at the file variable\u2019s path to the stream\nargument variable.",
        "Call the UDF, passing the location of the file in a scoped URL to reduce the likelihood of file injection attacks.",
        "The following statements create a simple Java UDF. This sample generally follows the file and directory structure\ndescribed in Organizing your files.",
        "In the root directory of your project (here, my_udf), create a src subdirectory to hold the source .java files and a\nclasses subdirectory to hold the generated .class files.",
        "You should have a directory hierarchy similar to the following:",
        "In the src directory, create a directory called mypackage to hold .java files whose classes are in the\nmypackage package.",
        "In the mypackage directory, create a MyUDFHandler.java file that contains your source code.",
        "From your project root directory (here, my_udf), use the javac command to compile the source code.",
        "The javac command in the following example compiles MyUDFHandler.java to generate a MyUDFHandler.class file in the\nclasses directory.",
        "This example includes the following arguments:",
        "-d classes \u2013 Directory into which generated class files should be written.",
        "src/mypackage/MyUDFHandler.java \u2013 Path to the .java file in the form: source_directory/package_directory/Java_file_name.",
        "Optionally, in the project root directory create a manifest file named my_udf.manifest that contains the following attributes:",
        "From your project root directory, run the jar command to create a JAR file containing the .class file and manifest.",
        "The jar command in the following example puts the generated MyUDFHandler.class file in a mypackage package folder\ninto a .jar file called my_udf.jar. The -C ./classes flag specifies the location of the .class files.",
        "This example includes the following arguments:",
        "cmf \u2013 Command arguments: c to create a JAR file, m to use the specified .manifest file, and f to give the JAR\nfile the specified name.",
        "my_udf.manifest \u2013 Manifest file.",
        "my_udf.jar \u2013 Name of the JAR file to create.",
        "-C ./classes \u2013 Directory containing the generated .class files.",
        "mypackage/MyUDFHandler.class \u2013 Package and name of .class file to include in the JAR.",
        "In Snowflake, create a stage called jar_stage to store the JAR file containing your UDF handler.",
        "For more information on creating a stage, see CREATE STAGE.",
        "Use the PUT command to copy the JAR file from the local file system to a stage.",
        "You can store the PUT command in a script file and then execute that file through SnowSQL.",
        "The snowsql command looks similar to the following:",
        "This example assumes that the user\u2019s password is specified in the SNOWSQL_PWD environment variable.",
        "Create the UDF:",
        "Call the UDF:",
        "Was this page helpful?",
        "On this page",
        "Related content"
    ]
}