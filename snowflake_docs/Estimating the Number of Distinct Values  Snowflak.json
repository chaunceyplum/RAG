{
    "url": "https://docs.snowflake.com/en/user-guide/querying-approximate-cardinality",
    "title": "Estimating the Number of Distinct Values | Snowflake Documentation",
    "paragraphs": [
        "Snowflake uses HyperLogLog to estimate the approximate number of distinct values in a data set. HyperLogLog is a state-of-the-art cardinality estimation algorithm, capable of estimating distinct\ncardinalities of trillions of rows with an average relative error of a few percent.",
        "HyperLogLog can be used in place of COUNT(DISTINCT \u2026) in situations where estimating cardinality is acceptable.",
        "Snowflake provides a bias-corrected implementation of the HyperLogLog algorithm presented in HyperLogLog: the analysis of a near-optimal cardinality estimation algorithm by Flajolet et al.",
        "We recommend using HyperLogLog whenever the input is potentially large and an approximate result is acceptable. The average relative error of our HyperLogLog implementation is 1.62338% (i.e. the\naverage relative difference to the corresponding COUNT(DISTINCT \u2026) result).",
        "The following Aggregate functions are provided for estimating cardinality using HyperLogLog:",
        "HLL: Returns an approximation of the distinct cardinality of the input.",
        "HLL_ACCUMULATE: Skips the final estimation step and returns the HyperLogLog state at the end of an aggregation.",
        "HLL_COMBINE: Combines (i.e. merges) input states into a single output state.",
        "HLL_ESTIMATE: Computes a cardinality estimate of a HyperLogLog state produced by HLL_ACCUMULATE and HLL_COMBINE.",
        "HLL_EXPORT: Converts HyperLogLog states from BINARY format to an OBJECT (which can then be printed and exported as JSON).",
        "HLL_IMPORT: Converts HyperLogLog states from OBJECT format to BINARY format.",
        "Our implementation hashes input rows to 64-bit values, of which the upper 12 bits or \u201cprecision\u201d (as referred to in the HyperLogLog algorithm paper; see above document link for details) are used to\npartition input values into so-called sub-streams. This yields an average relative error of:",
        "sqrt(3*ln(2)-1)/sqrt(2^precision) = 0.0162338 = 1.62338%",
        "In other words, for a query where COUNT(DISTINCT \u2026) would return a result of 1,000,000, HyperLogLog typically returns a result in the range of\n983,767 to 1,016,234.",
        "For each sub-stream, HyperLogLog maintains the maximum leading-zero count (between 0 and 52 for 64-bit values at precision = 12). The most straight-forward representation of this state is a simple byte\narray, one byte for each of the 2^12 = 4096 sub-streams. Our implementation indeed requires at most 4096 Byte (2^precision = 2^12 = 4096) of memory per aggregation group. Technically, only\n6 bits (rather than 8 bits) are required per sub-stream, but we trade some space efficiency for computational efficiency.",
        "For small input cardinalities, most of the sub-streams will never be hit. So rather than allocating an entire block of 4096 Byte per aggregation group up-front, our implementation uses a space-optimized\n\u201csparse\u201d representation of this state whenever beneficial. Consequently, the memory cost of HyperLogLog can be substantially lower than 4096 Byte per aggregation group (down to about 32 Byte per\naggregation group). This allows cardinality estimation over many aggregation groups (millions or even billions, as determined by the GROUP BY or OVER clause of the query), using orders of magnitude less\nmemory and CPU time than a corresponding COUNT(DISTINCT \u2026) query.",
        "In the (rare) case where an extremely large input table and many aggregation groups cause HyperLogLog to exceed its total memory budget, Snowflake is still able to spill to temp space and perform\nrecursive aggregation, as with any other aggregation function.",
        "The state of the HyperLogLog algorithm can be exported and imported (or reimported) using the HLL_EXPORT and HLL_IMPORT functions,\nrespectively. The exported state is of type OBJECT and contains the following fields.",
        "Version number of the HyperLogLog implementation.",
        "Number of hashed value bits to use to select sub-streams. Currently fixed to 12.",
        "An array of integers, each containing the maximum leading-zero count + 1 for the corresponding sub-stream. 0 indicates that the corresponding sub-stream has not been hit yet. Legal values\nare in the range of 0 to 53. The corresponding sub-stream index is given by the element position in the array.",
        "For example:",
        "Version number of the HyperLogLog implementation.",
        "Number of hashed value bits to use to select sub-streams. Currently fixed to 12.",
        "indices: An array of integers, each containing a sub-stream index (base 0). Legal values are in the range of 0 to 4095.",
        "Legal values are in the range of 0 to 53. The sub-stream for a given leading-zero count is given by the corresponding element in the indices array.",
        "The indices and maxLzCounts arrays must have the same length. The HLL_IMPORT function also checks that sub-stream indices are in the valid range, and\nthat there are no duplicate sub-stream indices. The indices array need not be sorted. The leading-zero counts are not validated. Invalid values will not cause query failures, but will lead\nto undefined results for HLL_ESTIMATE.",
        "For example:",
        "Environment set up:",
        "Step 1:",
        "Create a table that contains the calendar date (year/month/day) and the HLL structure. We use HLL_EXPORT to store the binary structure as a text object:",
        "Step 2:",
        "We can calculate the unique IPs by month by aggregating each day\u2019s HLL structure from Step 1. We use HLL_IMPORT to transform the text to the binary structure, then\nHLL_COMBINE to combine multiple HLL structures into a single structure, then HLL_ESTIMATE to compute the number of distinct values:",
        "Compare:",
        "We compare the use of the aggregation using the HLL functions to HLL on the detail level data. In this case, HLL() is equivalent to the HLL_ESTIMATE(HLL_COMBINE(HLL_IMPORT())) from\nStep 2:",
        "As you can see, aggregation of the HLL structures is significantly faster than aggregation over the base data, e.g. 1.3 seconds vs 149 seconds in this small example, which represents a 100x decrease\nin query time.",
        "Was this page helpful?",
        "On this page"
    ]
}