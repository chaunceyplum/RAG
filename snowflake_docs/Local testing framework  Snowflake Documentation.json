{
    "url": "https://docs.snowflake.com/en/developer-guide/snowpark/python/testing-locally",
    "title": "Local testing framework | Snowflake Documentation",
    "paragraphs": [
        "This topic explains how to test your code locally when working with the Snowpark Python library.",
        "The Snowpark Python local testing framework is an emulator that allows you to create and operate on Snowpark Python DataFrames locally without\nconnecting to a Snowflake account. You can use the local testing framework to test your DataFrame operations on your development\nmachine or in a CI (continuous integration) pipeline before deploying code changes to your account. The API is the same,\nso you can run your tests either locally or against a Snowflake account without making code changes.",
        "To use the local testing framework:",
        "You must use version 1.18.0 or later of the Snowpark Python library with the optional dependency localtest.",
        "The supported versions of Python are:",
        "3.9",
        "3.10",
        "3.11",
        "To install the library with the optional dependency, run the following command:",
        "Create a Snowpark Session and set the local testing configuration to True:",
        "Use the session to create and operate on DataFrames:",
        "You can create Snowpark DataFrames from Python primitives, files, and pandas DataFrames.\nThis is useful for specifying the input and expected output of test cases. With this method,\nthe data is in source control, which makes it easier to keep the test data in sync with the test cases.",
        "To load CSV files into a Snowpark DataFrame, first call Session.file.put() to load the file to the in-memory stage, and then use Session.read() to read the contents.",
        "Example",
        "Assume there is a file, data.csv, with the following contents:",
        "You can use the following code to load data.csv into a Snowpark DataFrame.\nYou need to put the file onto a stage first; If you do not, you will receive a \u201cfile cannot be found\u201d error.",
        "Expected output:",
        "To create a Snowpark Python DataFrame from a pandas DataFrame, call the create_dataframe method and pass the data as a pandas DataFrame.",
        "Example",
        "Expected output:",
        "To convert a Snowpark Python DataFrame to a pandas DataFrame, call the to_pandas method on the DataFrame.",
        "Example",
        "Expected output:",
        "PyTest fixtures are functions that are executed before a test (or module of tests),\ntypically to provide data or connections to tests. In this procedure, you create a fixture that returns a Snowpark Session object.",
        "If you do not already have a test directory, create one.",
        "In the test directory, create a file named conftest.py with the following contents, where connection_parameters is a dictionary with your Snowflake account credentials:",
        "For more information about the dictionary format, see Creating a Session.",
        "The call to pytest_addoption adds a command line option named snowflake-session to the pytest command.\nThe Session fixture checks this command line option and creates a local or live Session, depending on its value.\nThis lets you easily switch between local and live modes for testing, as shown in the following command line examples:",
        "Session.sql(...) is not supported in the local testing framework. Use Snowpark\u2019s DataFrame APIs whenever possible,\nand in cases where you must use Session.sql(...), you can mock the tabular return value by using Python\u2019s\nunittest.mock.patch to patch the expected response from a given Session.sql() call.",
        "In the following example, mock_sql() maps the SQL query text to the desired DataFrame response.\nThe conditional statement checks whether the current session is using local testing, and if so, applies the patch to the Session.sql() method.",
        "When local testing is enabled, all tables created by DataFrame.save_as_table() are saved as temporary tables in memory and can be\nretrieved using Session.table(). You can use the supported DataFrame operations on the table as usual.",
        "Some of the built-in functions under snowflake.snowpark.functions are not supported in the local testing framework.\nIf you use a function that is not supported, you can use the @patch decorator from snowflake.snowpark.mock to create a patch.",
        "For the patched function to be defined and implemented, the signature (parameter list) must align with the built-in function\u2019s parameters. The local testing framework passes parameters to the patched function using the following rules:",
        "For parameters of type ColumnOrName in the signature of built-in functions, ColumnEmulator is passed as the parameter of the patched functions.\nColumnEmulator is similar to a pandas.Series object that contains the column data.",
        "For parameters of type LiteralType in the signature of built-in functions, the literal value is passed as the parameter of the patched functions.",
        "Otherwise, the raw value is passed as the parameter of the patched functions.",
        "As for the returning type of the patched functions, returning an instance of ColumnEmulator is expected in correspondence with the returning type of Column of built-in functions.",
        "For example, the built-in function to_timestamp() could be patched like this:",
        "If your PyTest test suite contains a test case that is not well supported by local testing, you can skip those cases by using PyTest\u2019s mark.skipif decorator.\nThe following example assumes that you configured your session and parameters as described earlier. The condition checks whether the local_testing_mode is set to local; if so, the test case is skipped with an explanatory message.",
        "You can create and call user-defined functions (UDFs) and stored procedures in the local testing framework. To create the objects, you can\nuse the following syntax options:",
        "Syntax",
        "UDF",
        "Stored procedure",
        "Decorators",
        "@udf",
        "@sproc",
        "Register methods",
        "udf.register()",
        "sproc.register()",
        "Register-from-file methods",
        "udf.register_from_file()",
        "sproc.register_from_file()",
        "Example",
        "The following code example creates a UDF and stored procedure using the decorators, and then calls both by name:",
        "The following list contains the known limitations and behavior gaps in the local testing framework. Snowflake currently has no plans to address these\nitems.",
        "Raw SQL strings and operations that require parsing SQL strings, such as session.sql and DataFrame.filter(\"col1 > 12\"),\nare not supported.",
        "Asynchronous operations are not supported.",
        "Database objects such as tables, stored procedures, and UDFs are not persisted beyond the session level, and all operations are performed\nin memory. For example, permanent stored procedures registered in one mock session are not visible to other mock sessions.",
        "String collation related features, such as Column.collate, are not supported.",
        "Variant, Array, and Object data types are only supported with standard JSON encoding and decoding. Expressions\nlike [1,2,,3,] are considered valid JSON in Snowflake but not in local testing, where Python\u2019s built-in JSON functionalities are used. You\ncan specify the module-level variables snowflake.snowpark.mock.CUSTOM_JSON_ENCODER and\nsnowflake.snowpark.mock.CUSTOM_JSON_DECODER to override the default settings.",
        "Only a subset of Snowflake\u2019s functions (including window functions) are implemented. To learn how to inject your own function definition,\nsee Patching built-in functions.",
        "Patching rank-related functions is currently not supported.",
        "SQL format models are not supported. For example, the mock implementation of to_decimal does not handle the\noptional parameter format.",
        "The Snowpark Python library does not have a built-in Python API to create or drop stages, so the local testing framework assumes that every\nincoming stage has already been created.",
        "The current implementation of UDFs and stored procedures does not perform any package validation. All packages referenced in your code\nneed to be installed before the program is executed.",
        "Query tags are not supported.",
        "Query history is not supported.",
        "Lineage is not supported.",
        "When a UDF or stored procedure is registered, optional parameters such as parallel, execute_as, statement_params,\nsource_code_display, external_access_integrations, secrets, and comment are ignored.",
        "For Table.sample, SYSTEM or BLOCK sampling is the same as ROW sampling.",
        "Snowflake does not officially support running the local testing framework inside stored procedures. Sessions of local testing mode inside\nstored procedures might encounter or trigger unexpected errors.",
        "The following is a list of features that are currently not implemented in the local testing framework. Snowflake is actively working to address\nthese items.",
        "In general, any reference to these functionalities should raise a NotImplementedError:",
        "UDTFs (user-defined table functions)",
        "UDAFs (user-defined aggregate functions)",
        "Vectorized UDFs and UDTFs",
        "Built-in table functions",
        "Table stored procedures",
        "Geometry, Geography, and Vector data types",
        "Interval expressions",
        "Read file formats other than JSON and CSV",
        "For a supported file format, not all read options are supported. For example, infer_schema is not supported for the CSV format.",
        "For any features not listed here as unsupported or as a known limitation, check the latest list of feature requests for local testing, or\ncreate a feature request in the snowpark-python GitHub repository.",
        "The following is a list of known issues or behavior gaps that exist in the local testing framework. Snowflake is actively planning to address\nthese issues.",
        "Using window functions inside DataFrame.groupby or other aggregation operations is not supported.",
        "Selecting columns with the same name will only return one column. As a workaround, use\nColumn.alias to rename the columns to have distinct names.",
        "For Table.merge and Table.update, the session parameters ERROR_ON_NONDETERMINISTIC_UPDATE and\nERROR_ON_NONDETERMINISTIC_MERGE must be set to False. This means that for multi-joins, one of the matched rows is updated.",
        "Fully qualified stage names in GET and PUT file operations are not supported. Database and schema names are treated as part of the stage name.",
        "The mock_to_char implementation only supports timestamps in a format that has separators between different time parts.",
        "DataFrame.pivot has a parameter called values that allows a pivot to be limited to specific values. Only statistically\ndefined values can be used at this time. Values that are provided using a subquery will raise an error.",
        "Creating a DataFrame from a pandas DataFrame that contains a timestamp with timezone information is not supported.",
        "For any issues not mentioned in this list, check the latest list of open issues, or\ncreate a bug report in the snowpark-python GitHub repository.",
        "Was this page helpful?",
        "On this page"
    ]
}