{
    "url": "https://docs.snowflake.com/en/user-guide/querying-time-series-data",
    "title": "Analyzing time-series data | Snowflake Documentation",
    "paragraphs": [
        "You can analyze time-series data in Snowflake, using functionality designed specifically\nfor this purpose. Database administrators, data scientists, and application developers\nhave to make sure that the time series is stored and loaded efficiently, and in many cases\nsummarized into a form that is complete and consistent, before making the data available\nto business analysts and other consumers.",
        "A time series consists of sequential observations that capture how systems, processes, and\nbehaviors change over a period of time. Time-series data is collected from a broad\nrange of devices across a broad range of industries. Common examples include stock-trading data\ncollected for financial applications, weather observations, temperature readings collected from\nsensors in smart factories, and logs of user clicks in digital advertising.",
        "A single record in a time series typically has the following components:",
        "A date, time, or timestamp that has a consistent level of granularity\n(milliseconds, seconds, minutes, hours, etc.).",
        "One or more measurements or metrics of some kind, usually numeric (facts that might reveal trends\nor anomalies in the data).",
        "Dimensions of interest that are associated with the measurement, such as a location for a\ntemperature reading, or a stock symbol for a given trade.",
        "For example, the following weather observation has start and end timestamps, a rainfall measurement (0.32),\nand location information:",
        "The following data collected from a factory device has a namespace (IOT), a tag ID or sensor ID (3000),\na timestamp for the temperature reading on the device, the temperature reading itself (21.1673), and a \u201cbroker timestamp,\u201d\nwhich is when the data subsequently arrived at the data broker. For example, the data broker might be a Kafka server that ingests data into a Snowflake table.",
        "A time series might reveal spikes when readings change dramatically for some reason. For example, the following image shows a sequence\nof temperature readings taken at 15-second intervals, with values peaking over 40\u00b0C after being steadily in the 35\u00b0C range for the previous day.",
        "The following sections show how to analyze and visualize large volumes of this kind of data with SQL functions and joins that provide fast, accurate results.",
        "The following datetime data types are supported:",
        "DATE",
        "TIME",
        "TIMESTAMP (and variations, including TIMESTAMP_TZ)",
        "For information about loading, managing, and querying data that uses these data types, see\nWorking with date and time values.",
        "A number of commonly used SQL functions are available\nto help with both storing and querying time-series data. For example, you can use\nCONVERT_TIMEZONE to convert timestamps from one time zone to\nanother, and you can use functions such as EXTRACT and\nTIMEADD to manipulate time-based data as needed.",
        "Note",
        "For TIMESTAMP_TZ data, Snowflake stores the offset of a given time zone, not the actual time zone,\nat the moment of creation for a given value.",
        "To optimize query performance, tables used for time-series analytics are often clustered by time (and\nsometimes also by sensor ID or a similar dimension). See Clustering Keys & Clustered Tables.",
        "Management of time-series data might require the aggregation of large volumes of fine-grained\nrecords into a more summarized form (a process sometimes referred to as \u201cdownsampling\u201d).\nGiven a large set of records with a specific time-based granularity (milliseconds, seconds, minutes, etc.),\nyou can roll up these records to a coarser granularity, effectively producing a\nsmaller sample.",
        "Downsampling is valuable because it decreases the size of a data set and its storage requirements.\nA coarser level of granularity also reduces compute resource requirements during query execution.\nAnother key reason for downsampling is that a large number of records\nin a time series might be redundant from an analyst\u2019s point of view. For example, if a sensor\nemits a new value once every second, but this measurement rarely changes within each 60-second interval,\nthe data can be rolled up to the minute level for analysis.",
        "Another case for downsampling occurs when two different data sets need to be analyzed as one, but\nthey have different time granularities. For example, Sensor A in a factory collects data every 15 seconds,\nbut Sensor B collects related data every 30 seconds. In this case, aggregating the records into 1-minute\nbuckets might be a good solution. IDs and dimensions in each data set are retained as they are, but numeric\nmeasurements are summed or averaged by a common time interval.",
        "You can downsample a data set that is stored in a table by using the\nTIME_SLICE function.\nThis function calculates the start and end times of fixed-width \u201cbuckets\u201d so that individual\nrecords can be grouped and summarized, using standard aggregate functions, such as SUM and\nAVG.",
        "Similarly, the DATE_TRUNC function truncates part of a\nseries of date or timestamp values, reducing their granularity. The following sections show examples of each function.",
        "The following example downsamples a table named sensor_data_ts, which contains readings\nfrom two factory sensors and contains 5.3 million rows. These readings were ingested per second, so 5.3 million rows\nrepresents only one month of data, with just over 2.5 million rows per sensor. You can use the TIME_SLICE function to\naggregate up to a single row per minute, per hour, or per day, for example.",
        "To run this example, first create and load the sensor_data_ts table; see Creating the sensor_data_ts table.\nHere is a small sample of the data in the table:",
        "The table contains 60 readings like these per minute for each device, as shown by this query:",
        "In this downsampling query, the TIME_SLICE function defines one-minute buckets and returns the start time of each bucket.\nThe AVG function calculates the average temperature for each bucket per device. The COUNT(*) function\nis included for reference, just to show how many rows land in each time bucket.",
        "The vibration and motor_rpm columns are not included, but they could be aggregated in the same way\nas the temperature column or by using different aggregate functions.",
        "Important",
        "If you run this example yourself, your output will not match exactly because the sensor_data_ts table is loaded\nwith randomly generated values.",
        "By using the TIME_SLICE function, you can create smaller, aggregated tables for analysis\npurposes, and you can apply the downsampling process at different levels (hour, day, week, and so on).",
        "The following example selects data from a table named order_header in the raw.pos\nschema of the\nTasty Bytes sample database.\nThis table contains 248M rows.",
        "The order_header table has a TIMESTAMP column named order_ts. The query creates an aggregated time series by\nusing this column as the second argument to the DATE_TRUNC function. The first argument specifies a day interval.\nThis means that the individual records, which have an hours/minutes/seconds granularity, are rolled up by day.",
        "The query groups the records by two dimensions: truck_id and\nlocation_id. The avg_amount column returns the average price per order, per food truck, per\nlocation for each business day on record.",
        "The query shown here limits the results to the first 25 rows for January 1, 2022. If you remove this date filter\nand the LIMIT clause, the query downsamples the original 248M rows to about 500,000 rows.",
        "By using windowed aggregate functions to observe how a metric changes over time, you can\nanalyze a time series for trends. Windowed aggregations are useful for analyzing data within defined\nsubsets (\u201cwindows\u201d) of a larger data set. You can compute rolling calculations (such as moving averages and sums)\nfor each row in a data set, taking into account a group of rows before, after, or surrounding the current row.\nThis kind of analysis contrasts with regular aggregations, which summarize the entire data set.",
        "By using range-based window frames with explicit offsets, you can apply a very flexible approach to computing\nthese rolling aggregations. The RANGE BETWEEN window frame, ordered by either timestamps or numbers, is not disrupted\nby gaps that may occur in time-series data. For instance, in the following illustration, the fact that Day 4\ndata is missing in the series of records does not affect the computation of aggregate functions over a three-day moving\nwindow. In particular, frames 3, 4, and 5 are computed correctly, taking into account that Day 4 data is unknown.",
        "The following example calculates a moving sum over weather data that records hourly precipitation readings in\ndifferent cities and counties. You can run this kind of query to evaluate trends in various time-series data sets,\nsuch as sensors and other IoT devices, especially when those data sets are known or expected to have gaps.",
        "The window function includes in its frame the current precipitation reading and all the readings that fall within the\nspecified time interval before the current reading. The rolling calculation is based on this flexible and\nlogical range of rows rather than an exact number of rows. The first row for each city has matching precip and\nmoving_sum_precip values. After that, the sum is recalculated for each subsequent row in the frame. The raw values\nfluctuate significantly, but the moving sums have a strong smoothing effect.",
        "To run this example, follow these instructions first: Create and load the heavy_weather table.\nThis very small table contains sporadic hourly weather observations, with lots of gaps, including a missing day. The query\nreturns the moving sum of precipitation values ordered by the start_time column. The window frame defines a\nrange between 12 hours before the current row and the current row. Therefore, the frame consists of the current row plus only those\nrows that have timestamps up to 12 hours earlier than the ORDER BY timestamp for the current row.",
        "The three moving_sum_precip values for Big Bear City are calculated as follows:",
        "0.42 = 0.42 (no preceding rows)",
        "0.42 + 0.09 = 0.51 (the first two rows are within the 12-hour window)",
        "0.07 = 0.07 (no preceding rows are within the 12-hour window)",
        "The South Lake Tahoe rows include these calculations, for example:",
        "0.56 + 0.38 + 0.28 + 0.80 = 2.02 (all four rows for 2024-12-23 are within 12 hours of each other)",
        "0.80 + 0.17 = 0.97 (one preceding row is within the 12-hour window)",
        "Other window functions, such as the\nLEAD and LAG ranking functions,\nare also commonly used in time-series analysis. Use the LEAD window function to find the next data point in the time series,\nrelative to the current data point, and the LAG function to find the previous data point.",
        "You can use Snowsight to visualize the results of aggregation queries, and get a\nbetter sense of the smoothing effect of calculations with sliding window frames.\nIn the query worksheet, click the Chart button next to Results.",
        "For example, the yellow line in the following bar chart shows a much smoother trend for average\ntemperature versus the blue line for the raw temperature. The query itself looks like this:",
        "The ability to select one column based on the minimum or maximum value of another column in the\nsame row is a common requirement for SQL developers who are working with time-series data.\nMIN_BY and MAX_BY are\nconvenience functions that return the starting and ending (or highest and lowest, or first and last)\nvalues in a table when the data is sorted by some other column, such as a timestamp.",
        "The first example simply finds the last (most recent) precip value in the whole table. The MAX_BY function sorts all the rows\nby their start_time value, then returns the precip value for the \u201cmax\u201d start time.",
        "To create and load the table used in the following examples, see Creating the heavy_weather table.",
        "You can verify this result (and get more information about it) by running this query:",
        "You can add a GROUP BY clause to ask more interesting questions about this data. For example, the following query\nfinds the last precipitation value that was observed for each city in California, ordered by precip values\n(high to low). The results are grouped by city to return the last precip value for each different city.",
        "The last time an observation was taken for the city of Alta, the precip value was 0.89,\nand the last time an observation was taken for the cities of South Lake Tahoe, Big Bear City, Montague, and Lebec, the precip\nvalue was 0.07 for all four locations. (Note that the query does not tell you when those observations were taken.)",
        "You can return the \u201copposite\u201d result set (oldest precip record versus most recent) by using the MIN_BY function.",
        "You can use the ASOF JOIN construct to join tables that\ncontain time-series data. Although ASOF JOIN queries can be emulated through the use of complex SQL, other types\nof joins, and window functions, these queries are easier to write (and are optimized) if you use the ASOF JOIN syntax.",
        "A common use for ASOF joins is the analysis of financial trading data.\nTransaction-cost analysis, for example, requires \u201cslippage\u201d calculations, which measure the difference\nbetween the price quoted at the time of a decision to buy stocks and the price actually paid when the trade\nwas executed and recorded. The ASOF JOIN can expedite this type of analysis. Given that the key capability\nof this join method is the analysis of one time series with respect to another, ASOF JOIN can be useful for\nanalyzing any data set that is historical in nature. In many of these use cases, ASOF JOIN can be used to\nassociate data when readings from different devices have timestamps that are not exactly the same.",
        "The assumption is that the time-series data you need to analyze exists in two tables, and there is a timestamp\nfor each row in each table. This timestamp represents the precise \u201cas of\u201d date and time for a recorded event.\nFor each row in the first (or left) table, the join uses a \u201cmatch condition\u201d with a comparison operator that\nyou specify to find a single row in the second (or right) table where the timestamp value is one of the\nfollowing:",
        "Less than or equal to the timestamp value in the left table.",
        "Greater than or equal to the timestamp value in the left table.",
        "Less than the timestamp value in the left table.",
        "Greater than the timestamp value in the left table.",
        "The qualifying row on the right side is the closest match, which could be equal in time, earlier in time, or\nlater in time, depending on the specified comparison operator.",
        "The cardinality of the result of the ASOF JOIN is always equal to the cardinality of the left table.\nIf the left table contains 40 million rows, the ASOF JOIN returns 40 million rows. Therefore, the left table\ncan be thought of as the \u201cpreserving\u201d table, and the right table as the \u201creferenced\u201d table.",
        "For example, in a financial application, you might have a table named quotes and a table named trades.\nOne table records the history of bids to buy stock, and the other records the history of actual trades.\nA bid to buy stocks happens before the trade (or possibly at the \u201csame\u201d time, depending on the granularity of\nthe recorded time). Both tables have timestamps, and both have other columns of interest that you might want to\ncompare. A simple ASOF JOIN query will return the closest quote (in time) before each trade. In other words,\nthe query asks: What was the price of a given stock at the time I made a trade?",
        "Assume that the trades table contains three rows, and the quotes table contains seven rows.\nThe background color of the cells shows which three rows from quotes will qualify for the ASOF JOIN when the\nrows are joined on matching stock symbols and their timestamp columns are compared.",
        "TRADES Table (Left or \u201cPreserving\u201d Table)",
        "QUOTES Table (Right or \u201cReferenced\u201d Table)",
        "This conceptual example is easy to turn into a specific ASOF JOIN query:",
        "The ON condition groups the matched rows by their stock symbols.",
        "To run this example, create and load the tables as follows:",
        "For more examples of ASOF JOIN queries, see Examples.",
        "In addition to aligning the data in two tables via non-exact matches on time-based columns, ASOF JOIN is useful for filling gaps in a time series when your raw data table is missing rows for particular dates or timestamps. This process is known as \u201cgap-filling\u201d or \u201cinterpolation.\u201d When rows are missing because faulty equipment, or a power failure, results in skipped sensor readings, you can use ASOF JOIN to interpolate values from a generated time series into the table. The missing rows are filled in with the last known value for the readings that are missing. This value is also known as the \u201clast observation carried forward\u201d (LOCF). The ASOF JOIN query returns a complete set of rows that are in chronological order and contiguous.",
        "To use ASOF JOIN for interpolation, follow these steps:",
        "Identify the gaps in your table by running a simple query.",
        "Generate a complete time series, with the appropriate grain, for the period of time that you need to cover. For example, your time series might\nbe a simple sequence of dates for a particular year, or a much more granular sequence of timestamps per second for some number of days. You can use\nSQL or a spreadsheet application to generate the list of values.",
        "The time series will also need a meaningful ID or dimension for each row that you will specify later in the ASOF JOIN ON condition.",
        "Write an ASOF JOIN query that interpolates values into the missing rows. The generated time series will be the preserving table and the raw data table\nwill be the referenced table.",
        "The following example requires the sensor_data_ts table. If you haven\u2019t already created and loaded it, see\nCreating the sensor_data_ts table. To simulate the need for a gap-filling operation, delete some rows from the table as follows:",
        "The result is a table that is missing five rows for DEVICE2 on March 7th (1:16 through 1:20).",
        "Now follow these steps to complete the gap-filling exercise.",
        "Note",
        "If you run this example yourself, your output will not match exactly because the sensor_data_ts table is loaded\nwith randomly generated values.",
        "Run the following query to identify the gaps:",
        "This query returns two rows for DEVICE2: the last row before the gap and the first row\nafter the gap.",
        "To generate a time series with a fine grain (one row per second) for the gap in the sensor_data_ts\ntable, create the following table, which contains generated timestamps:",
        "In this SQL statement, 5 is the number of seconds that you need to cover the gap. Note that the device ID value\n(DEVICE2) is included in the generated rows.",
        "The following query returns the five generated rows.",
        "Now you can run an ASOF JOIN query that joins continuous_timestamps to sensor_data_ts and\ninterpolates values for missing rows for DEVICE2. The match condition finds the closest\nrow in time for each missing row, and the ON condition guarantees that interpolation occurs\non matching device IDs.",
        "The closest row for the missing rows is the row with the 2024-03-07 00:01:16.000 timestamp,\nassuming that >= is specified in the match condition, as shown in this example.",
        "This INSERT statement selects five rows from the ASOF JOIN operation and inserts them into the\nsensor_data_ts table.",
        "To check the results of the interpolation, select those five rows, and the two rows that directly precede and\nfollow them, from the sensor_data_ts table. Note that the five interpolated rows have picked up the same values\nfor the temperature, vibration, and motor_rpm columns that were recorded in the 2024-03-07 00:01:15.000\nrow. The interpolation was successful.",
        "You can train a model with ML Functions to do predictive analysis on time-series data:",
        "Time-Series Forecasting",
        "Anomaly Detection",
        "Top Insights",
        "Forecasting uses historical time-series data to make predictions about future data. Given a recorded time series\nwith actual observed values for dates and times in the past, the ML model forecasts what the observed values might be\nfor dates and times in the future.",
        "Anomaly detection identifies outliers, which are data points that deviate from an expected range. In the context of\na time series, an outlier is a measurement that is much larger or smaller than other measurements in a\nsimilar time interval. To find outliers, the ML function produces a forecast for the same time period that\nis being checked for anomalies, then compares the forecast results to the actual data.",
        "Top Insights finds the most important dimensions in a data set, builds segments from those dimensions, and detects which\nof those segments influenced a metric.",
        "Note",
        "For machine-learning purposes, the timestamps in your time series must represent fixed time intervals. If necessary,\nyou can use the DATE_TRUNC or TIME_SLICE function on TIMESTAMP columns to remove irregularities when training the\nforecast model.",
        "The following example uses a view with only 30 rows to train an anomaly detection model. Start by generating data into a\ntable, then create a view on the table. The view is not required (you can use a table to train a model), but the view option\ngives you some flexibility to train models iteratively, with different row counts, without updating the source data.",
        "Note",
        "If you run this example yourself, your output will not match exactly because the sensor_data_30_rows table is loaded\nwith randomly generated values.",
        "Now create the model:",
        "When the model has built successfully, call the <model_name>!DETECT_ANOMALIES\nmethod to detect outliers in the specified test data set. The timestamps in the test data must chronologically follow the\ntimestamps in the training data, but there must not be too great a gap in time between the training data and the test data. For\nexample, if you have timestamps for every second, do not use test data that is millions of seconds ahead of the training data.",
        "This example uses another table as the test data, with only three rows. These rows have timestamps that closely follow those\nin the training data.",
        "When the anomaly detection call finishes, it returns output similar to the following:",
        "The TS and Y columns return the timestamps and temperature values from the test data. In this very small test case,\nthe function found an anomaly (IS_ANOMALY=True). For more information about the output columns, see the \u201cReturns\u201d section in the\nfunction description.",
        "If you want to test the examples in this section that query the sensor_data_ts table, you can create and load\na copy of this table by running the following SQL script. The script generates one month of synthetic data for sensor\nreadings by calling the UNIFORM, RANDOM, and GENERATOR functions; therefore, your copy of the table will not return identical\nresults. The readings will be in the same range but they will not be the same.",
        "The following script creates and loads the heavy_weather table, which is used in the examples\nfor the MAX_BY functions. The table contains 55 rows of snowfall precipitation records for\nCalifornia cities during the last week of 2021.",
        "Was this page helpful?",
        "On this page"
    ]
}