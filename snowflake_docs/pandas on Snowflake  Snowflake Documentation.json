{
    "url": "https://docs.snowflake.com/en/developer-guide/snowpark/python/snowpark-pandas#installing-the-snowpark-pandas-api",
    "title": "pandas on Snowflake | Snowflake Documentation",
    "paragraphs": [
        "pandas on Snowflake lets you run your pandas code in a distributed manner directly on your data in Snowflake.\nJust by changing the import statement and a few lines of code, you can get the familiar\npandas experience you know and love with the scalability and security benefits of Snowflake.\nWith pandas on Snowflake, you can work with much larger datasets and avoid the time and expense of porting your pandas\npipelines to other big data frameworks or provisioning large and expensive machines. It runs workloads natively in Snowflake through\ntranspilation to SQL, enabling it to take advantage of parallelization and the data governance and security benefits of Snowflake. pandas on Snowflake is delivered through the Snowpark pandas API as part of the Snowpark Python library, which enables scalable data processing of Python code within the Snowflake platform.",
        "Meeting Python developers where they are \u2013 pandas on Snowflake  offers a familiar interface to Python developers by providing a\npandas-compatible layer that can run natively in Snowflake.",
        "Scalable distributed pandas \u2013 pandas on Snowflake bridges the convenience of pandas with the scalability of Snowflake by leveraging existing query optimization techniques in Snowflake. Minimal code rewrites are required, simplifying the migration journey, so you can seamlessly move from prototype to production.",
        "Security and governance \u2013 Data does not leave Snowflake\u2019s secure platform. pandas on Snowflake  allows uniformity within data\norganizations on how data is accessed, and allows for easier auditing and governance.",
        "No additional compute infrastructure to manage and tune \u2013 pandas on Snowflake leverages the Snowflake\u2019s powerful compute engine, so you do not need to set\nup or manage any additional compute infrastructure.",
        "You should use pandas on Snowflake if any of the following is true:",
        "You are familiar with the pandas API and the broader PyData ecosystem",
        "You work in a team with others who are familiar with pandas and want to collaborate on the same codebase",
        "You have existing code written in pandas",
        "Your workflow has order-related needs, as supported by pandas DataFrames. For example, you need the dataset to be in the same order for the entire workflow",
        "You prefer more accurate code completion from AI-based copilot tools",
        "To install pandas on Snowflake, you can use conda or pip to install the package. For detailed instructions, see Installation.",
        "Once pandas on Snowflake is installed, instead of importing pandas as import pandas as pd, use the following two lines:",
        "Here is an example of how you can start using pandas on Snowflake through the pandas on Snowpark Python library with Modin.",
        "The pandas on Snowflake and DataFrame API is highly interoperable, so you can build a pipeline that leverages both APIs.",
        "You can use the following operations to do conversions between Snowpark DataFrames and Snowpark pandas DataFrames:",
        "Operation",
        "Input",
        "Output",
        "Notes",
        "to_snowpark_pandas",
        "Snowpark DataFrame",
        "Snowpark pandas DataFrame",
        "This operation assigns an implicit order to each row, and maintains this row order during the lifetime of the DataFrame. I/O cost will be incurred in this conversion.",
        "to_snowpark",
        "Snowpark pandas DataFrame or Snowpark pandas Series",
        "Snowpark DataFrame",
        "This operation does not maintain the row ordering, and the resulting Snowpark DataFrame operates on a data snapshot of the source Snowpark pandas DataFrame.\nUnlike Snowpark DataFrames created from the table directly, this behavior means that changes to the underlying table will not be reflected during the\nevaluation of the Snowpark operations. No DDL operations and limited DML operations can be applied on the DataFrame. No I/O cost will be incurred in this conversion.",
        "Whenever possible, we advise using read_snowflake to read the table from Snowflake directly instead of converting it to and from a Snowpark DataFrame to avoid unnecessary conversion costs.",
        "For more information, see Snowpark DataFrames vs Snowpark pandas DataFrame: Which should I choose?.",
        "pandas on Snowflake and native pandas have similar DataFrame APIs with matching signatures and similar semantics.\npandas on Snowflake provides the same API signature as native pandas (pandas 2.2.1) and provides scalable computation with Snowflake.\npandas on Snowflake respects the semantics described in the native pandas documentation as much as possible, but it uses the Snowflake\ncomputation and type system. However, when native pandas executes on a client machine, it uses the Python computation and type system.\nFor information about the type mapping between pandas on Snowflake and Snowflake, see\nData types.",
        "Like native pandas, pandas on Snowflake also has the notion of an index and maintains row ordering. However, their distinct execution environments\ncause certain nuanced differences in their behavior. This section calls out the key differences to be aware of.",
        "pandas on Snowflake is best used with data which is already in Snowflake, but you can use the following operations to convert between native\npandas and pandas on Snowflake:",
        "Operation",
        "Input",
        "Output",
        "Notes",
        "to_pandas",
        "Snowpark pandas DataFrame",
        "native pandas DataFrame",
        "Materialize all data to the local environment. If the dataset is large, this may result in an out of memory error.",
        "pd.DataFrame(\u2026)",
        "Native pandas DataFrame, raw data, Snowpark pandas object",
        "Snowpark pandas DataFrame",
        "This should be reserved for small DataFrames. Creating a DataFrame with large amounts of local data will introduce a temp table and might incur performance issues due to data uploading.",
        "session.write_pandas",
        "Native pandas DataFrame, Snowpark pandas object",
        "Snowflake table",
        "The result can be subsequently loaded into pandas on Snowflake with pd.read_snowflake using the table name specified in the write_pandas call.",
        "pandas: Operates on a single machine and processes data in memory.",
        "pandas on Snowflake: Integrates with Snowflake, which allows for distributed computing across a cluster of machines. This\nintegration enables handling of much larger datasets that exceed the memory capacity of a single machine. Note that using the Snowpark\npandas API requires a connection to Snowflake.",
        "pandas: Executes operations immediately and materializes results fully in memory after each operation. This eager\nevaluation of operations might lead to increased memory pressure as data needs to be moved extensively within a machine.",
        "pandas on Snowflake: Provides the same API experience as pandas. It mimics the eager evaluation model of pandas, but internally\nbuilds a lazily-evaluated query graph to enable optimization across operations.",
        "Fusing and transpiling operations through a query graph enables additional optimization opportunities for the underlying distributed\nSnowflake compute engine, which decreases both cost and end-to-end pipeline runtime compared to running pandas directly within Snowflake.",
        "Note",
        "I/O related APIs and APIs whose return value is not a Snowpark pandas object (i.e. DataFrame, Series or Index) always evaluate eagerly. For example:",
        "read_snowflake",
        "to_snowflake",
        "to_pandas",
        "to_dict",
        "to_list",
        "__repr__",
        "The dunder method, __array__ which can be called automatically by some 3rd party libraries such as scikit-learn.\nCalls to this method will materialize results to the local machine.",
        "pandas: Supports the various readers and writers listed in the pandas documentation in\nIO tools (text, CSV, HDF5, \u2026).",
        "pandas on Snowflake: Can read and write from Snowflake tables and read local or staged CSV, JSON, or parquet files.\nFor more information, see IO (Read and Write).",
        "pandas: Has a rich set of data types, such as integers, floats, strings, datetime types, and categorical types. It also\nsupports user-defined data types. Data types in pandas are typically derived from the underlying data and are enforced strictly.",
        "pandas on Snowflake: Constrained by Snowflake type system, which maps pandas objects to SQL by translating the pandas data types to the SQL types in Snowflake. A majority\nof pandas types have a natural equivalent in Snowflake, but the mapping is not always one to one. In some cases, multiple pandas types\nare mapped to the same SQL type.",
        "The following table lists the type mappings between pandas and Snowflake SQL:",
        "pandas type",
        "Snowflake type",
        "All signed/unsigned integer types, including pandas extended integer types",
        "NUMBER(38, 0)",
        "All float types, including pandas extended float data types",
        "FLOAT",
        "bool, BooleanDtype",
        "BOOLEAN",
        "str, StringDtype",
        "STRING",
        "datetime.time",
        "TIME",
        "datetime.date",
        "DATE",
        "All timezone-naive datetime types",
        "TIMESTAMP_NTZ",
        "All timezone-aware datetime types",
        "TIMESTAMP_TZ",
        "list, tuple, array",
        "ARRAY",
        "dict, json",
        "MAP",
        "Object column with mixed data types",
        "VARIANT",
        "Timedelta64[ns]",
        "NUMBER(38, 0)",
        "Note",
        "Categorical, period, interval, sparse, and user-defined data types are not supported. Timedelta is only supported on the pandas on Snowpark client today. When writing Timedelta back to Snowflake, it will be stored as Number type.",
        "The following table provides the mapping of Snowflake SQL types back to pandas on Snowflake types using df.dtypes:",
        "Snowflake type",
        "pandas on Snowflake type (df.dtypes)",
        "NUMBER (scale = 0)",
        "int64",
        "NUMBER (scale > 0), REAL",
        "float64",
        "BOOLEAN",
        "bool",
        "STRING, TEXT",
        "object (str)",
        "VARIANT, BINARY, GEOMETRY, GEOGRAPHY",
        "object",
        "ARRAY",
        "object (list)",
        "OBJECT",
        "object (dict)",
        "TIME",
        "object (datetime.time)",
        "TIMESTAMP, TIMESTAMP_NTZ, TIMESTAMP_LTZ, TIMESTAMP_TZ",
        "datetime64[ns]",
        "DATE",
        "object (datetime.date)",
        "When converting from the Snowpark pandas DataFrame to native pandas DataFrame with to_pandas(), the native pandas DataFrame will\nhave refined data types compared to the pandas on Snowflake types, which are compatible with the SQL-Python Data Type Mappings for\nfunctions and procedures.",
        "pandas: Relies on NumPy and by default follows the NumPy and Python type system for implicit type casting\nand inference. For example, it treats booleans as integer types, so 1 + True returns 2.",
        "pandas on Snowflake: Maps NumPy and Python types to Snowflake types according to the preceding table, and uses the underlying\nSnowflake type system for implicit type casting and inference. For example, in accordance\nwith the Logical data types, it does not implicitly convert booleans to integer types, so 1 + True results in a\ntype conversion error.",
        "pandas: In pandas versions 1.x, pandas was flexible when\nhandling missing data, so it treated all of\nPython None, np.nan, pd.NaN, pd.NA, and pd.NaT as missing values.\nIn later versions of pandas (2.2.x) these values are treated as different values.",
        "pandas on Snowflake: Adopts a similar approach to earlier pandas versions that treats all of the preceding values listed as missing values.\nSnowpark reuses NaN, NA, and NaT from pandas. But note that all these missing values are treated interchangeably and stored as SQL NULL in the Snowflake table.",
        "pandas: Date offsets in pandas changed in version 2.2.1. The single-letter aliases 'M', 'Q', 'Y', and others have been deprecated in favor of two-letter offsets.",
        "pandas on Snowflake: Exclusively uses the new offsets described in the pandas time series documentation.",
        "Prerequisites: Python 3.9, 3.10 or 3.11, modin version 0.28.1, and pandas version 2.2.1 are required.",
        "Tip",
        "To use pandas on Snowflake in Snowflake Notebooks, see the setup instructions in pandas on Snowflake in notebooks.",
        "To install pandas on Snowflake  in your development environment, follow these steps:",
        "Change to your project directory and activate your Python virtual environment.",
        "Note",
        "The API is under active development, so we recommend installing it in a Python virtual environment rather than\nsystem-wide. This practice allows each project you create to use a specific version, insulating you from changes\nin future versions.",
        "You can create a Python virtual environment for a particular Python version using tools like\nAnaconda,\nMiniconda, or\nvirtualenv.",
        "For example, to use conda to create a Python 3.9 virtual environment, type:",
        "Note",
        "If you previously installed an older version of pandas on Snowflake using Python 3.8 and pandas 1.5.3, you will need to upgrade your Python and pandas\nversions as described above. Follow the steps to create a new environment with Python 3.9, 3.10, or 3.11.",
        "Install the Snowpark Python library with Modin.",
        "or",
        "Note",
        "Make sure snowflake-snowpark-python version 1.17.0 or later is installed.",
        "Before using pandas on Snowflake , you must establish a session with the Snowflake database.\nYou can use a config file to choose the connection parameters for your session or you can enumerate them in your code.\nFor more information, see Creating a Session for Snowpark Python.\nIf a unique active Snowpark Python session exists, pandas on Snowflake  will automatically use it. For example:",
        "The pd.session is a Snowpark session, so you can do anything with it that you can do with any other Snowpark session. For example, you can use it to execute an arbitrary SQL query,\nwhich results in a Snowpark DataFrame as per the Session API, but take note that\nthe results of this will be a Snowpark DataFrame, not a Snowpark pandas DataFrame.",
        "Alternatively, you can configure your Snowpark connection parameters in a configuration file.\nThis eliminates the need to enumerate connection parameters in your code, allowing you to write your pandas on Snowflake code almost as you would normally write pandas code.\nTo achieve this, create a configuration file located at ~/.snowflake/connections.toml that looks something like this:",
        "Then in the code, you only need to use snowflake.snowpark.Session.builder.create() to create a session using these credentials.",
        "You can also create multiple Snowpark sessions, then assign one of them to pandas on Snowflake. pandas on Snowflake only uses one session, so you have to explicitly assign one\nof the sessions to pandas on Snowflake with pd.session = pandas_session.",
        "The following example shows that trying to use pandas on Snowflake when there is no active Snowpark session will raise a SnowparkSessionException with an\nerror like \u201cpandas on Snowflake requires an active snowpark session, but there is none.\u201d Once you create a session, you can use pandas on Snowflake. For example:",
        "The following example shows that trying to use pandas on Snowflake when there are multiple active Snowpark sessions will cause\na SnowparkSessionException with a message like, \u201cThere are multiple active snowpark sessions, but you need to choose one for pandas on Snowflake.\u201d",
        "Note",
        "You must set the session used for a new Snowpark pandas DataFrame or Series via modin.pandas.session.\nHowever, joining or merging DataFrames created with different sessions is not supported, so you should avoid repeatedly setting different sessions\nand creating DataFrames with different sessions in a workflow.",
        "See the pandas on Snowflake API reference for the full list of currently implemented APIs and methods available.",
        "For a full list of supported operations, see the following tables in pandas on Snowflake reference:",
        "pandas general utilities supported APIs",
        "Series supported APIs",
        "DataFrame supported APIs",
        "Index supported APIs",
        "Windows supported APIs",
        "GroupBy supported APIs",
        "Resampler supported APIs",
        "DatetimeProperties supported APIs",
        "StringMethods supported APIs",
        "To use pandas on Snowflake in Snowflake notebooks, see pandas on Snowflake in notebooks.",
        "To use Snowpark pandas, you need to install Modin by selecting modin from Packages in the Python Worksheet environment.",
        "You can select the Return type of the Python function under Settings > Return type. By default, this is set as a Snowpark table. To display the Snowpark pandas DataFrame as a result, you can convert a Snowpark pandas DataFrame to a Snowpark DataFrame by calling to_snowpark(). No I/O cost will be incurred in this conversion.",
        "Here is an example of using Snowpark pandas with Python Worksheets:",
        "You can use pandas on Snowflake in a stored procedure to build a data pipeline and schedule the execution of the stored procedure with tasks.",
        "To call the stored procedure, you can run dt_pipeline_sproc() in Python or CALL run_data_transformation_pipeline_sp() in SQL.",
        "To schedule the stored procedure as a task, you can use the Snowflake Python API to create a task.",
        "When calling third-party library APIs with a Snowpark pandas DataFrame, we recommend converting the Snowpark pandas DataFrame to a pandas DataFrame by calling to_pandas() before passing the DataFrame to the third-party library call.",
        "Note",
        "Calling to_pandas() pulls your data out of Snowflake and into memory, so keep that in mind for large datasets and sensitive use cases.",
        "pandas on Snowflake currently has limited compatibility for certain NumPy and Matplotlib APIs, such as distributed implementation for np.where and interoperability with df.plot. Converting Snowpark pandas DataFrames via to_pandas() when working with these third-party libraries will avoid multiple I/O calls.",
        "Here is an example with Altair for visualization and scikit-learn for machine learning.",
        "We can now use scikit-learn to train a simple model after converting to pandas.",
        "pandas on Snowflake has the following limitations:",
        "pandas on Snowflake provides no guarantee of compatibility with OSS third-party libraries. Starting with version 1.14.0a1, however, Snowpark\npandas introduces limited compatibility for NumPy, specifically for np.where usage. For more information, see\nNumPy Interoperability.",
        "When calling third-party library APIs with a Snowpark pandas DataFrame,\nSnowflake recommends that you convert the Snowpark pandas DataFrame to a pandas DataFrame by calling to_pandas() before passing the DataFrame to\nthe third-party library call. For more information, see Using pandas on Snowflake with third-party libraries.",
        "pandas on Snowflake is not integrated with Snowpark ML. When using Snowpark ML, we recommend that you convert the Snowpark\npandas DataFrame to a Snowpark DataFrame using to_snowpark() before calling Snowpark ML.",
        "Lazy MultiIndex objects are not supported. When MultiIndex is used, it returns a native pandas MultiIndex object,\nwhich requires pulling all data to the client side.",
        "Not all pandas APIs have a distributed implementation yet in pandas on Snowflake. For unsupported APIs,\nNotImplementedError is thrown. Operations that have no distributed implementation fall back to a stored procedure.\nFor information about supported APIs, refer to the API reference documentation.",
        "pandas on Snowflake requires a specific pandas version. pandas on Snowflake requires pandas 2.2.1, and only provides compatibility with pandas 2.2.1.",
        "pandas on Snowflake cannot be referenced within the pandas on Snowflake apply() function. You can only use native pandas inside apply().",
        "This section describes troubleshooting tips when using pandas on Snowflake.",
        "When troubleshooting, try running the same operation on a native pandas DataFrame (or a sample) to see if the same error persists. This approach\nmight provide hints on how to fix your query. For example:",
        "If you have a long-running notebook opened, note that by default Snowflake sessions timeout after the session is idle for 240 minutes (4 hours). When the session expires,\nyou will get the following error if you run additional pandas on Snowflake queries: \u201cAuthentication token has expired. The user must authenticate again.\u201d At this point, you\nmust re-establish the connection to Snowflake again. This may result in loss of any unpersisted session variables.\nFor more information about how to configure the session idle timeout parameter, see Session Policies.",
        "This section describes best practices to follow when using pandas on Snowflake.",
        "Avoid using iterative code patterns, such as for loops, iterrows, and iteritems. Iterative code patterns quickly increase\nthe generated query complexity. Let pandas on Snowflake perform the data distribution and computation parallelization\nrather than the client code. When it comes to iterative code patterns, try to look for operations that can be performed on the\nwhole DataFrame and use the corresponding operations instead.",
        "Avoid calling apply, applymap and transform, which are eventually implemented with\nUDFs or\nUDTFs, which might not be as performant as\nregular SQL queries. If the function applied has an equivalent DataFrame or series operation, use that operation instead.\nFor example, instead of df.groupby('col1').apply('sum'), directly call df.groupby('col1').sum().",
        "Call to_pandas() before passing the DataFrame or series to a third-party library call. pandas on Snowflake does not provide a\ncompatibility guarantee with third-party libraries.",
        "Use a materialized regular Snowflake table to avoid extra I/O overhead. pandas on Snowflake works on top of a data snapshot that\nonly works for regular tables. For other types, including external tables, views, and Apache Iceberg\u2122 tables, a temporary table is\ncreated before taking the snapshot, which introduces extra materialization overhead.",
        "pandas on Snowflake provides fast and zero copy clone capability while creating DataFrames from Snowflake tables using read_snowflake. However, the snapshot capability is only provided for regular Snowflake tables under normal databases. Extra materialization to regular Snowflake tables will be introduced when loading tables with types like hybrid, iceberg etc., or tables under shared databases. The snapshot is required to provide data consistency and ordering guarantee, and there is currently no other way to work around the extra materialization, please try to use normal Snowflake tables as much as possible when using pandas on Snowflake.",
        "Double check the result type before proceeding to other operations, and do explicit type casting with astype if needed.",
        "Due to limited type inference capability, if no type hint is given, df.apply will return results of object (variant) type even if the result contains all\ninteger values. If other operations require the dtype to be int, you can do an explicit type casting by calling the\nastype method to correct the column type before you continue.",
        "Avoid calling APIs that require evaluation and materialization if not necessary.",
        "APIs that don\u2019t return Series or Dataframe require eager evaluation and materialization to produce the result in the\ncorrect type. Same for plotting methods. Reduce calls to those APIs to minimize unnecessary evaluations and materialization.",
        "Avoid calling np.where(<cond>, <scalar>, n) on large datasets. The <scalar> will be broadcast to a DataFrame\nthe size of <cond>, which may be slow.",
        "When working with iteratively built queries, df.cache_result can be used to materialize intermediate\nresults to reduce the repeated evaluation and improve the latency and reduce complexity of the overall query. For example:",
        "In the example above, the query to produce df2 is expensive to compute, and is reused in the creation of both df3 and df4.\nMaterializing df2 into a temporary table (making subsequent operations involving df2 a table scan instead of a pivot) can reduce the\noverall latency of the code block:",
        "Here is a code example with pandas operations. We start with a Snowpark pandas DataFrame named pandas_test, which contains three\ncolumns: COL_STR, COL_FLOAT, and COL_INT. To view the notebook associated with these examples, see the\npandas on Snowflake  examples in the Snowflake-Labs repository.",
        "We save the DataFrame as a Snowflake table named pandas_test which we will use throughout our examples.",
        "Next, we create a DataFrame from the Snowflake table. We drop the column COL_INT and then\nsave the result back to Snowflake with a column named row_position.",
        "You end up with a new table, pandas_test2, which looks like this:",
        "For more information, see Input/Output.",
        "For more information, see GroupBy.",
        "Quickstart: Getting Started with pandas on Snowflake",
        "Quickstart: Data Engineering Pipelines with Snowpark Python",
        "Was this page helpful?",
        "On this page"
    ]
}