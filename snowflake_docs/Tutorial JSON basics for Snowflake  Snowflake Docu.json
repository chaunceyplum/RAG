{
    "url": "https://docs.snowflake.com/en/user-guide/tutorials/json-basics-tutorial",
    "title": "Tutorial: JSON basics for Snowflake | Snowflake Documentation",
    "paragraphs": [
        "In this tutorial you will learn the basics of using JSON with Snowflake.",
        "In this tutorial, you learn how to do the following:",
        "Upload sample JSON data from a public S3 bucket into a column of the variant type in a\nSnowflake table.",
        "Test simple queries for JSON data in the table.",
        "Explore the FLATTEN function to flatten JSON data into a relational representation and save it\nin another table.",
        "Explore ways to ensure uniqueness as you insert rows in the flattened version of the data.",
        "The tutorial assumes the following:",
        "You have a Snowflake account that is configured to use Amazon AWS and a user with\na role that grants the necessary privileges to create a database, tables, and\nvirtual warehouse objects.",
        "You have SnowSQL (CLI client) installed.",
        "The Snowflake in 20 minutes tutorial provides the related\nstep-by-step instructions to meet these requirements.",
        "Snowflake provides sample data files in a public S3 bucket for use in this tutorial.\nBut before you start, you need to create a database, tables, a virtual warehouse,\nand an external stage for this tutorial. These are the basic Snowflake objects\nneeded for most Snowflake activities.",
        "For this tutorial, you use the following sample application events JSON data provided in a public S3 bucket.",
        "The data represents sample events that applications upload to S3. A variety of devices and applications, such as servers, cell phones, and browsers publish events. In a common data\ncollection scenario, a scalable web endpoint collects POSTed data from different sources and writes them to a queuing\nsystem. An ingest service/utility then writes the data to a S3\nbucket, from which you can load the data into Snowflake.",
        "The sample data illustrates the following concepts:",
        "Applications can choose to group events in batches. A batch is a container\nthat holds header information common to all of the events in the batch. For example, the preceding JSON is a batch of two\nevents with common header information: device_type and version that generated these events.",
        "Amazon S3 supports using folders concept to organize a bucket. Applications can leverage this feature to partition event data.\nPartitioning schemes typically identify details, such as application or location that generated the event, along with\nan event date when it was written to S3. Such a partitioning scheme enables you to copy any fraction of the partitioned\ndata to Snowflake with a single COPY command. For example, you can copy event data by the hour, data, month, or year\nwhen you initially populate tables.",
        "For example:",
        "s3://bucket_name/application_a/2016/07/01/11/",
        "s3://bucket_name/application_b/location_c/2016/07/01/14/",
        "Note the application_a, application_b, location_c, etc. identify details for the source\nof all data in the path. The data can be organized by the date when it was written.\nAn optional 24-hour directory reduces the amount of data in each directory.",
        "Note",
        "S3 transmits a directory list with each COPY statement used by Snowflake, so reducing\nthe number of files in each directory improves the performance of your COPY statements.\nYou may even consider creating 10-15 minute increment folders in each hour.",
        "The sample data provided in the S3 bucket uses a similar partitioning scheme. In a COPY command you\nwill specify a specific folder path to copy events data.",
        "Execute the following statements to create a database, a table, a virtual warehouse,\nand an external stage needed for this tutorial. After you complete the tutorial,\nyou can drop these objects.",
        "Note the following:",
        "The CREATE DATABASE statement creates a database. The database automatically\nincludes a schema named \u2018public\u2019.",
        "The USE SCHEMA statement specifies an active database and schema for the current user session.\nSpecifying a database now enables you to perform your work in this database without having\nto provide the name each time it is requested.",
        "The CREATE TABLE statement creates a target table for JSON data.",
        "The CREATE WAREHOUSE statement creates an initially suspended warehouse. The\nstatement also sets AUTO_RESUME = true, which starts the warehouse automatically\nwhen you execute SQL statements that require compute resources.\nThe USE WAREHOUSE statement specifies the warehouse you created as the active\nwarehouse for the current user session.",
        "The CREATE STAGE statement creates an external stage that points to the S3 bucket\ncontaining the sample file for this tutorial.",
        "Execute COPY INTO <table> to load your staged data into\nthe target RAW_SOURCE table.",
        "The command copies all new data from the specified path on the external stage\nto the target RAW_SOURCE table. In this example, the specified path targets data\nwritten on the 15th hour (3 PM) of July 15th, 2016.\nNote that Snowflake checks each file\u2019s S3 ETag value to ensure it is\ncopied only once.",
        "Execute a SELECT query to verify the data is copied successfully.",
        "The query returns the following result:",
        "In this sample JSON data, there are two events.  The device_type,\nand version key values identify a data source and version for\nevents from a specific device.",
        "In this section, you explore SELECT statements to query the JSON data.",
        "Retrieve device_type.",
        "The query return the following result:",
        "The query uses the src:device_type notation\nto specify the column name and the JSON element name to retrieve. This\nnotation is similar to the\nfamiliar SQL table.column notation.\nSnowflake allows you to specify a\nsub-column within a parent column, which Snowflake dynamically derives from the\nschema definition embedded in the JSON data. For more information,\nrefer to Querying Semi-structured Data.",
        "Note",
        "The column name is case-insensitive, however JSON element names\nare case-sensitive.",
        "Retrieve the device_type value without the quotes.",
        "The preceding query returns the JSON data value in quote. You can remove\nthe quotes by casting the data to a specific data type,\nin this example a string.",
        "This query also optionally assigns a name to the column using an alias.",
        "The query retuns the following result:",
        "Retrieve repeating f keys nested within the array event objects.",
        "The sample JSON data includes events array. Each event object in the array\nhas the f field as shown.",
        "To retrieve these nested keys, you can use the FLATTEN\nfunction. The function flattens the events into separate rows.",
        "The query returns the following result:",
        "Note the value is one of the columns that FLATTEN function returns.\nThe next step provides more details about using the FLATTEN function.",
        "FLATTEN is a table function that produces a lateral\nview of a VARIANT, OBJECT, or ARRAY column. In this step, you use this function\nto explore different levels of flattening.",
        "You can flatten the event objects in the events array into separate rows\nusing the FLATTEN function. The function output includes a\nVALUE column that stores these individual events.",
        "You can then use the LATERAL modifier to join the FLATTEN function output\nwith any information outside of the object \u2014 in this example,\nthe device_type and version.",
        "Query the data for each event:",
        "The query returns the following result:",
        "Use a CREATE TABLE AS SELECT statement to store the preceding query result in a table:",
        "Query the resulting table.",
        "The query returns the following result:",
        "In the preceding example, you flattened the event objects in the events array\ninto separate rows. The resulting flattened_source table retained the event structure\nin the src column of the VARIANT type.",
        "One benefit of retaining the\nevent objects in the src column of the VARIANT type is that when event format changes,\nyou don\u2019t have to recreate and repopulate such tables. But you also have the option to\ncopy individual keys in the event object into separate typed columns as shown\nin the following query.",
        "The following CREATE TABLE AS SELECT statement creates a new table named events with the event\nobject keys stored in separate columns. Each value is cast to a data type that is appropriate\nfor the value, using a double-colon (::) followed by the type. If you omit the casting,\nthe column assumes the VARIANT data type, which can hold any value:",
        "The statement flattens the nested data in the EVENTS.SRC:V key, adding a separate column for each value.\nThe statement outputs a row for each key/value pair. The following output shows the first two records in the new events table:",
        "So far in this tutorial, you did the following:",
        "Copied sample JSON event data from an S3 bucket into the RAW_SOURCE table\nand explored simple queries.",
        "You also explored the FLATTEN function to flatten the JSON data and obtain a relational\nrepresentation of the data. For example, you extracted event keys and stored the keys\nin separate columns in another EVENTS table.",
        "At the beginning, the tutorial explains the application scenario where multiple sources generate\nevents and a web endpoint saves it to your S3 bucket. As new events are added to the S3 bucket,\nyou might use a script to continuously copy new data into the RAW_SOURCE table.\nBut how do insert only new event data into the EVENTS table.",
        "There are numerous ways to maintain data consistency. This section explains two options.",
        "In this section you add a primary key to the EVENTS table. The primary key then guarantees uniqueness.",
        "Examine your JSON data for any values that are naturally unique and would be good\ncandidates for a primary key. For example, assume that the combination of\nsrc:device_type and value:rv can be a primary key. These two JSON keys\ncorrespond to the DEVICE_TYPE and RV columns in the EVENTS table.",
        "Note",
        "Snowflake does not enforce the primary key constraint. Rather, the constraint\nserves as metadata that identifies the natural key in the Information Schema.",
        "Add the primary key constraint to the EVENTS table:",
        "Insert a new JSON event record into the RAW_SOURCE table:",
        "Insert the new record that you added to the RAW_SOURCE table\ninto the EVENTS table based on a comparison of the primary key values:",
        "Querying the EVENTS table shows the added row:",
        "The query returns the following result:",
        "If the JSON data does not have fields that can be primary key candidates, you\ncould compare all repeating JSON keys in the RAW_SOURCE table with the\ncorresponding column values in the EVENTS table.",
        "No changes to your existing EVENTS table are required.",
        "Insert a new JSON event record into the RAW_SOURCE table:",
        "Insert the new record in the RAW_SOURCE table into the EVENTS table based on a comparison of all repeating key values:",
        "Querying the EVENTS table shows the added row:",
        "The query returns the following result:",
        "Congratulations, you have successfully completed the tutorial.",
        "Partitioning the event data in your S3 bucket using logical, granular paths allows you to copy a subset of the partitioned data into Snowflake with a single command.",
        "Snowflake\u2019s column:key notation, similar to the familiar SQL table.column notation,\nallows you to effectively query a column within the column (i.e., a sub-column), which is\ndynamically derived based on the schema definition embedded in the JSON data.",
        "The FLATTEN function allows you to parse JSON data into separate columns.",
        "Several options are available to update table data based on comparisons with staged data files.",
        "Execute the following DROP <object> commands to return your system to its state before you began the tutorial:",
        "Dropping the database automatically removes all child database objects such as tables.",
        "Was this page helpful?"
    ]
}