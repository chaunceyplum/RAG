{
    "url": "https://docs.snowflake.com/en/developer-guide/snowflake-ml/dataset",
    "title": "Snowflake Datasets | Snowflake Documentation",
    "paragraphs": [
        "Preview Feature \u2014 Open",
        "Snowflake Datasets are available in the Snowpark ML Python package (snowflake-ml-python) v1.5.0 and later.",
        "Datasets are new Snowflake schema-level objects specially designed for machine learning workflows. Snowflake Datasets\nhold collections of data organized into versions, where each version holds a materialized snapshot of your data with\nguaranteed immutability, efficient data access, and interoperability with popular deep learning frameworks.",
        "Note",
        "Although Datasets are SQL objects, they are intended for use exclusively with Snowpark ML. They do not\nappear in the Snowsight database object explorer, and you do not use SQL commands to work with them.",
        "You should use Snowflake Datasets in these situations:",
        "You need to manage and version large datasets for reproducible machine learning model training and testing.",
        "You want to leverage Snowflake\u2019s scalable and secure data storage and processing capabilities.",
        "You need fine-grained file-level access and/or data shuffling for distributed training or data streaming.",
        "You need to integrate with external machine learning frameworks and tools.",
        "Note",
        "Materialized datasets incur storage costs. To minimize these costs, delete unused datasets.",
        "The Dataset Python SDK is included in Snowpark ML (Python package snowflake-ml-python) starting in version 1.5.0.\nFor installation instructions, see Using Snowflake ML Locally.",
        "Creating Datasets requires the CREATE DATASET schema-level privilege. Modifying Datasets, for example adding or deleting\ndataset versions, requires OWNERSHIP on the Dataset. Reading from a Dataset requires only the USAGE privilege on the\nDataset (or OWNERSHIP). For more information about granting privileges in Snowflake, see GRANT <privileges>.",
        "Tip",
        "Setting up privileges for the Snowflake Feature Store using either the setup_feature_store method or the\nprivilege setup SQL script also sets up Dataset privileges.\nIf you have already set up feature store privileges by one of these methods, no further action is needed.",
        "Datasets are created by passing a Snowpark DataFrame to the snowflake.ml.dataset.create_from_dataframe function.",
        "Datasets are versioned. Each version is an immutable, point-in-time snapshot of the data managed by the Dataset. The\nPython API includes a Dataset.selected_version property that indicates whether a given dataset is selected for use.\nThis property is automatically set by the dataset.create_from_dataframe and dataset.load_dataset factory\nmethods, so creating a dataset automatically selects the created version. The Dataset.select_version and\nDataset.create_version methods can also be used to explicitly switch between versions. Reading from a Dataset\nreads from the active selected version.",
        "Dataset version data is stored as evenly sized files in the Apache Parquet format. The Dataset class provides an API\nsimilar to that of FileSet for reading data from Snowflake\nDatasets, including built-in connectors for TensorFlow and PyTorch. The API is extensible to support custom framework\nconnectors.",
        "Reading from a Dataset requires an active selected version.",
        "Datasets can be converted to TensorFlow\u2019s tf.data.Dataset and streamed in batches for efficient training and evaluation.",
        "Datasets also support conversion to PyTorch DataPipes and can be streamed in batches for efficient training and\nevaluation.",
        "Datasets can also be converted back to Snowpark DataFrames for integration with Snowpark ML Modeling. The converted\nSnowpark DataFrame is not the same as the DataFrame that was provided during Dataset creation, but instead points to the\nmaterialized data in the Dataset version.",
        "The Dataset API also exposes an fsspec interface, which can be\nused to build custom integrations with external libraries like PyArrow, Dask, or any other package that supports\nfsspec and allows distributed and/or stream-based model training.",
        "Dataset names are SQL identifiers and subject to Snowflake identifier requirements.",
        "Dataset versions are strings and have a maximum length of 128 characters. Some characters are not permitted and will\nproduce an error message.",
        "Certain query operations on Datasets with wide schemas (more than about 4,000 columns) are not fully optimized. This\nshould improve in upcoming releases.",
        "Was this page helpful?",
        "On this page",
        "Related content"
    ]
}