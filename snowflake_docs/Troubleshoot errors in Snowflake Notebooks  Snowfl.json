{
    "url": "https://docs.snowflake.com/en/user-guide/ui-snowsight/notebooks-troubleshoot",
    "title": "Troubleshoot errors in Snowflake Notebooks | Snowflake Documentation",
    "paragraphs": [
        "Feature \u2014 Generally Available",
        "Generally available in Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) commercial regions.",
        "The following scenarios can help you troubleshoot issues that can occur when using Snowflake Notebooks.",
        "The following error occurs when the total number of notebooks in your account exceeds 6,000 and you refresh the Notebooks list:",
        "Users can still create new notebooks; however, Snowflake recommends that you remove notebooks that are no longer being used by the account.",
        "Snowflake has deprecated the older snowflake-ml package, which is no longer supported. It has been removed from the package selector and is\nnot available in the Snowflake Anaconda channel. If you are using snowflake-ml and try to add, remove, or update packages in your\nnotebooks, those notebooks will fail because snowflake-ml is no longer accessible.",
        "To avoid issues, switch to snowflake-ml-python, which is the correct package for Snowflake ML.",
        "Plotly will switch to webgl if there are more than 1,000 datapoints.",
        "The following error occurs when a cell is renamed to the same name as an existing variable in the notebook:",
        "For example, you have the following in a Python cell called cell1:",
        "If you then rename cell2 to \u201csession\u201d, and reference \u201csession\u201d in cell3, Notebooks attempts to reference \u201csession\u201d (the cell\nname) and not the Snowpark session, causing an error.",
        "The notebook session runs as a stored procedure. If your notebook is unexpectedly disconnecting before the 1 hour timeout, your\nACCOUNTADMIN or the warehouse owner could have set the STATEMENT_TIMEOUT_IN_SECONDS parameter to a particular value (for example, 5\nmins), which limits how long all statements can run on the warehouse, including notebook sessions. This parameter is set at the warehouse\nor account level and when it is set for both a warehouse and a session, the lowest non-zero value is enforced.",
        "For details on the idle time setting, see Idle time and reconnection.",
        "If you do not have cookies enabled on your browser, you cannot automatically reconnect to the notebook session while it should still be\nactive (before timing out due to inactivity). When you reopen the notebook, an error message displays:",
        "Restarting the session will end the current EXECUTE NOTEBOOK query and start a new session. Ending the session\nwill end the current EXECUTE NOTEBOOK query.",
        "If you do not take either action, the current EXECUTE NOTEBOOK query will continue running on the warehouse,\nshown in Query History.",
        "The following popup occurs when you try to start your notebook:",
        "Ensure that *.snowflake.app is on the allowlist in your network, including content filtering systems, and can connect to Snowflake.\nWhen this domain is on the allowlist, your apps can communicate with Snowflake servers without any restrictions.",
        "In addition, to prevent any issues connecting to the Snowflake backend, ensure that WebSockets are not blocked in your network configuration.",
        "The following message occurs in a cell output if you\u2019re trying to use a package that is not installed in your notebook environment:",
        "Import the necessary package by following the instructions on the Import Python packages to use in notebooks page.",
        "New versions of notebooks are continually being released and notebooks are auto-upgraded to the latest version.\nSometimes, when upgrading an old notebook, the packages in the notebook environment aren\u2019t compatible with the upgrade.\nThis could possibly cause the notebook to fail to start.",
        "The following is an example of an error message when the Libpython package is missing:",
        "To resolve this error, try the following steps:",
        "Refresh the webpage and start the notebook again.",
        "If the issue persists, open the package selector and check whether all installed packages are valid. In the drop-down for each package, you\ncan see the available versions. Selecting the latest version of the package usually clears the error.",
        "Some Python libraries download or cache data to a local user directory. However, the default user directory /home/udf is read-only.\nTo work around this, set the path as /tmp which is a writable location.\nNote that the environment variable used to set the write directory may vary depending on which library you are using.\nThe following is a list of known libraries that present this issue:",
        "matplotlib",
        "HuggingFace",
        "catboost",
        "You might see this warning when using matplotlib:",
        "Resolve this warning using this code, which sets the MPLCONFIGDIR variable to /tmp/:",
        "You might see this warning when using Huggingface:",
        "The following code sets the HF_HOME and SENTENCE_TRANSFORMERS_HOME variables to /tmp/ to get rid of this error:",
        "The following message is displayed in the cell output when you run df.collect():",
        "Snowflake Notebooks automatically truncates results in the cell output for large datasets in following cases:",
        "All SQL cell results.",
        "Python cell results if it\u2019s a snowpark.Dataframe.",
        "The issue with the above cell is that df.collect() returns a List instead of snowpark.Dataframe. Lists are not automatically\ntruncated. To get around this issue, directly output the results of the DataFrame.",
        "When running df.to_pandas(), all the data is loaded into memory and may result in the Notebook session terminating if the data size\nexceeds the associated Notebook warehouse\u2019s memory limit.",
        "The following example shows how you can rewrite the code to read in the table with Snowpark pandas.",
        "If you have the following code in a SQL cell called cell1, the output result is 500M rows.",
        "Then, when you fetch the results into a pandas DataFrame, the notebook crashes because the data is too large to fit in memory:",
        "In general, for large datasets, Snowflake recommends that you avoid using df.to_pandas(). Instead, to operate on your data with pandas, use\nthe Snowpark pandas API and a Snowpark-optimized warehouse. The\nSnowpark pandas API lets you run your\npandas code directly on your data in Snowflake with the query performed in SQL. This allows you to run pandas code on data that does not\nfit in the notebook\u2019s memory.",
        "In the second cell referencing example above, you can convert your SQL cell result to a Snowpark DataFrame first. Then, you can convert it\ninto Snowpark pandas.",
        "For more details, see pandas on Snowflake in notebooks.",
        "If your VPN is configured to use split tunneling, you must add both *.snowflake.com and *.snowflake.app to your network\npolicy allowlist.",
        "Was this page helpful?",
        "On this page"
    ]
}