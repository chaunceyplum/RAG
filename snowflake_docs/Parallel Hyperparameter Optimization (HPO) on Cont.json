{
    "url": "https://docs.snowflake.com/en/developer-guide/snowflake-ml/container-hpo",
    "title": "Parallel Hyperparameter Optimization (HPO) on Container Runtime for ML | Snowflake Documentation",
    "paragraphs": [
        "Preview Feature \u2014 Open",
        "Available to all accounts.",
        "The Snowflake ML Hyperparameter Optimization (HPO) API is a model-agnostic framework that enables efficient,\nparallelized hyperparameter tuning of models using popular tuning algorithms.",
        "Today, this API is available for use within a Snowflake Notebook configured to use the Container Runtime on Snowpark\nContainer Services (SPCS). After you create such a notebook, you can:",
        "Train a model using any open source package, and use this API to distribute the hyperparameter tuning process",
        "Train a model using Snowflake ML distributed training APIs, and scale HPO while also scaling each of the training runs",
        "The HPO workload, initiated from the Notebook, executes inside Snowpark Container Services on either CPU or GPU\ninstances, and scales out to the cores (CPUs or GPUs) available on a single node in the SPCS compute pool.",
        "The parallelized HPO API provides the following benefits:",
        "A single API that automatically handles all the complexities of distributing the training across multiple resources",
        "The ability to train with virtually any framework or algorithm using open-source ML frameworks or the Snowflake ML\nmodeling APIs",
        "A selection of tuning and sampling options, including Bayesian and random search algorithms along with various\ncontinuous and non-continuous sampling functions",
        "Tight integration with the rest of Snowflake; for example efficient data ingestion via Snowflake Datasets or Dataframes\nand automatic ML lineage capture",
        "This example illustrates a typical HPO use case by first ingesting data from a Snowflake table through the Container Runtime\nDataConnector API, then defining a training function that creates an XGBoost model. The Tuner interface provides the tuning\nfunctionality, based on the given training function and search space.",
        "The expected output looks similar to this:",
        "The tuner_results object contains all results, the best model, and the best model path.",
        "The HPO API is in the snowflake.ml.modeling.tune namespace. The main HPO API is the tune.Tuner class. When\ninstantiating this class, you specify the following:",
        "A training function that fits a model",
        "A search space (tune.SearchSpace) that defines the method of sampling hyperparameters",
        "A tuner configuration object (tune.TunerConfig) that defines the search algorithm, the metric to optimize, and the number of trials",
        "After instantiating Tuner, call its run method with a dataset map (which specifies a DataConnector\nfor each input dataset) to start the tuning process.",
        "For more information, execute the following Python statements to retrieve documentation on each class:",
        "Bayesian optimization works only with the uniform sampling function. Bayesian optimization relies on Gaussian processes\nas surrogate models, and therefore requires continuous search spaces. It is incompatible with discrete parameters\nsampled using the tune.randint or tune.choice methods. To work around this limitation, either use\ntune.uniform and cast the parameter inside the training function, or switch to a sampling algorithm that handles\nboth discrete and continuous spaces, such as tune.RandomSearch.",
        "Error message",
        "Possible causes",
        "Possible solutions",
        "Invalid search space configuration: BayesOpt requires all sampling functions to be of type \u2018Uniform\u2019.",
        "Bayesian optimization works only with uniform sampling, not with discrete samples. (See Limitations above.)",
        "Use tune.uniform and cast the result in your training function.",
        "Switch to RandomSearch algorithm, which accepts both discrete and non-discrete samples.",
        "Insufficient CPU resources. Required: 16, Available: 8. May refer to CPU or GPU. The numbers of required and available resources may differ.",
        "max_concurrent_trials is set to a value higher than the available cores.",
        "The full error message describes several options you can try.",
        "Was this page helpful?",
        "On this page"
    ]
}