{
    "url": "https://docs.snowflake.com/en/sql-reference/functions/try_complete-snowflake-cortex",
    "title": "TRY_COMPLETE (SNOWFLAKE.CORTEX) | Snowflake Documentation",
    "paragraphs": [
        "String & binary functions (Large Language Model)",
        "Performs the same operation as the COMPLETE function\nbut returns NULL instead of raising an error when the operation cannot be performed. For error conditions that\nreturns NULL, see Error conditions.",
        "Required:",
        "A string specifying the model to be used. Specify one of the following values.",
        "gemma-7b",
        "jamba-1.5-mini",
        "jamba-1.5-large",
        "jamba-instruct",
        "llama2-70b-chat",
        "llama3-8b",
        "llama3-70b",
        "llama3.1-8b",
        "llama3.1-70b",
        "llama3.1-405b",
        "llama3.2-1b",
        "llama3.2-3b",
        "mistral-large",
        "mistral-large2",
        "mistral-7b",
        "mixtral-8x7b",
        "reka-core",
        "reka-flash",
        "snowflake-arctic",
        "Supported models might have different costs.",
        "The prompt or conversation history to be used to generate a completion.",
        "If options is not present, the prompt given must be a string.",
        "If options is present, the argument must be an array of objects representing a\nconversation in chronological order. Each object must contain a role key and a\ncontent key. The content value is a prompt or a response, depending on the role. The role must be one of the\nfollowing.",
        "role value",
        "content value",
        "'system'",
        "An initial plain-English prompt to the language model to provide it with background information and\ninstructions for a response style. For example, \u201cRespond in the style of a pirate.\u201d The model does not\ngenerate a response to a system prompt. Only one system prompt may be provided, and if it is present,\nit must be the first in the array.",
        "'user'",
        "A prompt provided by the user. Must follow the system prompt (if there is one) or an assistant response.",
        "'assistant'",
        "A response previously provided by the language model. Must follow a user prompt. Past responses can be used to\nprovide a stateful conversational experience; see Usage Notes.",
        "Optional:",
        "An object containing zero or more of the following options that affect the model\u2019s\nhyperparameters. See LLM Settings.",
        "temperature: A value from 0 to 1 (inclusive) that controls the randomness of the output of the language model. A\nhigher temperature (for example, 0.7) results in more diverse and random output, while a lower temperature (such as\n0.2) makes the output more deterministic and focused.",
        "Default: 0",
        "top_p: A value from 0 to 1 (inclusive) that controls the randomness and diversity of the language model,\ngenerally used as an alternative to temperature. The difference is that top_p restricts the set of possible tokens\nthat the model outputs, while temperature influences which tokens are chosen at each step.",
        "Default: 0",
        "max_tokens: Sets the maximum number of output tokens in the response. Small values can result in truncated responses.",
        "Default: 4096",
        "guardrails: Filters potentially unsafe and harmful responses from a language model using Cortex Guard.\nEither TRUE or FALSE.",
        "Default: FALSE",
        "Specifying the options argument, even if it is an empty object ({}), affects how the prompt argument is\ninterpreted and how the response is formatted.",
        "When the options argument is not specified, returns a string.",
        "When the options argument is given, returns a string representation of a JSON object containing the following keys.",
        "\"choices\": An array of the model\u2019s responses. (Currently, only one response is provided.) Each response is\nan object containing a \"messages\" key whose value is the model\u2019s response to the latest prompt.",
        "\"created\": UNIX timestamp (seconds since midnight, January 1, 1970) when the response was generated.",
        "\"model\": The name of the model that created the response.",
        "\"usage\": An object recording the number of tokens consumed and generated by this completion. Includes\nthe following sub-keys:",
        "\"completion_tokens\": The number of tokens in the generated response.",
        "\"prompt_tokens\": The number of tokens in the prompt.",
        "\"total_tokens\": The total number of tokens consumed, which is the sum of the other two values.",
        "Users must use a role that has been granted the SNOWFLAKE.CORTEX_USER database role.\nSee Required privileges for more information on this privilege.",
        "TRY_COMPLETE does not retain any state from one call to the next. To use the TRY_COMPLETE function to provide a stateful,\nconversational experience, pass all previous user prompts and model responses in the conversation as part of the prompt_or_history\narray (see Templates for Chat Models).\nKeep in mind that the number of tokens processed increases for each \u201cround,\u201d and costs increase proportionally.",
        "The following examples use the TRY_COMPLETE function in various use cases.",
        "To generate a single response:",
        "This example illustrates the use of the function\u2019s options argument to control the inference hyperparameters in a\nsingle response. Note that in this form of the function, the prompt must be provided as an array, since this form\nsupports multiple prompts and responses.",
        "The response is a JSON object containing the message from the language model and other information. Note that the response\nis truncated as instructed in the options argument.",
        "For additional examples, see the COMPLETE (SNOWFLAKE.CORTEX) reference.",
        "Refer to Snowflake AI and ML.",
        "Was this page helpful?",
        "On this page",
        "Related content"
    ]
}