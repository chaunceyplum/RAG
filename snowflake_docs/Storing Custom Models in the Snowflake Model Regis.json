{
    "url": "https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/custom-models",
    "title": "Storing Custom Models in the Snowflake Model Registry | Snowflake Documentation",
    "paragraphs": [
        "The Snowflake model registry allows you to register (log) models and use\nthem for inference inside Snowflake. The registry supports several types of models:",
        "Snowpark ML Modeling",
        "scikit-learn",
        "XGBoost",
        "LightGBM",
        "CatBoost",
        "PyTorch",
        "TensorFlow",
        "MLFlow PyFunc",
        "Sentence Transformer",
        "Hugging Face pipeline",
        "The model registry API also allows you to log other types of models, including those trained using external tools or\nobtained from open source repositories, as long as they are serializable and derived from the\nsnowflake.ml.model.custom_model.CustomModel class.",
        "Note",
        "This guide provides an example of logging a custom model.",
        "See also this blog post\nand accompanying Quickstarts for information about importing models from AWS SageMaker or from Azure ML.",
        "This topic explains how to create models, log them to the Snowflake Model Registry, and deploy them. A common use case\nfor this feature is to define pipelines of multiple classes, such as a few transformers or imputers followed by a\npredictor or classifier. In such cases, the custom model class itself is relatively simple, calling these classes in\nsequence and passing the result of one as the input to the next.",
        "Models often require one or more static files, such as configuration files or model weights, in addition to the code.\nCustom models must provide information on all such artifacts so the registry knows to pack them along with the model.\nArtifacts can be declared using the ModelContext class as follows.",
        "The paths to artifact files in model context are relative to the current directory in the environment from which the\nmodel is logged. The Snowflake Model Registry uses this information to ensure that all necessary code and data is\ndeployed to the warehouse where your model will run. At runtime, the model can read the artifacts with\nself.context.path('config') as a Python file object (the value 'config' is the same as the key in the\ndictionary passed to ModelContext).",
        "Besides static files, a model can compose other models or pipelines of a supported type (for example, a\nsnowflake.ml.modeling.pipeline or a scikit-learn model). The registry already knows how to log these types of\nobjects, so you can pass the Python objects directly in the model context using model_refs, as shown here. You don\u2019t\nneed to package these objects yourself. This can be useful for bootstrap aggregation (bagging) or for preprocessing or\npostprocessing.",
        "Note",
        "model1 and model2 are objects of any type of model natively supported by the registry. feature_preproc\nis a snowflake.ml.modeling.pipeline object.",
        "The model registry serializes these model references when logging the model, and rehydrates them at runtime.  Your model\ncan retrieve references to these subordinate models using, for example, self.context.model_ref('m1').\nIf the model exposes a predict method, your code can call it directly from the retrieved model reference,\nfor example with self.context.model_ref('m1').predict().",
        "In summary, then, a custom model\u2019s context declares the Python objects to be serialized along with the model\u2019s\nartifacts, which are local files that are used by the model and which must be stored in Snowflake along with the code.\nYour model class uses the context to locate the code and the artifacts; this works whether your model is running locally\nor in Snowflake.",
        "To tell the model registry how to log and deploy your custom model, inherit from snowflake.ml.model.custom_model.CustomModel.",
        "Models can expose multiple inference methods (for example, scikit-learn models expose predict and predict_proba\nmethods). To declare inference functions in your custom model, define them as public methods of your subclass and\ndecorate them with @custom_model.inference_api.  This decorator indicates that a method is part of the model\u2019s\npublic API, allowing it to be called from Python or SQL via the model registry. Methods decorated with inference_api\nmust accept and return a pandas DataFrame. Any number of methods can be decorated with inference_api.",
        "Note",
        "The requirement for the public API to accept and return a pandas DataFrame is the same as for\nvectorized UDFs. As with vectorized UDFs,\nthese inference APIs can be called from Python passing a Snowpark DataFrame as well",
        "A skeleton custom model class with a public API is shown below.",
        "Note the use of context.path to open the bias file\nand self.context.model_ref to obtain references to the subordinate model classes so that their predict methods\ncan be called.",
        "Putting all the pieces together, the following is a fully-functional custom model.",
        "Snowflake ML supports passing an arbitrary number of keyword arguments to the ModelContext class, allowing\ndevelopers to easily include additional parameters when defining and initializing custom models.",
        "Below is an example demonstrating how to provide keyword arguments to the model context and use them in a custom\nmodel class:",
        "You can test your new custom model (pipeline) by running it locally as follows:",
        "Or log it in the registry and deploy it to Snowflake. As shown in the next code example, provide conda_dependencies (or\npip_requirements) to specify the libraries that the model class needs.",
        "Provide sample_input_data (a pandas DataFrame) to infer the input signature for the model. Alternatively,\nprovide a model signature.",
        "As of Snowpark ML 1.5.4, you can log models with inference methods that return\nmultiple columns. To do so, log your model with option {\"function_type\": \"TABLE_FUNCTION\"} and use the\n@inference_api decorator as above. In the following example, the decorated method returns a pandas DataFrame that includes\ntwo output columns.",
        "If the model includes multiple inference methods, use the method_options option to log the model,\nindicating which are FUNCTION and which are TABLE_FUNCTION:",
        "The logged model table function inference method can also be invoked via SQL as follows:",
        "To learn more about partitioned model inference methods where the inference method takes a data partition\nas input and outputs multiple rows and columns, see\nPartitioned Custom Models.",
        "Was this page helpful?",
        "On this page"
    ]
}