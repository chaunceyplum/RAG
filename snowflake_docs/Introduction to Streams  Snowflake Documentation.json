{
    "url": "https://docs.snowflake.com/en/user-guide/streams-intro",
    "title": "Introduction to Streams | Snowflake Documentation",
    "paragraphs": [
        "A stream object records data manipulation language (DML) changes made to tables, including inserts, updates, and deletes, as well as\nmetadata about each change, so that actions can be taken using the changed data. This process is referred to as change data capture (CDC).\nThis topic introduces key concepts for change data capture using streams.",
        "An individual table stream tracks the changes made to rows in a source table. A table stream (also referred to as simply a \u201cstream\u201d) makes\na \u201cchange table\u201d available of what changed, at the row level, between two transactional points of time in a table. This allows querying and\nconsuming a sequence of change records in a transactional fashion.",
        "Streams can be created to query change data on the following objects:",
        "Standard tables, including shared tables.",
        "Views, including secure views",
        "Directory tables",
        "Dynamic tables",
        "Apache Iceberg\u2122 tables with Limitations.",
        "Event tables",
        "External tables",
        "When created, a stream logically takes an initial snapshot of every row in the source object (e.g. table, external table, or the underlying\ntables for a view) by initializing a point in time (called an offset) as the current transactional version of the object. The change\ntracking system utilized by the stream then records information about the DML changes after this snapshot was taken. Change records provide\nthe state of a row before and after the change. Change information mirrors the column structure of the tracked source object and includes\nadditional metadata columns that describe each change event.",
        "Note that a stream itself does not contain any table data. A stream only stores an offset for the source object and returns CDC\nrecords by leveraging the versioning history for the source object. When the first stream for a table is created, several hidden columns\nare added to the source table and begin storing change tracking metadata. These columns consume a small amount of storage. The CDC records\nreturned when querying a stream rely on a combination of the offset stored in the stream and the change tracking metadata stored in the\ntable. Note that for streams on views, change tracking must be enabled explicitly for the view and underlying tables to add the hidden\ncolumns to these tables.",
        "It might be useful to think of a stream as a bookmark, which indicates a point in time in the pages of a book (i.e. the source object). A\nbookmark can be thrown away and other bookmarks inserted in different places in a book. So too, a stream can be dropped and other streams\ncreated at the same or different points of time (either by creating the streams consecutively at different times or by using Time\nTravel) to consume the change records for an object at the same or different offsets.",
        "One example of a consumer of CDC records is a data pipeline, in which only the data in staging tables that has\nchanged since the last extraction is transformed and copied into other tables.",
        "A new table version is created whenever a transaction that includes one or more DML statements is committed\nto the table. This applies to the following table types:",
        "Standard tables",
        "Directory tables",
        "Dynamic tables",
        "External tables",
        "Apache Iceberg\u2122 tables",
        "Underlying tables for a view",
        "In the transaction history for a table, a stream offset is located between two table versions. Querying a stream returns the\nchanges caused by transactions committed after the offset and at or before the current time.",
        "The following example shows a source table with 10 committed versions in the timeline. The offset for stream s1 is currently between\ntable versions v3 and v4. When the stream is queried (or consumed), the records returned include all transactions between table\nversion v4, the version immediately after the stream offset in the table timeline, and v10, the most recent committed table version\nin the timeline, inclusive.",
        "A stream provides the minimal set of changes from its current offset to the current version of the table.",
        "Multiple queries can independently consume the same change data from a stream without changing the offset. A stream advances the offset\nonly when it is used in a DML transaction. This includes a Create Table As Select (CTAS) transaction or a COPY INTO location\ntransaction and this behavior applies to both explicit and autocommit transactions. (By default, when a\nDML statement is executed, an autocommit transaction is implicitly started and the transaction is committed at the completion of the\nstatement. This behavior is controlled with the AUTOCOMMIT parameter.) Querying a stream alone does not advance its offset,\neven within an explicit transaction; the stream contents must be consumed in a DML statement.",
        "Note",
        "To advance the offset of a stream to the current table version without consuming the change data in a DML operation, complete either of\nthe following actions:",
        "Recreate the stream (using the CREATE OR REPLACE STREAM syntax).",
        "Insert the current change data into a temporary table. In the INSERT statement, query the stream but include a WHERE clause that\nfilters out all of the change data (e.g. WHERE 0 = 1).",
        "When a SQL statement queries a stream within an explicit transaction, the stream is queried at the stream advance point (i.e. the timestamp)\nwhen the transaction began rather than when the statement was run. This behavior pertains both to DML statements and CREATE TABLE \u2026 AS\nSELECT (CTAS) statements that populate a new table with rows from an existing stream.",
        "A DML statement that selects from a stream consumes all of the change data in the stream as long as the transaction commits successfully. To\nensure multiple statements access the same change records in the stream, surround them with an explicit transaction statement\n(BEGIN .. COMMIT). This locks the stream. DML updates to the source object in parallel\ntransactions are tracked by the change tracking system but do not update the stream until the explicit transaction statement is committed\nand the existing change data is consumed.",
        "Streams support repeatable read isolation. In repeatable read mode, multiple SQL statements within a transaction see the same set of records\nin a stream. This differs from the read committed mode supported for tables, in which statements see any changes made by previous statements\nexecuted within the same transaction, even though those changes are not yet committed.",
        "The delta records returned by streams in a transaction is the range from the current position of the stream until the transaction start\ntime. The stream position advances to the transaction start time if the transaction commits; otherwise it stays at the same position.",
        "Consider the following example:",
        "Time",
        "Transaction 1",
        "Transaction 2",
        "1",
        "Begin transaction.",
        "2",
        "Query stream s1 on table t1. The stream returns the change data capture records . between the current position to the Transaction 1 start time. If the stream is used in a DML statement . the stream is then locked to avoid changes by concurrent transactions.",
        "3",
        "Update rows in table t1.",
        "4",
        "Query stream s1. Returns the same state of stream when it was used at Time 2.",
        "5",
        "Commit transaction.  If the stream was consumed in DML statements within the transaction, the stream position advances to the transaction start time.",
        "6",
        "Begin transaction.",
        "7",
        "Query stream s1. Results include table changes committed by Transaction 1.",
        "Within Transaction 1, all queries to stream s1 see the same set of records. DML changes to table t1 are recorded to the stream only\nwhen the transaction is committed.",
        "In Transaction 2, queries to the stream see the changes recorded to the table in Transaction 1. Note that if Transaction 2 had begun\nbefore Transaction 1 was committed, queries to the stream would have returned a snapshot of the stream from the position of the\nstream to the beginning time of Transaction 2 and would not see any changes committed by Transaction 1.",
        "A stream stores an offset for the source object and not any actual table columns or data. When queried, a stream accesses and returns the\nhistoric data in the same shape as the source object (i.e. the same column names and ordering) with the following additional columns:",
        "Indicates the DML operation (INSERT, DELETE) recorded.",
        "Indicates whether the operation was part of an UPDATE statement. Updates to rows in the source object are represented as a pair of DELETE\nand INSERT records in the stream with a metadata column METADATA$ISUPDATE values set to TRUE.",
        "Note that streams record the differences between two offsets. If a row is added and then updated in the current offset, the delta change\nis a new row. The METADATA$ISUPDATE row records a FALSE value.",
        "Specifies the unique and immutable ID for the row, which can be used to track changes to specific rows over time.",
        "Snowflake provides the following guarantees with respect to METADATA$ROW_ID:",
        "The  METADATA$ROW_ID depends on the stream\u2019s source object.",
        "For instance, a stream stream1 on table table1 and stream stream2 on table table1 produce the same METADATA$ROW_IDs for the same\nrows, but a stream stream_view on view view1 is not guaranteed to produce the same METADATA$ROW_IDs as stream1, even if view is\ndefined using the statement CREATE VIEW view AS SELECT * FROM table1.",
        "A stream on a source object and a stream on the source object\u2019s clone produce the same METADATA$ROW_IDs for the rows that exist at the time of the\ncloning.",
        "A stream on a source object and a stream on the source object\u2019s replica produce the same METADATA$ROW_IDs for the rows that were replicated.",
        "The following stream types are available based on the metadata recorded by each:",
        "Supported for streams on standard tables, dynamic tables, Snowflake-managed Apache Iceberg\u2122 tables, directory tables, or views. A standard (i.e. delta) stream tracks all DML\nchanges to the source object, including inserts, updates, and deletes (including table truncates). This stream type performs a join on\ninserted and deleted rows in the change set to provide the row level delta. As a net effect, for example, a row that is inserted and\nthen deleted between two transactional points of time in a table is removed in the delta (i.e. is not returned when the stream is queried).",
        "Note",
        "Standard streams cannot retrieve change data for geospatial data. We recommend creating append-only streams on objects that contain\ngeospatial data.",
        "Supported for streams on standard tables, dynamic tables, Snowflake-managed Apache Iceberg\u2122 tables, or views. An append-only stream exclusively tracks row\ninserts. Update, delete, and truncate operations are not captured by append-only streams. For instance, if 10 rows are\ninitially inserted into a table, and then 5 of those rows are deleted before advancing the offset for an append-only stream, the\nstream would only record the 10 inserted rows.",
        "An append-only stream specifically returns the appended rows, making it notably more performant than a standard stream for\nextract, load, and transform (ELT), and similar scenarios reliant solely on row inserts. For example, a source table can be\ntruncated immediately after the rows in an append-only stream are consumed, and the record deletions do not contribute to the\noverhead the next time the stream is queried or consumed.",
        "Creating an append-only streams in a target account using a secondary object as the source is not supported.",
        "Supported for streams on Apache Iceberg\u2122 or external tables. An insert-only stream tracks row inserts only; they do not record delete\noperations that remove rows from an inserted set (i.e. no-ops). For example, in-between any two offsets, if File1 is removed from the\ncloud storage location referenced by the external table, and File2 is added, the stream returns records for the rows in File2 only.\nUnlike when tracking CDC data for standard tables, Snowflake cannot access the historical records for files in cloud storage.",
        "Overwritten or appended files are essentially handled as new files: The old version of the file is removed from cloud storage, but the\ninsert-only stream does not record the delete operation. The new version of the file is added to cloud storage, and the insert-only\nstream records the rows as inserts. The stream does not record the diff of the old and new file versions. Note that appends may not\ntrigger an automatic refresh of the external table metadata, such as when using\nAzure AppendBlobs.",
        "The following diagram shows how the contents of a standard stream change as rows in the source table are updated. Whenever a DML\nstatement consumes the stream contents, the stream position advances to track the next set of DML changes to the table (i.e. the changes in\na table version):",
        "A stream becomes stale when its offset falls outside of the data retention period for\nits source table (or underlying tables for a source view). In a stale state, historical\ndata and any unconsumed change records for the source table are no longer accessible. To\ncontinue tracking new change records, you must recreate the stream using the\nCREATE STREAM command.",
        "To prevent a stream from becoming stale, consume the stream records within a DML\nstatement during the table\u2019s retention period and regularly consume its change data\nbefore its STALE_AFTER timestamp (that is, within the extended data retention period\nfor the source object). Additionally, calling\nSYSTEM$STREAM_HAS_DATA on the stream prevents it from\nbecoming stale, provided the stream is empty and the SYSTEM$STREAM_HAS_DATA function\nreturns FALSE.",
        "For more information on data retention periods, see Understanding & using Time Travel.",
        "Note",
        "Streams on shared tables or views don\u2019t extend the data retention period for the table\nor underlying tables, respectively. For more information, see\nStreams on shared objects.",
        "If the data retention period for a table is less than 14 days and a stream hasn\u2019t been\nconsumed, Snowflake temporarily extends this period to prevent the stream from going\nstale. The retention period is extended to the stream\u2019s offset, up to a maximum of 14 days\nby default, regardless of your Snowflake edition. The\nmaximum number of days for which Snowflake can extend the data retention period is\ndetermined by the MAX_DATA_EXTENSION_TIME_IN_DAYS parameter value. Once the\nstream is consumed, the extended data retention period reverts to the table\u2019s default.",
        "The following table shows examples of DATA_RETENTION_TIME_IN_DAYS and\nMAX_DATA_EXTENSION_TIME_IN_DAYS values, indicating how often the stream contents should be\nconsumed to avoid staleness:",
        "DATA_RETENTION_TIME_IN_DAYS",
        "MAX_DATA_EXTENSION_TIME_IN_DAYS",
        "Consume Streams in X Days",
        "14",
        "0",
        "14",
        "1",
        "14",
        "14",
        "0",
        "90",
        "90",
        "To check the staleness status of a stream, use the DESCRIBE STREAM\nor SHOW STREAMS command. The STALE_AFTER column timestamp is\nthe extended data retention period for the source object. It shows when the stream is\npredicted to become stale or when it became stale if the timestamp is in the past. This\ntimestamp is calculated by  adding the greater value of the DATA_RETENTION_TIME_IN_DAYS\nor MAX_DATA_EXTENSION_TIME_IN_DAYS parameters setting for the source object to the\ncurrent timestamp.",
        "Note",
        "If the data retention period for the source table is set at the schema or database level, the current role must have access to the schema or database to calculate the STALE_AFTER value.",
        "Consuming change data for a stream updates the STALE_AFTER timestamp. Although reading\nfrom the stream might succeed for some time after the STALE_AFTER timestamp, the stream\ncan become stale at any moment. The STALE column indicates if the stream is expected to\nbe stale, though it might not be stale yet.",
        "To prevent a stream from becoming stale, regularly consume its change data before its\nSTALE_AFTER timestamp (that is, within the extended data retention period for the\nsource object). Don\u2019t rely on the results from a stream after the STALE_AFTER period has\nelapsed because the STREAM_HAS_DATA function might return unexpected results.",
        "After the STALE_AFTER timestamp has passed, the stream can become stale at any time,\neven if it has no unconsumed records. Querying a stream might return 0 records even\nif there is change data for the source object. For example, an append-only stream\ntracks row inserts only, but updates and deletes also write change records to the\nsource object. Additionally, some table writes, like reclustering, don\u2019t produce change\ndata. Consuming change data for a stream advances its offset to the present, regardless\nof whether there is intervening change data.",
        "Important",
        "Recreating an object (using the CREATE OR REPLACE TABLE syntax) drops its history, which also makes any stream on the table or view\nstale. In addition, recreating or dropping any of the underlying tables for a view makes any stream on the view stale.",
        "Currently, when a database or schema that contains a stream and its source table (or the underlying tables for a source view) is\ncloned, any unconsumed records in the stream clone are inaccessible. This behavior is consistent with\nTime Travel for tables. If a table is cloned, historical data for the table clone begins at the\ntime/point when the clone was created.",
        "Renaming a source object does not break a stream or cause it to go stale. In addition, if a source object is dropped and a new object\nis created with the same name, any streams linked to the original object are not linked to the new object.",
        "We recommend that users create a separate stream for each consumer of change records for an object. \u201cConsumer\u201d refers to a task, script, or\nother mechanism that consumes the change records for an object using a DML transaction. As stated earlier in this topic, a stream advances its\noffset only when it is used in a DML transaction. This includes a Create Table As Select (CTAS) transaction or a COPY INTO location transaction.",
        "Different consumers of change data in a single stream retrieve different deltas unless Time Travel is used. When the change data captured from\nthe latest offset in a stream is consumed using a DML transaction, the stream advances the offset. The change data is no longer available for\nthe next consumer. To consume the same change data for an object, create multiple streams for the object. A stream only stores an offset\nfor the source object and not any actual table column data; therefore, you can create any number of streams for an object without incurring significant cost.",
        "Streams on views support both local views and views shared using Snowflake Secure Data Sharing, including secure views. Currently, streams\ncannot track changes in materialized views.",
        "Streams are limited to views that satisfy the following requirements:",
        "All of the underlying tables must be native tables.",
        "The view may only apply the following operations:",
        "Projections",
        "Filters",
        "Inner or cross joins",
        "UNION ALL",
        "A UNION ALL of a table with itself is not supported.",
        "Nested views and subqueries in the FROM clause are supported as long as the fully-expanded query satisfies the other requirements in this requirements table.",
        "General requirements:",
        "The query can select any number of columns.",
        "The query can contain any number of WHERE predicates.",
        "Views with the following operations are not yet supported:",
        "GROUP BY clauses",
        "QUALIFY clauses",
        "Subqueries not in the FROM clause",
        "Correlated subqueries",
        "LIMIT clauses",
        "DISTINCT clauses",
        "Functions:",
        "Functions in the select list must be system-defined, scalar functions.",
        "Change tracking must be enabled in the underlying tables.",
        "Prior to creating a stream on a view, you must enable change tracking on the underlying tables for the view. For instructions, see\nEnabling Change Tracking on Views and Underlying Tables.",
        "When examining the results of a stream that tracks changes to a view containing a join,\nit\u2019s important to understand what data is being joined.\nChanges that have occurred on the left table since the stream offset are being joined with the right table,\nchanges on the right table since the stream offset are being joined with the left table,\nand changes on both tables since the stream offset are being joined with each other.",
        "Consider the following example:",
        "Two tables are created:",
        "A view is created to join the two tables on id. Each table has a single row that joins with the other:",
        "A stream is created that tracks changes to the view:",
        "The view has one entry and the stream has none since there have been no changes to the tables since the stream\u2019s current offset:",
        "Once updates are made to the underlying tables, selecting ordersByCustomerStream will produce records of orders x \u0394 customers + \u0394 orders x\ncustomers + \u0394 orders x \u0394 customers where:",
        "\u0394 orders and \u0394 customers are the changes that have occurred to each table since the stream offset.",
        "orders and customers are the total contents of the tables at the current stream offset.",
        "Note that due to optimizations in Snowflake the cost of computing this expression is not always linearly proportional to the size of the inputs.",
        "If another joining row is inserted in orders then ordersByCustomer will have a new row:",
        "Selecting from ordersByCustomersStream produces one row because \u0394 orders x customers contains the new insert and orders x \u0394 customers +\n\u0394 orders x \u0394 customers is empty:",
        "If another joining row is then inserted into customers then ordersByCustomer will have a total of three new rows:",
        "Selecting from ordersByCustomersStream produces three rows because\n\u0394 orders x customers, orders x \u0394 customers, and \u0394 orders x \u0394 customers will each produce one row:",
        "Note that for append-only streams, \u0394 orders and \u0394 customers will contain row inserts only,\nwhile orders and customers will contain the complete contents of the tables including any updates that happened before the stream offset.",
        "As an alternative to streams, Snowflake supports querying change tracking metadata for tables or views using the\nCHANGES clause for SELECT statements. The CHANGES clause enables querying change tracking metadata between\ntwo points in time without having to create a stream with an explicit transactional offset. Using the CHANGES clause does not\nadvance the offset (i.e. consume the records). Multiple queries can retrieve the change tracking metadata between different transactional\nstart and endpoints. This option requires specifying a transactional start point for the metadata using an\nAT | BEFORE clause; the end point for the change tracking interval can be set using the optional END clause.",
        "A stream stores the current transactional table version and is the appropriate source of CDC\nrecords in most scenarios. For infrequent scenarios that require managing the offset for arbitrary periods of time, the CHANGES clause is\navailable for your use.",
        "Currently, the following must be true before change tracking metadata is recorded:",
        "Either enable change tracking on the table (using ALTER TABLE \u2026 CHANGE_TRACKING = TRUE), or create a stream\non the table (using CREATE STREAM).",
        "Enable change tracking on the view and its underlying tables. For instructions, see Enabling Change Tracking on Views and Underlying Tables.",
        "Enabling change tracking adds several hidden columns to the table and begins storing change tracking metadata. The values in these\nhidden CDC data columns provide the input for the stream metadata columns. The columns consume a\nsmall amount of storage.",
        "No change tracking metadata for the object is available for the period before one of these conditions is satisfied.",
        "Querying a stream requires a role with a minimum of the following role permissions:",
        "Object",
        "Privilege",
        "Notes",
        "Database",
        "USAGE",
        "Schema",
        "USAGE",
        "Stream",
        "SELECT",
        "Table",
        "SELECT",
        "Streams on tables only.",
        "View",
        "SELECT",
        "Streams on views only.",
        "External stage",
        "USAGE",
        "Streams on directory tables (on external stages) only",
        "Internal stage",
        "READ",
        "Streams on directory tables (on internal stages) only",
        "As described in Data Retention Period and Staleness (in this topic), when a stream is not consumed regularly, Snowflake\ntemporarily extends the data retention period for the source table or the underlying tables in the source view. If the data\nretention period for the table is less than 14 days, then behind the scenes, the period is extended to the smaller of the\nstream transactional offset or 14 days (if the data retention period for the table is less than 14 days) regardless of the\nSnowflake edition for your account.",
        "Extending the data retention period requires additional storage which will be reflected in your monthly storage charges.",
        "The main cost associated with a stream is the processing time used by a virtual warehouse to query the stream. These charges appear on your\nbill as familiar Snowflake credits.",
        "The following limitations apply for streams:",
        "You can\u2019t use standard or append-only streams on Apache Iceberg\u2122 tables that use an external catalog. (Insert-only streams are supported.)",
        "You can\u2019t track changes on a view with UNION ALL of a table and itself or a clone of itself.",
        "You can\u2019t track changes on a view with GROUP BY clauses.",
        "For streams on views, you can\u2019t use UNION ALL of a table with itself.",
        "After adding or modifying a column to be NOT NULL, queries on streams might fail if the stream outputs rows with impermissible NULL\nvalues. This happens because the stream\u2019s schema enforces the current NOT NULL constraint, which doesn\u2019t match the historical data\nreturned by the stream.",
        "Was this page helpful?",
        "On this page",
        "Related content"
    ]
}